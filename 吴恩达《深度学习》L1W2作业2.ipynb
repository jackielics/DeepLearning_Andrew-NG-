{"cells":[{"source":"# 用神经网络思想实现Logistic回归\n\n欢迎来到你的第一个编程作业！ 你将学习如何建立逻辑回归分类器用来识别猫。 这项作业将引导你逐步了解神经网络的思维方式，同时磨练你对深度学习的直觉。\n\n\n**说明：**\n除非指令中明确要求使用，否则请勿在代码中使用循环（for / while）。\n\n**你将学习以下内容：**\n* 建立学习算法的一般架构，包括：\n\t* \t初始化参数\n\t* \t计算损失函数及其梯度\n\t* \t使用优化算法（梯度下降）\n* 按正确的顺序将以上所有三个功能集成到一个主模型上。\n\n","cell_type":"markdown","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"571F19AD1DCE4B49894CF23016CA385F","mdEditEnable":false}},{"source":"## 1- 安装包##\n\n首先，让我们运行下面的单元格，以导入作业中所需的包。\n* [numpy](www.numpy.org) 是Python科学计算的基本包。\n* [h5py](http://www.h5py.org)是一个常用的包，可以处理存储为H5文件格式的数据集。\n* [matplotlib](http://matplotlib.org)是一个著名的Python图形库。\n* 这里最后通过[PIL]（http://www.pythonware.com/products/pil/）和[scipy](https://www.scipy.org/) 使用你自己的图片去测试模型效果。","cell_type":"markdown","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"34490B440DFE48FA89D0F1C9329DA6E3","mdEditEnable":false}},{"outputs":[{"output_type":"stream","text":"/home/kesci/input/deeplearningai17761\n","name":"stdout"}],"execution_count":1,"source":"cd ../input/deeplearningai17761","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"34224F324F6E49DC91FBEAD65B0B9F6A","scrolled":false}},{"outputs":[],"execution_count":2,"source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom lr_utils import load_dataset\n\n%matplotlib inline\n","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B97666B54EE8480E84E0B15D1A3CDE67","mdEditEnable":false,"scrolled":false}},{"metadata":{"id":"58635C73983246E38E11858CF80E3DAE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## 2- 问题概述##\n\n**问题说明**：你将获得一个包含以下内容的数据集（\"data.h5\"）：\n*      标记为cat（y = 1）或非cat（y = 0）的**m_train**训练图像集\n*      标记为cat或non-cat的**m_test**测试图像集\n*      图像维度为（num_px，num_px，3），其中3表示3个通道（RGB）。 因此，每个图像都是正方形（高度= num_px）和（宽度= num_px）。\n\n你将构建一个简单的图像识别算法，该算法可以将图片正确分类为猫和非猫。\n让我们熟悉一下数据集吧， 首先通过运行以下代码来加载数据。"},{"metadata":{"id":"0C6507111123402F8AEF52B3DB98DC4A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# Loading the data (cat/non-cat)\ntrain_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()","execution_count":3},{"metadata":{"id":"672DA01540034EAF93A20F22A6313DB4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"我们在图像数据集（训练和测试）的末尾添加了\"_orig\"，以便对其进行预处理。 预处理后，我们将得到train_set_x和test_set_x（标签train_set_y和test_set_y不需要任何预处理）。\n\ntrain_set_x_orig和test_set_x_orig的每一行都是代表图像的数组。 你可以通过运行以下代码来可视化示例。 还可以随意更改`index`值并重新运行以查看其他图像。"},{"metadata":{"id":"857FD8D2CA354D3F87815C65FC89CB2F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"y = [0], it's a 'non-cat' picture.\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/857FD8D2CA354D3F87815C65FC89CB2F/qhvrebove5.png\">"},"transient":{}}],"source":"# Example of a picture\nindex = 5\nplt.imshow(train_set_x_orig[index])\nprint (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")","execution_count":5},{"metadata":{"id":"B42952E0488C428491E6A9250041D24D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"深度学习中的许多报错都来自于矩阵/向量尺寸不匹配。 如果你可以保持矩阵/向量的尺寸不变，那么将消除大多错误。\n\n**练习：** 查找以下各项的值：\n*      m_train（训练集示例数量）\n*      m_test（测试集示例数量）\n*      num_px（=训练图像的高度=训练图像的宽度）\n\n请记住，“ train_set_x_orig”是一个维度为（m_train，num_px，num_px，3）的四维numpy数组（存储多张图像，图像数=第一维m_train，如果是正方形则第二维=第三维）。 例如，你可以通过编写“ train_set_x_orig.shape [0]”来访问“ m_train”。"},{"metadata":{"id":"E01D9A35547B4C5C850AED09367775C7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Number of training examples: m_train = 209\nNumber of testing examples: m_test = 50\nHeight/Width of each image: num_px = 64\nEach image is of size: (64, 64, 3)\ntrain_set_x shape: (209, 64, 64, 3)\ntrain_set_y shape: (1, 209)\ntest_set_x shape: (50, 64, 64, 3)\ntest_set_y shape: (1, 50)\n","name":"stdout"}],"source":"### START CODE HERE ### (≈ 3 lines of code)\nm_train = train_set_x_orig.shape[0]\nm_test = test_set_x_orig.shape[0]\nnum_px = train_set_x_orig.shape[1]\n### END CODE HERE ###\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))\nprint (\"Height/Width of each image: num_px = \" + str(num_px))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_set_x shape: \" + str(train_set_x_orig.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x shape: \" + str(test_set_x_orig.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))","execution_count":6},{"metadata":{"id":"2E4941780D914EB895A0A637A799128E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"**预期输出**：\n训练集数量：m_train = 209\n测试集数量：m_test = 50\n每个图像的高度/宽度：num_px = 64\n每个图像的大小：（64，64，3）\ntrain_set_x维度：（209、64、64、3）\ntrainsety维度：（1，209）\ntest_set_x维度：（50、64、64、3）\ntest_set_y维度：（1，50）"},{"metadata":{"id":"877F449D65754307AE134CB6232D17D1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"为了方便起见，你现在应该以维度**(num_px $*$ num_px $*$ 3, 1)的numpy数组重塑维度（num_px，num_px，3）**的图像（像素数不变，依然为num_px $*$ num_px $*$ 3，第二维“1”代表一张图片）。 此后，我们的训练（和测试）数据集是一个numpy数组，其中每列代表一个展平的图像。 应该有m_train（和m_test）列。\n\n**练习：** 重塑训练和测试数据集，以便将大小（num_px，num_px，3）的图像展平为单个形状的向量(num\\_px $*$ num\\_px $*$ 3, 1)。\n\n当你想将维度为（a，b，c，d）的矩阵X展平为形状为(b$*$c$*$d, a)的矩阵X_flatten时的一个技巧是：\n```python\nX_flatten = X.reshape（X.shape [0]，-1）.T     ＃ 其中X.T是X的转置矩阵\n```"},{"metadata":{"id":"910201D3254342DA9263EA0F1C460725","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"train_set_x_flatten shape: (12288, 209)\ntrain_set_y shape: (1, 209)\ntest_set_x_flatten shape: (12288, 50)\ntest_set_y shape: (1, 50)\nsanity check after reshaping: [17 31 56 22 33]\n","name":"stdout"}],"source":"# Reshape the training and test examples\n\n### START CODE HERE ### (≈ 2 lines of code)\ntrain_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\ntest_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n### END CODE HERE ###\n\nprint (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\nprint (\"train_set_y shape: \" + str(train_set_y.shape))\nprint (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\nprint (\"test_set_y shape: \" + str(test_set_y.shape))\nprint (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))","execution_count":7},{"metadata":{"id":"48580F692ECA4F79A784133FB831621C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"**预期输出**：\ntrain_set_x_flatten维度：（12288，209）（209列每列代表一张图像）\ntrainsety维度：（1，209）\ntest_set_x_flatten维度：（12288，50）\ntest_set_y维度：（1，50）\n重塑后的检查维度：[17 31 56 22 33]"},{"metadata":{"id":"E4B14C27F3D74B7F8B3EB94270A02D67","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"为了表示彩色图像，必须为每个像素指定红、绿、蓝色通道（RGB）（train_set_x_flatten的每一个数值其实代表R/G/B值），因此像素值实际上是一个从0到255的三个数字的向量。\n\n机器学习中一个常见的预处理步骤是对数据集进行居中和标准化Norm，这意味着你要从每个示例中减去整个numpy数组的均值，然后除以整个numpy数组的标准差。但是图片数据集则更为简单方便，并且只要将数据集的每一行除以255（像素通道的最大值），效果也差不多。\n\n在训练模型期间，你将要乘以权重并向一些初始输入添加偏差以观察神经元的激活。然后，使用反向梯度传播以训练模型。但是，让特征具有相似的范围以至渐变不会爆炸是非常重要的。具体内容我们将在后面的教程中详细学习！\n\n开始标准化我们的数据集吧！"},{"metadata":{"id":"DAC773801FE34940B782D40B4F631A59","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"train_set_x = train_set_x_flatten/255.\ntest_set_x = test_set_x_flatten/255.","execution_count":10},{"metadata":{"id":"83D8B5A78DFF460892F60B697DC158FD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"**你需要记住的内容：**\n\n预处理数据集的常见步骤是：\n* 找出数据的尺寸和维度（m_train，m_test，num_px等）\n* 重塑数据集，以使每个示例都是大小为（num_px \\ * num_px \\ * 3，1）的向量\n* “标准化”数据"},{"metadata":{"id":"AE890356CEB64A2192E34FA57DF421BB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"## 3- 学习算法的一般架构##\n\n现在是时候设计一种简单的算法来区分猫图像和非猫图像了。\n\n你将使用神经网络思维方式建立Logistic回归。 下图说明了为什么“逻辑回归实际上是一个非常简单的神经网络！”\n\n![Image Name](https://cdn.kesci.com/upload/image/q15kx21spv.png?imageView2/0/w/960/h/960)\n\n**算法的数学表达式**：\n\nFor one example $x^{(i)}$:\n$z^{(i)} = w^T x^{(i)} + b \\tag{1}$\n$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$ \n$$\n\\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}\n$$\n\nThe cost is then computed by summing over all training examples:\n$$\nJ = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}\n$$\n\t\t\n**关键步骤**：\n在本练习中，你将执行以下步骤：\n*      初始化模型参数\n*      通过最小化损失来学习模型的参数\n*      使用学习到的参数进行预测（在测试集上）\n*      分析结果并得出结论\n"},{"metadata":{"id":"02AD00B0C7A64F1492F9508E785983FD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## 4- 构建算法的各个部分##\n\n建立神经网络的主要步骤是：\n1.定义模型结构（例如输入特征的数量）\n2.初始化模型的参数\n3.循环：\n*      计算当前损失（正向传播）\n*      计算当前梯度（向后传播）\n*      更新参数（梯度下降）\n\n你通常会分别构建1-3，然后将它们集成到一个称为“ model（）”的函数中。\n\n### 4.1- 辅助函数\n\n**练习**：使用“Python基础”中的代码，实现`sigmoid（）`。 如上图所示，你需要计算$sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ 去预测。 使用np.exp（）。\n"},{"metadata":{"id":"E2B223F714DD45198ABB982479E4D293","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# GRADED FUNCTION: sigmoid\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n\n    ### START CODE HERE ### (≈ 1 line of code)\n    s = 1 / (1 + np.exp(-z))\n    ### END CODE HERE ###\n    \n    return s","execution_count":12},{"metadata":{"id":"93F4F66996CB4B87BADAC0F164D97EB0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"sigmoid([0, 2]) = [0.5        0.88079708]\n","name":"stdout"}],"source":"print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))","execution_count":13},{"metadata":{"id":"9CF32CE87CF9490F91746E5983C32109","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"**预期输出**: \nsigmoid([0, 2]) = [0.5        0.88079708]"},{"metadata":{"id":"36CB36FAD507473499DDF4507DD3D870","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# GRADED FUNCTION: initialize_with_zeros\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias)\n    \"\"\"\n    \n    ### START CODE HERE ### (≈ 1 line of code)\n    w = np.zeros((dim, 1))\n    b = 0\n    ### END CODE HERE ###\n\n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b","execution_count":14},{"metadata":{"id":"A2F1C1ED14D4475984A1E0DA6D6C0628","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"w = [[0.]\n [0.]]\nb = 0\n","name":"stdout"}],"source":"dim = 2\nw, b = initialize_with_zeros(dim)\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))","execution_count":15},{"metadata":{"id":"435AFB96E9F84A77844F999B2CE50F2F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"**预期输出**: \nw = [[0.]\n[0.]]\nb = 0\n\nw,b非常重要，所以使用assert来确保准确性。\n对于图像输入，w的维度为(num_px $\\times$ num_px $\\times$ 3, 1)。"},{"metadata":{"id":"053B874C2A4F476F88E6B8D33C450D66","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"### 4.3- 前向和后向传播\n\n现在，你的参数已初始化，你可以执行“向前”和“向后”传播步骤来学习参数。\n\n**练习：** 实现函数propagate（）来计算损失函数及其梯度。\n\n**提示**：\n正向传播：\n* 得到X\n* 计算$A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n* 计算损失函数：$J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n\n### 可求得以下两个公式（不要忽略激活函数sigmoid的表达式）：\n$$\n\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}\n$$\n一个(m,1)维的矩阵\n$$\n\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}\n$$\n一个数"},{"metadata":{"id":"FF67C965945040DB91D7E076A44D77D2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# GRADED FUNCTION: propagate\n\ndef propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n    \n    Tips:\n    - Write your code step by step for the propagation. np.log(), np.dot()\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    A = sigmoid(np.dot(w.T, X) + b)            # compute activation\n    cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))         # compute cost\n    ### END CODE HERE ###\n    \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    dw = 1 / m * np.dot(X, (A - Y).T)\n    db = 1 / m * np.sum(A - Y)\n    ### END CODE HERE ###\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    # np.squeeze从数组的形状中删除单维度条目，即把shape中为1的维度去掉\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost","execution_count":17},{"metadata":{"id":"D2746C195EDD4AC8830EDDC44AFA334A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"dw = [[0.99993216]\n [1.99980262]]\ndb = 0.49993523062470574\ncost = 6.000064773192205\n","name":"stdout"}],"source":"w, b, X, Y = np.array([[1],[2]]), 2, np.array([[1,2],[3,4]]), np.array([[1,0]])\ngrads, cost = propagate(w, b, X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))","execution_count":18},{"metadata":{"id":"8B403E5AE4BD499D91B9DC630E6D8214","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"**预期输出**:\ndw = [[0.99993216]\n [1.99980262]]\ndb = 0.49993523062470574\ncost = 6.000064773192205"},{"metadata":{"id":"68D85F64183040A68E02B1DAFC2CF20C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"### d）优化函数\n* 初始化参数。\n* 计算损失函数及其梯度。\n* 使用梯度下降来更新参数。\n\n**练习：** 写下优化函数。 目标是通过最小化损失函数 $J$ 来学习 $w$ 和 $b$。 对于参数$\\theta$，更新规则为$\\theta = \\theta - \\alpha \\text{ }  d\\theta$，其中$\\alpha$是学习率。\n"},{"metadata":{"id":"CBAF3F482AAC42BF86E59EC4EA49CC62","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# GRADED FUNCTION: optimize\n\ndef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    Tips:\n    You basically need to write down two steps and iterate through them:\n        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n        2) Update the parameters using gradient descent rule for w and b.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        \n        # Cost and gradient calculation (≈ 1-4 lines of code)\n        ### START CODE HERE ### \n        grads, cost = propagate(w, b, X, Y)\n        ### END CODE HERE ###\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule (≈ 2 lines of code)\n        ### START CODE HERE ###\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        ### END CODE HERE ###\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        # Print the cost every 100 training examples\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","execution_count":19},{"metadata":{"id":"EE444E0F8E6B4D3B823EC78782FA851B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"w = [[0.1124579 ]\n [0.23106775]]\nb = 1.5593049248448891\ndw = [[0.90158428]\n [1.76250842]]\ndb = 0.4304620716786828\n[6.000064773192205]\n","name":"stdout"}],"source":"params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint(costs)","execution_count":20},{"metadata":{"id":"6BACEB0BF9CB40D687898F63FB0A4B38","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"**预期输出**: \nw = [[0.1124579 ]\n [0.23106775]]\nb = 1.5593049248448891\ndw = [[0.90158428]\n [1.76250842]]\ndb = 0.4304620716786828\n[6.000064773192205]"},{"metadata":{"id":"0A1FDB293EC14A6F8C7B6E7A52B26E79","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"**练习：** 上一个函数将输出学习到的w和b。 我们能够使用w和b来预测数据集X的标签。实现`predict（）`函数。 预测分类有两个步骤：\n1.计算$\\hat{Y} = A = \\sigma(w^T X + b)$\n2.将a的项转换为0（如果激活<= 0.5）或1（如果激活> 0.5），并将预测结果存储在向量“ Y_prediction”中。 如果愿意，可以在for循环中使用if / else语句。"},{"metadata":{"id":"19B8FA70D2C94A5B8963B662FBFBFB9D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# GRADED FUNCTION: predict\n\ndef predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n    ### START CODE HERE ### (≈ 1 line of code)\n    A = sigmoid(np.dot(w.T, X) + b)\n    ### END CODE HERE ###\n\n    for i in range(A.shape[1]):\n        \n        # Convert probabilities A[0,i] to actual predictions p[0,i]\n        ### START CODE HERE ### (≈ 4 lines of code)\n        if A[0, i] <= 0.5:\n            Y_prediction[0, i] = 0\n        else:\n            Y_prediction[0, i] = 1\n        ### END CODE HERE ###\n    \n    assert(Y_prediction.shape == (1, m))\n    \n    return Y_prediction","execution_count":23},{"metadata":{"id":"1067B63A99104BB581F07CD1541C50BF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"predictions = [[1. 1.]]\n","name":"stdout"}],"source":"print (\"predictions = \" + str(predict(w, b, X)))","execution_count":24},{"metadata":{"id":"97E6C8ED8398499C96DEC771AADF4CD7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"**预期输出**: \npredictions = [[1. 1.]]\n"},{"metadata":{"id":"DF722896854048F88CF0ECA4BAEE43BF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"**你需要记住以下几点：**\n你已经实现了以下几个函数：\n* 初始化（w，b）\n* 迭代优化损失以学习参数（w，b）：\n\t*     计算损失及其梯度\n\t*     使用梯度下降更新参数\n* 使用学到的（w，b）来预测给定示例集的标签"},{"metadata":{"id":"6CE9E8145B754A4280630AD894B98258","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## 5- 将所有功能合并到模型中##\n\n现在，将所有构件（在上一部分中实现的功能）以正确的顺序放在一起，从而得到整体的模型结构。\n\n**练习：** 实现模型功能，使用以下符号：\n*      Y_prediction对测试集的预测\n*      Y_prediction_train对训练集的预测\n*      w，损失，optimize（）输出的梯度\n\t\t \n\t\t "},{"metadata":{"id":"FD1DA8BFF119448182E801B34B615FC9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# GRADED FUNCTION: model\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function you've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n    \n    ### START CODE HERE ###\n    \n    # initialize parameters with zeros (≈ 1 line of code)\n    w, b = initialize_with_zeros(X_train.shape[0])\n\n    # Gradient descent (≈ 1 line of code)\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n    \n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict test/train set examples (≈ 2 lines of code)\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n\n    ### END CODE HERE ###\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d","execution_count":25},{"metadata":{"id":"174259EB9B0D459E83137C48CA2359B7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"运行以下单元格训练模型："},{"metadata":{"id":"588EE178E8474F4786B14CB022726D21","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Cost after iteration 0: 0.693147\nCost after iteration 100: 0.584508\nCost after iteration 200: 0.466949\nCost after iteration 300: 0.376007\nCost after iteration 400: 0.331463\nCost after iteration 500: 0.303273\nCost after iteration 600: 0.279880\nCost after iteration 700: 0.260042\nCost after iteration 800: 0.242941\nCost after iteration 900: 0.228004\nCost after iteration 1000: 0.214820\nCost after iteration 1100: 0.203078\nCost after iteration 1200: 0.192544\nCost after iteration 1300: 0.183033\nCost after iteration 1400: 0.174399\nCost after iteration 1500: 0.166521\nCost after iteration 1600: 0.159305\nCost after iteration 1700: 0.152667\nCost after iteration 1800: 0.146542\nCost after iteration 1900: 0.140872\nCost after iteration 2000: 0.135608\nCost after iteration 2100: 0.130708\nCost after iteration 2200: 0.126137\nCost after iteration 2300: 0.121861\nCost after iteration 2400: 0.117855\nCost after iteration 2500: 0.114093\nCost after iteration 2600: 0.110554\nCost after iteration 2700: 0.107219\nCost after iteration 2800: 0.104072\nCost after iteration 2900: 0.101097\nCost after iteration 3000: 0.098280\nCost after iteration 3100: 0.095610\nCost after iteration 3200: 0.093075\nCost after iteration 3300: 0.090667\nCost after iteration 3400: 0.088374\nCost after iteration 3500: 0.086190\nCost after iteration 3600: 0.084108\nCost after iteration 3700: 0.082119\nCost after iteration 3800: 0.080219\nCost after iteration 3900: 0.078402\nCost after iteration 4000: 0.076662\nCost after iteration 4100: 0.074994\nCost after iteration 4200: 0.073395\nCost after iteration 4300: 0.071860\nCost after iteration 4400: 0.070385\nCost after iteration 4500: 0.068968\nCost after iteration 4600: 0.067604\nCost after iteration 4700: 0.066291\nCost after iteration 4800: 0.065027\nCost after iteration 4900: 0.063807\nCost after iteration 5000: 0.062631\nCost after iteration 5100: 0.061496\nCost after iteration 5200: 0.060400\nCost after iteration 5300: 0.059341\nCost after iteration 5400: 0.058317\nCost after iteration 5500: 0.057327\nCost after iteration 5600: 0.056368\nCost after iteration 5700: 0.055440\nCost after iteration 5800: 0.054541\nCost after iteration 5900: 0.053669\nCost after iteration 6000: 0.052824\nCost after iteration 6100: 0.052005\nCost after iteration 6200: 0.051209\nCost after iteration 6300: 0.050436\nCost after iteration 6400: 0.049686\nCost after iteration 6500: 0.048957\nCost after iteration 6600: 0.048248\nCost after iteration 6700: 0.047559\nCost after iteration 6800: 0.046888\nCost after iteration 6900: 0.046236\nCost after iteration 7000: 0.045601\nCost after iteration 7100: 0.044982\nCost after iteration 7200: 0.044380\nCost after iteration 7300: 0.043793\nCost after iteration 7400: 0.043220\nCost after iteration 7500: 0.042662\nCost after iteration 7600: 0.042118\nCost after iteration 7700: 0.041587\nCost after iteration 7800: 0.041069\nCost after iteration 7900: 0.040563\nCost after iteration 8000: 0.040069\nCost after iteration 8100: 0.039587\nCost after iteration 8200: 0.039116\nCost after iteration 8300: 0.038655\nCost after iteration 8400: 0.038205\nCost after iteration 8500: 0.037765\nCost after iteration 8600: 0.037335\nCost after iteration 8700: 0.036914\nCost after iteration 8800: 0.036502\nCost after iteration 8900: 0.036099\nCost after iteration 9000: 0.035704\nCost after iteration 9100: 0.035318\nCost after iteration 9200: 0.034940\nCost after iteration 9300: 0.034570\nCost after iteration 9400: 0.034207\nCost after iteration 9500: 0.033851\nCost after iteration 9600: 0.033503\nCost after iteration 9700: 0.033161\nCost after iteration 9800: 0.032826\nCost after iteration 9900: 0.032498\ntrain accuracy: 100.0 %\ntest accuracy: 70.0 %\n","name":"stdout"}],"source":"d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 10000, learning_rate = 0.005, print_cost = True)","execution_count":26},{"metadata":{"id":"3DDA04FA978A412C8292A7F23E8B3B56","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"**预期输出**: \ntrain accuracy: 99.04306220095694 %\ntest accuracy: 70.0 %"},{"metadata":{"id":"53B7B440A089483C8A0E191D79E1E128","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"**评价**：训练准确性接近100％。 这是一个很好的情况：你的模型正在运行，并且具有足够的容量来适合训练数据。 测试误差为68％。 考虑到我们使用的数据集很小，并且逻辑回归是线性分类器，对于这个简单的模型来说，这实际上还不错。 但请放心，下周你将建立一个更好的分类器！\n\n此外，你会看到该模型明显适合训练数据。 在本专业的稍后部分，你将学习如何减少过度拟合，例如通过使用正则化。 使用下面的代码（并更改`index`变量），你可以查看测试集图片上的预测。"},{"metadata":{"id":"1116FF91E7D24C4FBF1C65FE465D5CE8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"y = 1, you predicted that it is a \"cat\" picture.\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/1116FF91E7D24C4FBF1C65FE465D5CE8/qhvrqtrvd8.png\">"},"transient":{}}],"source":"# Example of a picture that was wrongly classified.\nindex = 1\nplt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\nprint (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(d[\"Y_prediction_test\"][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")","execution_count":27},{"metadata":{"id":"E83A4E2AB9FF48ED8F82458FE9449FB9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"让我们绘制损失函数和梯度吧。"},{"metadata":{"id":"4CE8150DF2E64E66B072729D0223A393","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/4CE8150DF2E64E66B072729D0223A393/qhvrrifawg.png\">"},"transient":{}}],"source":"# Plot learning curve (with costs)\ncosts = np.squeeze(d['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\nplt.show()","execution_count":28},{"metadata":{"id":"49C8F44478B14B0FAE38141E6B71B8E4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"**解释**：\n损失下降表明正在学习参数。 但是，你看到可以在训练集上训练更多模型。 尝试增加上面单元格中的迭代次数，然后重新运行这些单元格。 你可能会看到训练集准确性提高了，但是测试集准确性却降低了。 这称为过度拟合。"},{"metadata":{"id":"996500019E8B484599CFC0E57E055342","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"\n## 6- 进一步分析（可选练习）##\n\n祝贺你建立了第一个图像分类模型。 让我们对其进行进一步分析，并研究如何选择学习率$\\alpha$。"},{"metadata":{"id":"2BAA55389F304CF281BD34A0166FE719","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"#### 学习率的选择 ####\n\n**提醒**：\n为了使梯度下降起作用，你必须明智地选择学习率。 学习率$\\alpha$决定我们更新参数的速度。 如果学习率太大，我们可能会“超出”最佳值。 同样，如果太小，将需要更多的迭代才能收敛到最佳值。 这也是为什么调整好学习率至关重要。\n\n让我们将模型的学习曲线与选择的几种学习率进行比较。 运行下面的单元格。 这大约需要1分钟。 还可以尝试与我们初始化要包含的“ learning_rates”变量的三个值不同的值，然后看看会发生什么。"},{"metadata":{"id":"A8FF4BF175B8443382E259E88AA3DA99","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"learning rate is: 0.01\ntrain accuracy: 99.52153110047847 %\ntest accuracy: 68.0 %\n\n-------------------------------------------------------\n\nlearning rate is: 0.001\ntrain accuracy: 88.99521531100478 %\ntest accuracy: 64.0 %\n\n-------------------------------------------------------\n\nlearning rate is: 0.0001\ntrain accuracy: 68.42105263157895 %\ntest accuracy: 36.0 %\n\n-------------------------------------------------------\n\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/A8FF4BF175B8443382E259E88AA3DA99/qhvrtsrq93.png\">"},"transient":{}}],"source":"learning_rates = [0.01, 0.001, 0.0001]\nmodels = {}\nfor i in learning_rates:\n    print (\"learning rate is: \" + str(i))\n    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor i in learning_rates:\n    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n\nplt.ylabel('cost')\nplt.xlabel('iterations')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()","execution_count":31},{"metadata":{"id":"52765431AD714EE4BAE8978BC8C4C751","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"\t\t\n**解释**：\n* 不同的学习率会带来不同的损失，因此会有不同的预测结果。\n* 如果学习率太大（0.01），则成本可能会上下波动。 它甚至可能会**发散**（尽管在此示例中，使用0.01最终仍会以较高的损失值获得收益）。\n* 较低的损失并不意味着模型效果很好。当训练精度比测试精度高很多时，就会发生过拟合情况。\n* 在深度学习中，我们通常建议你：\n\t*      选择好能最小化损失函数的学习率。\n\t*      如果模型过度拟合，请使用其他方法来减少过度拟合。 （我们将在后面的教程中讨论。）\n"},{"metadata":{"id":"B552402AE5DB46AA83D99BC0A5D033EE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"\n## 7-使用自己的图像进行测试（可选练习）##\n\n祝贺你完成此作业。 你可以使用自己的图像并查看模型的预测输出。 要做到这一点：\n     1.单击此笔记本上部栏中的 \"File\"，然后单击\"Open\" 以在Coursera Hub上运行。\n     2.将图像添加到Jupyter Notebook的目录中，在\"images\"文件夹中\n     3.在以下代码中更改图像的名称\n     4.运行代码，检查算法是否正确（1 = cat，0 = non-cat）！\n"},{"metadata":{"id":"A488278DAB3846B6895CCB73635FE1A5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `imresize` is deprecated!\n`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\nUse Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n  \n","name":"stderr"},{"output_type":"stream","text":"y = 1.0, your algorithm predicts a \"cat\" picture.\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/A488278DAB3846B6895CCB73635FE1A5/qhvruajj2t.png\">"},"transient":{}}],"source":"## START CODE HERE ## (PUT YOUR IMAGE NAME) \n#my_image = \"cat_in_iran.jpg\"   # change this to the name of your image file \n## END CODE HERE ##\n\n# We preprocess the image to fit your algorithm.\nfname = '/home/kesci/input/deeplearningai17761/cat_in_iran.jpg'\nimage = np.array(plt.imread(fname))\nmy_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\nmy_predicted_image = predict(d[\"w\"], d[\"b\"], my_image)\n\nplt.imshow(image)\nprint(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")","execution_count":33},{"metadata":{"id":"91D40A762FB1423C9CD915F54652D5DA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"**此作业要记住的内容：**\n\n1. 预处理数据集很重要。\n1. 如何实现每个函数：initialize（），propagation（），optimize（），并用此构建一个model（）。\n1. 调整学习速率（这是“超参数”的一个示例）可以对算法产生很大的影响。 你将在本课程的稍后部分看到更多示例！"},{"metadata":{"id":"38A64F2ECAF948B0B4B5E815095C6364","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"最后，如果你愿意，我们邀请你在此笔记本上尝试其他操作。 在尝试任何操作之前，请确保你正确提交。 提交后，你可以学习了解的包括：\n*      发挥学习率和迭代次数\n*      尝试不同的初始化方法并比较结果\n*      测试其他预处理（将数据居中，或将每行除以其标准偏差）"},{"metadata":{"id":"B46C561E428B4DEC999D8E7E9BD85A21","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"参考书目：\n- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"},{"metadata":{"id":"A7C36AF2519F4E1DBA97D3ACB70B91B0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}