{"cells":[{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"93B116EFE6484C20952946844FCE93C4","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"# 深度神经网络应用--图像分类\n\n完成此作业后，你将完成第4周最后的编程任务，也是本课程最后的编程任务！\n\n你将使用在上一个作业中实现的函数来构建深层网络，并将其应用于分类cat图像和非cat图像。 希望你会看到相对于先前的逻辑回归实现的分类，准确性有所提高。\n\n**完成此任务后，你将能够：**\n- 建立深度神经网络并将其应用于监督学习。\n\n让我们开始吧！"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"0916416FD49A43ED8542C174AF0D6E0F","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"## 1 安装包"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"8F4AE8B11B9F401C87C7939B4B347BA9","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"让我们首先导入在作业过程中需要的所有软件包。\n- [numpy](www.numpy.org)是Python科学计算的基本包。\n- [matplotlib](http://matplotlib.org) 是在Python中常用的绘制图形的库。\n- **[h5py](http://www.h5py.org)**是一个常用的包，可以处理存储为H5文件格式的数据集\n- 这里最后通过[PIL](http://www.pythonware.com/products/pil/)和 [scipy](https://www.scipy.org/)用你自己的图片去测试模型效果。\n- dnn_app_utils提供了上一作业教程“逐步构建你的深度神经网络”中实现的函数。\n- np.random.seed（1）使所有随机函数调用保持一致。 这将有助于我们评估你的作业。"},{"metadata":{"id":"5D3D40E4FC804356AD6CABC5AEEDD335","slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"/home/kesci/input/deeplearning46278\n","name":"stdout"}],"source":"cd ../input/deeplearning46278","execution_count":1},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"EF8C6411BAC84C978C721EC5C427048C","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"import time\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nfrom dnn_app_utils_v2 import *\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\nnp.random.seed(1)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"73DF8D2D33C649038B4D7CE176DFBCEB","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 2 数据集\n\n你将使用与“用神经网络思想实现Logistic回归”（作业2）中相同的“cats vs non-cats”数据集。 此前你建立的模型在对猫和非猫图像进行分类时只有70％的准确率。 希望你的新模型会更好！\n\n**问题说明**：你将获得一个包含以下内容的数据集（\"data.h5\"）：\n     - 标记为cat（1）和非cat（0）图像的训练集**m_train**\n     - 标记为cat或non-cat图像的测试集**m_test**\n     - 每个图像的维度都为（num_px，num_px，3），其中3表示3个通道（RGB）。\n\n让我们熟悉一下数据集吧， 首先通过运行以下代码来加载数据。"},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"93F463BA184E4C6A8280F90C8AAAEAB3","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"D2644BB5DD5C44469B409B943333E3ED","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"运行以下代码以展示数据集中的图像。 通过更改索引，然后重新运行单元以查看其他图像。"},{"cell_type":"code","execution_count":4,"metadata":{"slideshow":{"slide_type":"slide"},"id":"553318068BDE4D5B9416E5CCD12EDF14","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"y = 1. It's a cat picture.\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/553318068BDE4D5B9416E5CCD12EDF14/qhm4l2un1r.png\">"},"transient":{}}],"source":"# Example of a picture\nindex = 7\nplt.imshow(train_x_orig[index])\nprint (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"},{"metadata":{"id":"FF1727840A04483598641A0660CCB491","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## h5数据的也许无法直接被观察，但是可以根据根据代码.shape推断其内部结构\n可知train_x_orig(图片个数..每张图片的详细信息)"},{"cell_type":"code","execution_count":5,"metadata":{"slideshow":{"slide_type":"slide"},"id":"198F951320E748498564EF1B6543F42F","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Number of training examples: 209\nNumber of testing examples: 50\nEach image is of size: (64, 64, 3)\ntrain_x_orig shape: (209, 64, 64, 3)\ntrain_y shape: (1, 209)\ntest_x_orig shape: (50, 64, 64, 3)\ntest_y shape: (1, 50)\n","name":"stdout"}],"source":"# Explore your dataset \nm_train = train_x_orig.shape[0]\nnum_px = train_x_orig.shape[1]\nm_test = test_x_orig.shape[0]\n\nprint (\"Number of training examples: \" + str(m_train))\nprint (\"Number of testing examples: \" + str(m_test))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_x_orig shape: \" + str(train_x_orig.shape))\nprint (\"train_y shape: \" + str(train_y.shape))\nprint (\"test_x_orig shape: \" + str(test_x_orig.shape))\nprint (\"test_y shape: \" + str(test_y.shape))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"80E1B2DB52004707BB9CD6449B8B0AB7","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"与往常一样，在将图像输入到网络之前，需要对图像进行重塑和标准化。 下面单元格给出了相关代码。\n\n![Image Name](https://cdn.kesci.com/upload/image/q1m5srjut8.png?imageView2/0/w/960/h/960)\n\n图1：图像转换为向量。"},{"metadata":{"id":"1EB8A6E0C20A459FA1131D8970608E62","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"通过$flatten$和转置使得每张图片的信息占一列，"},{"cell_type":"code","execution_count":5,"metadata":{"slideshow":{"slide_type":"slide"},"id":"33DFF49EA1324A098AEADDD0FAE3F849","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"train_x's shape: (12288, 209)\ntest_x's shape: (12288, 50)\n","name":"stdout"}],"source":"# Reshape the training and test examples \ntrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n\n# Standardize data to have feature values between 0 and 1.\ntrain_x = train_x_flatten/255.\ntest_x = test_x_flatten/255.\n\nprint (\"train_x's shape: \" + str(train_x.shape))\nprint (\"test_x's shape: \" + str(test_x.shape))\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"EBAF9F41059D4E7AABA465CC79D56DD4","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"$12,288$ 等于 $64 \\times 64 \\times 3$，这是图像重塑为向量的大小。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"BB08638F388445F583D0F53CED0F5233","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 3 模型的结构"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"5D51D7C83EFC48A9A782443DAABAD539","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"现在你已经熟悉了数据集，是时候建立一个深度神经网络来区分猫图像和非猫图像了。\n\n你将建立两个不同的模型：\n- 2层神经网络\n- L层深度神经网络\n\n然后，你将比较这些模型的性能，并尝试不同的$L$值。\n\n让我们看一下两种架构。\n\n### 3.1 2层神经网络\n\n![Image Name](https://cdn.kesci.com/upload/image/q1m4zl8yie.png?imageView2/0/w/960/h/960)\n\n图2：2层神经网络。\n\n该模型可以总结为：***INPUT -> LINEAR -> RELU -> LINEAR -> SIGMOID -> OUTPUT***\n\n图2的详细架构：\n- 输入维度为（64,64,3）的图像，将其展平为大小为$（12288,1）$的向量。\n- 相应的向量：$[x_0,x_1,...,x_{12287}]^T$乘以大小为$(n^{[1]}, 12288)$的权重矩阵$W^{[1]}$。\n- 然后添加一个偏差项并按照公式获得以下向量：$[a_0^{[1]}, a_1^{[1]},..., a_{n^{[1]}-1}^{[1]}]^T$。\n- 然后，重复相同的过程。\n- 将所得向量乘以$W^{[2]}$并加上截距（偏差）。\n- 最后，采用结果的sigmoid值。 如果大于0.5，则将其分类为猫。\n\n### 3.2 L层深度神经网络\n\n用上面的方式很难表示一个L层的深度神经网络。 这是一个简化的网络表示形式：\n\n![Image Name](https://cdn.kesci.com/upload/image/q1m50zsqx3.png?imageView2/0/w/960/h/960)\n\n图3：L层神经网络。\n该模型可以总结为：***[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID***\n\n图3的详细结构：\n- 输入维度为（64,64,3）的图像，将其展平为大小为$（12288,1）$的向量。\n- 相应的向量：$[x_0,x_1,...,x_{12287}]^T$乘以权重矩阵$W^{[1]}$，然后加上截距$b^{[1]}$，结果为线性单位。\n- 接下来计算获得的线性单元。对于每个$(W^{[l]}, b^{[l]})$，可以重复数次，具体取决于模型体系结构。\n- 最后，采用最终线性单位的sigmoid值。如果大于0.5，则将其分类为猫。\n\n### 3.3 通用步骤\n\n与往常一样，你将遵循深度学习步骤来构建模型：\n    1.初始化参数/定义超参数\n    2.循环num_iterations次：\n        a. 正向传播\n        b. 计算损失函数\n        C. 反向传播\n        d. 更新参数（使用参数和反向传播的梯度）\n    4.使用训练好的参数来预测标签\n\n现在让我们实现这两个模型！"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"51CFFB0C3A3D4149AF203F57A2A5B779","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 4 两层神经网络\n\n**问题**：使用你在上一个作业中实现的辅助函数来构建具有以下结构的2层神经网络：*LINEAR -> RELU -> LINEAR -> SIGMOID*，你可能需要的函数及其输入为：\n\n```python\ndef initialize_parameters(n_x, n_h, n_y):\n    ...\n    return parameters \ndef linear_activation_forward(A_prev, W, b, activation):\n    ...\n    return A, cache\ndef compute_cost(AL, Y):\n    ...\n    return cost\ndef linear_activation_backward(dA, cache, activation):\n    ...\n    return dA_prev, dW, db\ndef update_parameters(parameters, grads, learning_rate):\n    ...\n    return parameters\n```"},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"81A068D08317494AAA0FDBAAB079ED27","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"### CONSTANTS DEFINING THE MODEL ####\nn_x = 12288     # num_px * num_px * 3\nn_h = 7\nn_y = 1\nlayers_dims = (n_x, n_h, n_y)"},{"cell_type":"code","execution_count":15,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"300979A448B244819FB2031C85DC11A0","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: two_layer_model\n\ndef two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n    \"\"\"\n    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (n_x, number of examples)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- If set to True, this will print the cost every 100 iterations \n    \n    Returns:\n    parameters -- a dictionary containing W1, W2, b1, and b2\n    \"\"\"\n    \n    np.random.seed(1)\n    grads = {}\n    costs = []                              # to keep track of the cost\n    m = X.shape[1]                           # number of examples\n    (n_x, n_h, n_y) = layers_dims\n    \n    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n    ### START CODE HERE ### (≈ 1 line of code)\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    ### END CODE HERE ###\n    \n    # Get W1, b1, W2 and b2 from the dictionary parameters.\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n        ### START CODE HERE ### (≈ 2 lines of code)\n        A1, cache1 =linear_activation_forward(X, W1, b1, activation = \"relu\")\n        A2, cache2 = linear_activation_forward(A1, W2, b2, activation = \"sigmoid\")\n        ### END CODE HERE ###\n        \n        # Compute cost\n        ### START CODE HERE ### (≈ 1 line of code)\n        cost = compute_cost(A2, Y)\n        ### END CODE HERE ###\n        \n        # Initializing backward propagation\n        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n        \n        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n        ### START CODE HERE ### (≈ 2 lines of code)\n        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation = \"sigmoid\")\n        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation = \"relu\")\n        ### END CODE HERE ###\n        \n        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n        grads['dW1'] = dW1\n        grads['db1'] = db1\n        grads['dW2'] = dW2\n        grads['db2'] = db2\n        \n        # Update parameters.\n        ### START CODE HERE ### (approx. 1 line of code)\n        parameters = update_parameters(parameters, grads, learning_rate)\n        ### END CODE HERE ###\n\n        # Retrieve W1, b1, W2, b2 from parameters\n        W1 = parameters[\"W1\"]\n        b1 = parameters[\"b1\"]\n        W2 = parameters[\"W2\"]\n        b2 = parameters[\"b2\"]\n        \n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n       \n    # plot the cost\n\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"FFC663C5F6724BFAB270A5854B1F66BE","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"运行下面的单元格以训练模型参数。观察损失下降以查看模型是否运行。运行2500次迭代可能最多需要5分钟。 检查“迭代0次后的损失”是否与下面的预期输出匹配。如果没有，单击笔记本上方栏上的正方形（⬛）以停止单元格并尝试查找错误。"},{"cell_type":"code","execution_count":17,"metadata":{"slideshow":{"slide_type":"slide"},"id":"D4D174B35FFB4B54B721C9F612CEEB54","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Cost after iteration 0: 0.693049735659989\nCost after iteration 100: 0.6464320953428849\nCost after iteration 200: 0.6325140647912678\nCost after iteration 300: 0.6015024920354665\nCost after iteration 400: 0.5601966311605748\nCost after iteration 500: 0.515830477276473\nCost after iteration 600: 0.4754901313943325\nCost after iteration 700: 0.43391631512257495\nCost after iteration 800: 0.4007977536203886\nCost after iteration 900: 0.3580705011323798\nCost after iteration 1000: 0.3394281538366413\n","name":"stdout"}],"source":"parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"342B29DBCDFB4AFD8A1C586CBB1639E1","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\n**0次迭代后的损失**：\n0.6930497356599888\n**100次迭代后的损失**：\n0.6464320953428849\n...\n**2400次迭代后的损失**：\n0.048554785628770206"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"31E1BA88138E4AFA8E7015F2AA7923D9","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"你构建了向量化的实现！ 否则，可能需要花费10倍的时间来训练它。\n\n你可以使用训练好的参数对数据集中的图像进行分类。 要查看训练和测试集的预测结果，请运行以下单元格。"},{"cell_type":"code","execution_count":21,"metadata":{"slideshow":{"slide_type":"slide"},"id":"A06BB6BB83044CEC805C4907E29E1D98","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Accuracy: 0.9999999999999998\n","name":"stdout"}],"source":"predictions_train = predict(train_x, train_y, parameters)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"99A6D1049B2E4D2584B3948841CC5494","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nAccuracy: 0.9999999999999998"},{"cell_type":"code","execution_count":22,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"FF0B68AD72ED43DB8D127A0600177D39","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Accuracy: 0.72\n","name":"stdout"}],"source":"predictions_test = predict(test_x, test_y, parameters)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"D0E2EDC9DC8A404FAF0AAFBC2C7569F6","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nAccuracy: 0.72"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"BA7768EC76134119B94F738493256870","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**注意**：你可能会注意到，以较少的迭代次数（例如1500）运行模型可以使测试集具有更高的准确性。 这称为“尽早停止”，我们将在下一课程中讨论。 提前停止是防止过拟合的一种方法。\n\n恭喜你！看来你的2层神经网络的性能（72％）比逻辑回归实现（70％，第2周的作业）更好。 让我们看看使用$L$层模型是否可以做得更好。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"E92197ADD8AA4608875C0988D4D69BED","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 5 L层神经网络\n\n**问题**：使用之前实现的辅助函数来构建具有以下结构的$L$层神经网络：*[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID*。 你可能需要的函数及其输入为：\n\n```python\ndef initialize_parameters_deep(layer_dims):\n    ...\n    return parameters \ndef L_model_forward(X, parameters):\n    ...\n    return AL, caches\ndef compute_cost(AL, Y):\n    ...\n    return cost\ndef L_model_backward(AL, Y, caches):\n    ...\n    return grads\ndef update_parameters(parameters, grads, learning_rate):\n    ...\n    return parameters\n```"},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"197B1955E1C24F949092C4FEC6E13249","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"### CONSTANTS ###\nlayers_dims = [12288, 20, 7, 5, 1] #  5-layer model"},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"449F4AE86B4740CF9BB908B043493855","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: L_layer_model\n\ndef L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n    \n    Arguments:\n    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n\n    np.random.seed(1)\n    costs = []                         # keep track of cost\n    \n    # Parameters initialization.\n    ### START CODE HERE ###\n    parameters = initialize_parameters_deep(layers_dims)\n    ### END CODE HERE ###\n    \n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n\n        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n        ### START CODE HERE ### (≈ 1 line of code)\n        AL, caches = L_model_forward(X, parameters)\n        ### END CODE HERE ###\n        \n        # Compute cost.\n        ### START CODE HERE ### (≈ 1 line of code)\n        cost = compute_cost(AL, Y)\n        ### END CODE HERE ###\n    \n        # Backward propagation.\n        ### START CODE HERE ### (≈ 1 line of code)\n        grads = L_model_backward(AL, Y, caches)\n        ### END CODE HERE ###\n \n        # Update parameters.\n        ### START CODE HERE ### (≈ 1 line of code)\n        parameters = update_parameters(parameters, grads, learning_rate)\n        ### END CODE HERE ###\n                \n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n            \n    # plot the cost\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"0A2E30362AE141128F9EAEA39E81BC54","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"现在，你将训练5层的神经网络模型。\n\n运行下面的单元格以训练你的模型。 每次迭代的损失都应该降低。 运行2500次迭代最多可能需要5分钟。 检查“迭代0次后的损失”是否与下面的预期输出匹配，如果没有单击笔记本上方的正方形（⬛）以停止单元格并尝试查找错误。"},{"cell_type":"code","execution_count":14,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"219635A59A7D4AD18A1ABD86AD4B1D6F","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Cost after iteration 0: 0.771749\nCost after iteration 100: 0.672053\nCost after iteration 200: 0.648263\nCost after iteration 300: 0.611507\nCost after iteration 400: 0.567047\nCost after iteration 500: 0.540138\nCost after iteration 600: 0.527930\nCost after iteration 700: 0.465477\nCost after iteration 800: 0.369126\nCost after iteration 900: 0.391747\nCost after iteration 1000: 0.315187\nCost after iteration 1100: 0.272700\nCost after iteration 1200: 0.237419\nCost after iteration 1300: 0.199601\nCost after iteration 1400: 0.189263\nCost after iteration 1500: 0.161189\nCost after iteration 1600: 0.148214\nCost after iteration 1700: 0.137775\nCost after iteration 1800: 0.129740\nCost after iteration 1900: 0.121225\nCost after iteration 2000: 0.113821\nCost after iteration 2100: 0.107839\nCost after iteration 2200: 0.102855\nCost after iteration 2300: 0.100897\nCost after iteration 2400: 0.092878\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/219635A59A7D4AD18A1ABD86AD4B1D6F/q1m662cnsn.png\">"},"transient":{}}],"source":"parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"DDA53A00E5D94824831B9A8A84059B9D","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\n**0次迭代后的损失**：\n0.771749\n**100次迭代后的损失**：\n0.672053\n...\n**2400次迭代后的损失**：\n0.092878"},{"cell_type":"code","execution_count":26,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"699CF41352B3418EA61077A49F7FCF74","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Accuracy: 0.9999999999999998\n","name":"stdout"}],"source":"pred_train = predict(train_x, train_y, parameters)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"6BC34B97EB21483980AF073C307EA686","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nAccuracy: 0.985645933014"},{"cell_type":"code","execution_count":27,"metadata":{"slideshow":{"slide_type":"slide"},"id":"2B0EE27E9AAC45A58DCBB58CEAFF2DB7","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Accuracy: 0.72\n","name":"stdout"}],"source":"pred_test = predict(test_x, test_y, parameters)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"828D0034AC8045CFB75E1043DBF88811","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nAccuracy: 0.8"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"2B8BCC56FAAB4D66894A55836E463B17","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"Congrats! It seems that your 5-layer neural network has better performance (80%) than your 2-layer neural network (72%) on the same test set. \n\nThis is good performance for this task. Nice job! \n\nThough in the next course on \"Improving deep neural networks\" you will learn how to obtain even higher accuracy by systematically searching for better hyperparameters (learning_rate, layers_dims, num_iterations, and others you'll also learn in the next course). \n\nGood！在相同的测试集上，你的5层的神经网络似乎比2层神经网络具有更好的性能（80％）。做的好！\n\n在下一作业教程“改善深度神经网络”中，你将学习如何通过系统地匹配更好的超参数（学习率，层数，迭代次数以及下一门课程中还将学习到的其他参数）来获得更高的准确性。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"FFF8F483E34C40C3BF6F8A4EFCFDB991","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"## 6 结果分析\n\n首先，让我们看一下L层模型标记错误的一些图像。 这将显示一些分类错误的图像。"},{"cell_type":"code","execution_count":28,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"62FC65339D20404D998B3D5D547CD630","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 2880x2880 with 14 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/62FC65339D20404D998B3D5D547CD630/qhm7qejorp.png\">"},"transient":{}}],"source":"print_mislabeled_images(classes, test_x, test_y, pred_test)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"E8B6163E1F5048779B22BE8654C9CB01","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**该模型在表现效果较差的的图像包括：**\n- 猫身处于异常位置\n- 图片背景与猫颜色类似\n- 猫的种类和颜色稀有\n- 相机角度\n- 图片的亮度\n- 比例变化（猫的图像很大或很小）"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"CC4730A44C3A4565A807DF3768F63D6B","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"## 7 使用你自己的图像进行测试（可选练习）##\n\n祝贺你完成此作业。 你可以使用自己的图像测试并查看模型的输出。要做到这一点：\n     1.单击此笔记本上部栏中的“File”，然后单击“Open”以在Coursera Hub上运行。\n     2.将图像添加到Jupyter Notebook的目录中，在“images”文件夹中\n     3.在以下代码中更改图像的名称\n     4.运行代码，检查算法是否正确（1 = cat，0 = non-cat）！"},{"cell_type":"code","execution_count":29,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"A870F4B3829F4FCB8C71B2B63EFA7849","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'num_px' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-a84293591123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmy_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_px\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_px\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_px\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_px\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmy_predicted_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_label_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'num_px' is not defined"]}],"source":"## START CODE HERE ##\nmy_image = \"my_image.jpg\" # change this to the name of your image file \nmy_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\n## END CODE HERE ##\n\nfname = my_image\nimage = np.array(plt.imread(fname))\nmy_image = np.array(Image.fromarray(image).resize(size=(num_px,num_px))).reshape((num_px*num_px*3,1))\nmy_predicted_image = predict(my_image, my_label_y, parameters)\n\nplt.imshow(image)\nprint (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"BB0F049316F542B394CFAB119FB51F17","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**参考**：\n- for auto-reloading external module: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython"},{"metadata":{"id":"EE488EB54CEF4F0B8B46C1238BFD9674","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}