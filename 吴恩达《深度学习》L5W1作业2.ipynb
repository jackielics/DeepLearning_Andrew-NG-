{"cells":[{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"6BBBF62F1F3E49939E7307018224A1EC","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"# 字母级语言模型 - Dinosaurus land\n\n欢迎来到恐龙大陆！ 6500万年前，恐龙就已经存在，并且在该作业下它们又回来了。假设你负责一项特殊任务，领先的生物学研究人员正在创造新的恐龙品种，并计划将它们带入地球，而你的工作就是为这些新恐龙起名字。如果恐龙不喜欢它的名字，它可能会变得疯狂，所以需要明智地选择！\n\n![Image Name](https://cdn.kesci.com/upload/image/q1z6pa3s2s.jpg?imageView2/0/w/960/h/960)\n\n幸运的是，你掌握了深度学习的一些知识，你将使用它来节省时间。你的助手已收集了他们可以找到的所有恐龙名称的列表，并将其编译到此[dataset](dinos.txt)中。（请单击上一个链接查看）要创建新的恐龙名称，你将构建一个字母级语言模型来生成新名称。你的算法将学习不同的名称模式，并随机生成新名称。希望该算法可以使你和你的团队免受恐龙的愤怒！\n\n完成此作业，你将学习：\n\n- 如何存储文本数据以供RNN使用\n- 如何在每个时间步采样预测并将其传递给下一个RNN单元以合成数据\n- 如何建立一个字母级的文本生成循环神经网络\n- 为什么梯度裁剪很重要\n\n我们将从加载`rnn_utils`中为你提供的一些函数开始。具体来说，你可以访问诸如`rnn_forward`和`rnn_backward`之类的函数，这些函数与你在上一个作业中实现的函数等效。"},{"metadata":{"id":"E90DC2A1FB4945E08B9A28ADDB772DEF","slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"/home/kesci/input/deeplearning133797\n","name":"stdout"}],"source":"cd /home/kesci/input/deeplearning133797","execution_count":1},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"E6D75B35B7B548F1A93C8715E0A1AB90","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"import numpy as np\nfrom utils import *\nimport random\nfrom random import shuffle"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"F3378C7594044B699241594540C6CCA5","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 1 问题陈述\n\n### 1.1 数据集和预处理\n\n运行以下单元格以读取包含恐龙名称的数据集，创建唯一字符列表（例如a-z），并计算数据集和词汇量。"},{"cell_type":"code","execution_count":3,"metadata":{"slideshow":{"slide_type":"slide"},"id":"0516050D913A4A3E9CC425BD7F800FE7","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"There are 19909 total characters and 27 unique characters in your data.\n","name":"stdout"}],"source":"data = open('dinos.txt', 'r').read()\ndata= data.lower()\nchars = list(set(data))\ndata_size, vocab_size = len(data), len(chars)\nprint('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"9FBC4AD010D845C3B8FBA5D540AFC16F","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"这些字符是a-z（26个字符）加上“\\n”（换行符），在此作业中，其作用类似于我们在讲座中讨论的`<EOS>`（句子结尾）标记，仅在此处表示恐龙名称的结尾，而不是句子的结尾。在下面的单元格中，我们创建一个python字典（即哈希表），以将每个字符映射为0-26之间的索引。我们还创建了第二个python字典，该字典将每个索引映射回对应的字符。这将帮助你找出softmax层的概率分布输出中哪个索引对应于哪个字符。下面的`char_to_ix`和`ix_to_char`是python字典。"},{"cell_type":"code","execution_count":4,"metadata":{"slideshow":{"slide_type":"slide"},"id":"9FF0BE1AA75C41F4AE087056B7EA2725","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n","name":"stdout"}],"source":"char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\nix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\nprint(ix_to_char)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"01718F7A860242E7B9F929E1AAD535D2","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.2 模型概述\n\n你的模型将具有以下结构：\n\n- 初始化参数\n- 运行优化循环\n\t*     正向传播以计算损失函数\n\t*     反向传播以计算相对于损失函数的梯度\n\t*     剪裁梯度以避免梯度爆炸\n\t*     使用梯度下降方法更新参数。\n- 返回学习的参数\n\n![Image Name](https://cdn.kesci.com/upload/image/q1z6qwq7fq.png?imageView2/0/w/960/h/960)\n\n**图1**：循环神经网络，类似于你在上一个笔记本“手把手实现循环神经网络”中构建的内容。\n\n在每个时间步，RNN都会根据给定的先前字符来预测下一个字符。数据集$X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$是训练集中的字符列表，而$Y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$使得每个时间步$t$，我们有$y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$。\n "},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"95E65EAF900E434B8C8768F590DD154D","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 2 构建模型模块\n\n在这一部分中，你将构建整个模型的两个重要模块：\n- 梯度裁剪：避免梯度爆炸\n- 采样：一种用于生成字符的技术\n\n然后，你将应用这两个函数来构建模型。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"CC056068C0154F8A98982F7A3A03DD80","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 2.1 在优化循环中裁剪梯度\n\n在本节中，你将实现在优化循环中调用的`clip`函数。回想一下，你的总体循环结构通常由正向传播，损失计算，反向传播和参数更新组成。在更新参数之前，你将在需要时执行梯度裁剪，以确保你的梯度不会“爆炸”，这意味着要采用很大的值。\n\n在下面的练习中，你将实现一个函数`clip`，该函数接收梯度字典，并在需要时返回裁剪后的梯度。梯度裁剪有多种方法。我们将使用简单的按元素裁剪程序，其中将梯度向量的每个元素裁剪为位于范围[-N，N]之间。通常，你将提供一个`maxValue`（例如10）。在此示例中，如果梯度向量的任何分量大于10，则将其设置为10；并且如果梯度向量的任何分量小于-10，则将其设置为-10。如果介于-10和10之间，则将其保留。\n\n![Image Name](https://cdn.kesci.com/upload/image/q1z6rigbi7.png?imageView2/0/w/960/h/960)\n\n**图2**：在网络遇到轻微的“梯度爆炸”的情况下，使用与不使用梯度裁剪的梯度下降对比。\n\n**练习**：实现以下函数以返回字典`gradients`的裁剪梯度。你的函数接受最大阈值，并返回裁剪后的梯度。你可以查看此[hint](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.clip.html)，以获取有关如何裁剪numpy的示例。你将需要使用参数`out = ...`。\n"},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"AE57C340BB8040B88978A60DEEE8362F","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"### GRADED FUNCTION: clip\n\ndef clip(gradients, maxValue):\n    '''\n    Clips the gradients' values between minimum and maximum.\n    \n    Arguments:\n    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n    \n    Returns: \n    gradients -- a dictionary with the clipped gradients.\n    '''\n    \n    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n   \n    ### START CODE HERE ###\n    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)\n    for gradient in [dWax, dWaa, dWya, db, dby]:\n        np.clip(gradient,-maxValue , maxValue, out=gradient)\n    ### END CODE HERE ###\n    \n    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n    \n    return gradients"},{"cell_type":"code","execution_count":6,"metadata":{"slideshow":{"slide_type":"slide"},"id":"63E565ABC98F43D7994196A4F33AFB2B","collapsed":false,"scrolled":true,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"gradients[\"dWaa\"][1][2] = 10.0\ngradients[\"dWax\"][3][1] = -10.0\ngradients[\"dWya\"][1][2] = 0.2971381536101662\ngradients[\"db\"][4] = [10.]\ngradients[\"dby\"][1] = [8.45833407]\n","name":"stdout"}],"source":"np.random.seed(3)\ndWax = np.random.randn(5,3)*10\ndWaa = np.random.randn(5,5)*10\ndWya = np.random.randn(2,5)*10\ndb = np.random.randn(5,1)*10\ndby = np.random.randn(2,1)*10\ngradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\ngradients = clip(gradients, 10)\nprint(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\nprint(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\nprint(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\nprint(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\nprint(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"1C12F3D4B09B4D44946B4EBED1B5328C","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出:**\ngradients[\"dWaa\"][1][2] = 10.0\ngradients[\"dWax\"][3][1] = -10.0\ngradients[\"dWya\"][1][2] = 0.2971381536101662\ngradients[\"db\"][4] = [10.]\ngradients[\"dby\"][1] = [8.45833407]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"4AC1F1B1C9A14BABAFFFF1BE77EF78D6","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 2.2 采样\n\n现在假设你的模型已经训练好。你想生成新文本（字符）。下图说明了生成过程：\n![Image Name](https://cdn.kesci.com/upload/image/q1z6setajw.png?imageView2/0/w/960/h/960)\n**图3**：在此图中，我们假设模型已经训练好。我们在第一步中传入$x^{\\langle 1\\rangle} = \\vec{0}$，然后让网络一次采样一个字符。\n\n**练习**：实现以下的`sample`函数来采样字母。你需要执行4个步骤：\n\n- **步骤1**：将第一个\"dummy\"输入$x^{\\langle 1 \\rangle} = \\vec{0}$（零向量）传递给网络。这是我们生成任意字母之前的默认输入。我们还设置$a^{\\langle 0 \\rangle} = \\vec{0}$。\n\n- **步骤2**：执行向正向传播的步骤，即可获得$a^{\\langle 1 \\rangle}$ and $\\hat{y}^{\\langle 1 \\rangle}$。以下是等式：\n\n$$\na^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}\n$$\n\n$$\nz^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}\n$$\n\n$$\n\\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}\n$$\n\n注意$\\hat{y}^{\\langle t+1 \\rangle }$是一个（softmax）概率向量（其条目在0到1之间且总和为1）。$\\hat{y}^{\\langle t+1 \\rangle}_i$ 表示由\"i\"索引的字符是下一个字符的概率。我们提供了一个`softmax（）`函数供你使用。\n\n- **步骤3**：执行采样：根据$\\hat{y}^{\\langle t+1 \\rangle }$指定的概率分布，选择下一个字符的索引。这意味着，如果$\\hat{y}^{\\langle t+1 \\rangle }_i = 0.16$，你将以16％的概率选择索引\"i\"。要实现它，你可以使用[`np.random.choice`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html)。\n\n以下是一个使用`np.random.choice()`的例子：\n```python\nnp.random.seed(0)\np = np.array([0.1, 0.0, 0.7, 0.2])\nindex = np.random.choice([0, 1, 2, 3], p = p.ravel())\n```\n这意味着你将根据分布选择`index`：\n$P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$。\n\n- **步骤4**：要在`sample()`中实现的最后一步是覆盖变量`x`，该变量当前存储$x^{\\langle t \\rangle }$，其值为$x^{\\langle t + 1 \\rangle }$。通过创建与预测字符相对应的独热向量以表示$x^{\\langle t + 1 \\rangle }$。然后，你将在步骤1中前向传播$x^{\\langle t + 1 \\rangle }$，并继续重复此过程，直到获得“\\n”字符，表明你已经到达恐龙名称的末尾。"},{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"BB3B7AA5B02A44E7894C205F5FBEE4A0","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: sample\n\ndef sample(parameters, char_to_ix, seed):\n    \"\"\"\n    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n\n    Arguments:\n    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n    char_to_ix -- python dictionary mapping each character to an index.\n    seed -- used for grading purposes. Do not worry about it.\n\n    Returns:\n    indices -- a list of length n containing the indices of the sampled characters.\n    \"\"\"\n    \n    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n    vocab_size = by.shape[0]\n    n_a = Waa.shape[1]\n    \n    ### START CODE HERE ###\n    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)\n    x = np.zeros((vocab_size,1))\n    # Step 1': Initialize a_prev as zeros (≈1 line)\n    a_prev = np.zeros((n_a,1))\n    \n    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)\n    indices = []\n    \n    # Idx is a flag to detect a newline character, we initialize it to -1\n    idx = -1 \n    \n    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n    # trained model), which helps debugging and prevents entering an infinite loop. \n    counter = 0\n    newline_character = char_to_ix['\\n']\n    \n    while (idx != newline_character and counter != 50):\n        \n        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n        a = np.tanh(np.dot(Wax,x)+np.dot(Waa,a_prev)+b)\n        z = np.dot(Wya,a)+by\n        y = softmax(z)\n        \n        # for grading purposes\n        np.random.seed(counter+seed) \n        \n        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n        idx = np.random.choice(range(len(y)),p=y.ravel())\n\n\n        # Append the index to \"indices\"\n        indices.append(idx)\n        \n        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n        x = np.zeros((vocab_size,1))\n        x[idx] = 1\n        \n        # Update \"a_prev\" to be \"a\"\n        a_prev = a\n        \n        # for grading purposes\n        seed += 1\n        counter +=1\n        \n    ### END CODE HERE ###\n\n    if (counter == 50):\n        indices.append(char_to_ix['\\n'])\n    \n    return indices"},{"cell_type":"code","execution_count":8,"metadata":{"slideshow":{"slide_type":"slide"},"id":"E208DEFC676143368B691D2098268AB1","collapsed":false,"scrolled":true,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Sampling:\nlist of sampled indices: [18, 2, 26, 0]\nlist of sampled characters: ['r', 'b', 'z', '\\n']\n","name":"stdout"}],"source":"np.random.seed(2)\nn, n_a = 20, 100\na0 = np.random.randn(n_a, 1)\ni0 = 1 # first character is ix_to_char[i0]\nWax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\nb, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\nparameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n\n\nindices = sample(parameters, char_to_ix, 0)\nprint(\"Sampling:\")\nprint(\"list of sampled indices:\", indices)\nprint(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"91DC31494DA2475188B7B426F944A205","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出:**\nSampling:\nlist of sampled indices: [18, 2, 26, 0]\nlist of sampled characters: ['r', 'b', 'z', '\\n']"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"C242E3A05F0942578F9141B58E14273F","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 3 建立语言模型\n\n现在是时候建立用于文字生成的字母级语言模型了。\n\n### 3.1 梯度下降\n\n在本部分中，你将实现一个函数，该函数执行随机梯度下降的一个步骤（梯度裁剪）。你将一次查看一个训练示例，因此优化算法为随机梯度下降。提醒一下，以下是RNN常见的优化循环的步骤：\n\n- 通过RNN正向传播以计算损失\n- 随时间反向传播以计算相对于参数的损失梯度\n- 必要时裁剪梯度\n- 使用梯度下降更新参数\n\n**练习**：实现此优化过程（随机梯度下降的一个步骤）。\n\n我们为你提供了以下函数：\n\n```python\ndef rnn_forward(X, Y, a_prev, parameters):\n    \"\"\" Performs the forward propagation through the RNN and computes the cross-entropy loss.\n    It returns the loss' value as well as a \"cache\" storing values to be used in the backpropagation.\"\"\"\n    ....\n    return loss, cache\n    \ndef rnn_backward(X, Y, parameters, cache):\n    \"\"\" Performs the backward propagation through time to compute the gradients of the loss with respect\n    to the parameters. It returns also all the hidden states.\"\"\"\n    ...\n    return gradients, a\n\ndef update_parameters(parameters, gradients, learning_rate):\n    \"\"\" Updates parameters using the Gradient Descent Update Rule.\"\"\"\n    ...\n    return parameters\n```"},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"276309E8623A48C98C351E67AF5E3346","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: optimize\n\ndef optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n    \"\"\"\n    Execute one step of the optimization to train the model.\n    \n    Arguments:\n    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n    Y -- list of integers, exactly the same as X but shifted one index to the left.\n    a_prev -- previous hidden state.\n    parameters -- python dictionary containing:\n                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n                        b --  Bias, numpy array of shape (n_a, 1)\n                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n    learning_rate -- learning rate for the model.\n    \n    Returns:\n    loss -- value of the loss function (cross-entropy)\n    gradients -- python dictionary containing:\n                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n                        db -- Gradients of bias vector, of shape (n_a, 1)\n                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n    \"\"\"\n    \n    ### START CODE HERE ###\n    \n    # Forward propagate through time (≈1 line)\n    loss, cache = rnn_forward(X,Y,a_prev,parameters)\n    \n    # Backpropagate through time (≈1 line)\n    gradients, a = rnn_backward(X,Y,parameters,cache)\n    \n    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n    gradients = clip(gradients,5)\n    \n    # Update parameters (≈1 line)\n    parameters = update_parameters(parameters,gradients,learning_rate)\n    \n    ### END CODE HERE ###\n    \n    return loss, gradients, a[len(X)-1]"},{"cell_type":"code","execution_count":10,"metadata":{"slideshow":{"slide_type":"slide"},"id":"C3CBBC69541E4770A432971E988626AC","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Loss = 126.50397572165363\ngradients[\"dWaa\"][1][2] = 0.19470931534719205\nnp.argmax(gradients[\"dWax\"]) = 93\ngradients[\"dWya\"][1][2] = -0.007773876032003275\ngradients[\"db\"][4] = [-0.06809825]\ngradients[\"dby\"][1] = [0.01538192]\na_last[4] = [-1.]\n","name":"stdout"}],"source":"np.random.seed(1)\nvocab_size, n_a = 27, 100\na_prev = np.random.randn(n_a, 1)\nWax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\nb, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\nparameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\nX = [12,3,5,11,22,3]\nY = [4,14,11,22,25, 26]\n\nloss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\nprint(\"Loss =\", loss)\nprint(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\nprint(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\nprint(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\nprint(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\nprint(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\nprint(\"a_last[4] =\", a_last[4])"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"152447EF842D4177A710D8C0FDE67395","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出:**\nLoss = 126.50397572165363\ngradients[\"dWaa\"][1][2] = 0.19470931534719205\nnp.argmax(gradients[\"dWax\"]) = 93\ngradients[\"dWya\"][1][2] = -0.007773876032003275\ngradients[\"db\"][4] = [-0.06809825]\ngradients[\"dby\"][1] = [0.01538192]\na_last[4] = [-1.]"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"1A23E25257704C0293B409768C838028","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 3.2 训练模型"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"BB9392AAAC5C431286B1FF5A34266A8C","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"给定恐龙名称数据集，我们将数据集的每一行（一个名称）用作一个训练示例。每100步随机梯度下降，你将抽样10个随机选择的名称，以查看算法的运行情况。请记住要对数据集进行混洗，以便随机梯度下降以随机顺序访问示例。\n\n**练习**：按照说明进行操作并实现`model()`。当`examples [index]`包含一个恐龙名称（字符串）时，创建示例（X，Y），可以使用以下方法：\n```python\n        index = j % len(examples)\n        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n        Y = X[1:] + [char_to_ix[\"\\n\"]]\n```\n注意，我们使用：`index= j % len(examples)`，其中`j = 1....num_iterations`，以确保`examples [index]`始终是有效的语句（`index`小于`len(examples)`）。\n`X`的第一个条目为None将被`rnn_forward()`解释为设置$x^{\\langle 0 \\rangle} = \\vec{0}$。此外，这确保了`Y`等于`X`，但向左移动了一步，并附加了“\\n”以表示恐龙名称的结尾。"},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"F7FA32E0BA294A038707B136CBA5BEA2","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: model\n\ndef model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n    \"\"\"\n    Trains the model and generates dinosaur names. \n    \n    Arguments:\n    data -- text corpus\n    ix_to_char -- dictionary that maps the index to a character\n    char_to_ix -- dictionary that maps a character to an index\n    num_iterations -- number of iterations to train the model for\n    n_a -- number of units of the RNN cell\n    dino_names -- number of dinosaur names you want to sample at each iteration. \n    vocab_size -- number of unique characters found in the text, size of the vocabulary\n    \n    Returns:\n    parameters -- learned parameters\n    \"\"\"\n    \n    # Retrieve n_x and n_y from vocab_size\n    n_x, n_y = vocab_size, vocab_size\n    \n    # Initialize parameters\n    parameters = initialize_parameters(n_a, n_x, n_y)\n    \n    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n    loss = get_initial_loss(vocab_size, dino_names)\n    \n    # Build list of all dinosaur names (training examples).\n    with open(\"dinos.txt\") as f:\n        examples = f.readlines()\n    examples = [x.lower().strip() for x in examples]\n    \n    # Shuffle list of all dinosaur names\n    shuffle(examples)\n    \n    # Initialize the hidden state of your LSTM\n    a_prev = np.zeros((n_a, 1))\n    \n    # Optimization loop\n    for j in range(num_iterations):\n        \n        ### START CODE HERE ###\n        \n        # Use the hint above to define one training example (X,Y) (≈ 2 lines)\n        index = j%len(examples)\n        X = [None] + [char_to_ix[ch] for ch in examples[index]]\n        Y = X[1:] + [char_to_ix[\"\\n\"]]\n        \n        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n        # Choose a learning rate of 0.01\n        curr_loss, gradients, a_prev = optimize(X,Y,a_prev,parameters,learning_rate=0.01)  \n        \n        ### END CODE HERE ###\n        \n        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n        loss = smooth(loss, curr_loss)\n\n        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n        if j % 2000 == 0:\n            \n            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n            \n            # The number of dinosaur names to print\n            seed = 0\n            for name in range(dino_names):\n                \n                # Sample indices and print them\n                sampled_indices = sample(parameters, char_to_ix, seed)\n                print_sample(sampled_indices, ix_to_char)\n                \n                seed += 1  # To get the same result for grading purposed, increment the seed by one. \n      \n            print('\\n')\n        \n    return parameters"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"1FF73369A7074CF288C3C95829B6BE2C","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"运行以下单元格，你应该观察到模型在第一次迭代时输出看似随机的字符。经过数千次迭代后，你的模型应该学会生成看起来合理的名称。"},{"cell_type":"code","execution_count":12,"metadata":{"scrolled":true,"slideshow":{"slide_type":"slide"},"id":"24653FEAC8A44E138B4CEEE32B5B2ADA","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Iteration: 0, Loss: 23.070859\n\nNkzxwtdmfqoeyhsqwasjkjvu\nKneb\nKzxwtdmfqoeyhsqwasjkjvu\nNeb\nZxwtdmfqoeyhsqwasjkjvu\nEb\nXwtdmfqoeyhsqwasjkjvu\n\n\nIteration: 2000, Loss: 27.986441\n\nLhusluinasaus\nHiba\nHvrosaurus\nLacalosalapsauruskolaybhis\nXusganclolveros\nA\nTos\n\n\nIteration: 4000, Loss: 25.995355\n\nOnytosaurus\nKlecahus\nLytosaurus\nOia\nWusmcheopeuroshaschitochushelamalue\nCa\nToraperohurus\n\n\nIteration: 6000, Loss: 24.776055\n\nPhyusodonlonunosiargilus\nLlecakptia\nLyussaurus\nPecahosaperthuranus\nXustaokoraurus\nDa\nTrrasaurus\n\n\nIteration: 8000, Loss: 24.122363\n\nNhyusiandopeunoshapkoptoa\nKlecaisaurus\nLwusoceosaurus\nNdaaerka\nXusraohoraviraucorantrantixalapelus\nDaaerokachusheiivia\nTrraohopeurosarres\n\n\nIteration: 10000, Loss: 23.825352\n\nNiwussaurus\nKieeahosaurus\nLustreolopeus\nNecberte\nXussaurosaurus\nDaberteg\nTroenesaurus\n\n\nIteration: 12000, Loss: 23.432078\n\nNiwusialfsegyhustatloptochustnhaleitanbaphaer\nKlecaertegaosaurus\nKutrochesteurortathonnochustomalelugamang\nNgcagosaurus\nXustameptius\nDabbosaurus\nTosaurus\n\n\nIteration: 14000, Loss: 23.387299\n\nNhysscanborex\nInee\nIusosaurus\nNecagosaurus\nXprodonophus\nCa\nTrodonophus\n\n\nIteration: 16000, Loss: 23.158160\n\nNhyusia\nLicaaisil\nLustolmashauhorratosaurus\nOla\nXstreolosaurus\nDaalosaurus\nTrocheosaurus\n\n\nIteration: 18000, Loss: 23.023754\n\nOntosaurus\nLicechosaurus\nLustononio\nOncalosaurus\nXstononiobus\nDaakosaurus\nToraposaurus\n\n\nIteration: 20000, Loss: 22.963849\n\nPhyusbceismeulosparnimus\nLideberon\nLustrhong\nPadagosaurus\nXusphelosaurus\nEdalosaurus\nTrodonis\n\n\nIteration: 22000, Loss: 22.914431\n\nOnyxinaphosaurus\nKcacaitia\nKusssaurus\nOna\nYusianguravarisaurus\nCa\nTrocemptotaururus\n\n\nIteration: 24000, Loss: 22.790483\n\nNgyxnmangnictitrs\nKlacalosaurus\nKutqpangosaurus\nNabadps\nXusmandosaurus\nDaadosaurus\nTorandos\n\n\nIteration: 26000, Loss: 22.786011\n\nNgytosaurus\nJiccalosaurus\nKuspramanopuosaurus\nNec\nXprocheptes\nCa\nTorapiosaurus\n\n\nIteration: 28000, Loss: 22.737789\n\nNixrsialgosaurus\nLlecalosaurus\nLussperatops\nNeeahosaurus\nXushanfosaurus\nDaaisul\nTrodon\n\n\nIteration: 30000, Loss: 22.673867\n\nMawtosaurus\nInga\nJusspanchodus\nMacaesmekanosaurus\nXosiangosaurus\nDaalosaurus\nTorbikosaurus\n\n\nIteration: 32000, Loss: 22.391556\n\nPhustonghoratermteranosatrus\nLelbakus\nMusurepiordus\nPehaeropeltylurenus\nXusterissaurus\nElaeosaurus\nTorclisaurus\n\n\nIteration: 34000, Loss: 22.615709\n\nPettosaurus\nLidacerosaurus\nLurosaurus\nPaiaeosaurus\nXuspanasaurus\nDabasoma\nTrodonsbhunosianeosaurus\n\n\n","name":"stdout"}],"source":"parameters = model(data, ix_to_char, char_to_ix)"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"662B6E09341C495F8FB8A188E326EE42","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 结论\n你可以看到，在训练即将结束时，你的算法已开始生成合理的恐龙名称。刚开始时，它会生成随机字符，但是到最后，你会看到恐龙名字的结尾很酷。运行该算法更长时间，并调整超参数来看看是否可以获得更好的结果。我们的实现产生了一些非常酷的名称，例如“maconucon”，“marloralus”和“macingsersaurus”。你的模型还有望了解到恐龙名称往往以`saurus`，`don`，`aura`，`tor`等结尾。\n\n如果你的模型生成了一些不酷的名字，请不要完全怪罪模型-并非所有实际的恐龙名字听起来都很酷。（例如，dromaeosauroides是实际存在的恐龙名称，并且也在训练集中。）但是此模型应该给你提供了一组可以从中挑选的候选名字！\n\n该作业使用了相对较小的数据集，因此你可以在CPU上快速训练RNN。训练英语模型需要更大的数据集，并且通常需要更多的计算，在GPU上也要运行多个小时。我们使用恐龙的名字已经有一段时间了，到目前为止，我们最喜欢的名字是great, undefeatable,且fierce的：Mangosaurus!\n![Image Name](https://cdn.kesci.com/upload/image/q1z6w2f3dj.jpeg?imageView2/0/w/960/h/960)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"C9EFBEE0E74F4740997E6760BCC8D32B","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 4 像莎士比亚一样创作\n\n该笔记本的其余部分是可选的，尚未评分，但我们希望你都尝试做一下，因为它非常有趣且内容丰富。\n\n一个类似（但更复杂）的任务是生成莎士比亚诗歌。无需从恐龙名称的数据集中学习，而是使用莎士比亚诗歌集。使用LSTM单元，你可以学习跨文本中许多字符的长期依赖关系。例如，某个字符出现在序列的某个地方可能会影响序列后面的其他字符。这些长期依赖关系对于恐龙名称来说不太重要，因为它们的名称很短。\n![Image Name](https://cdn.kesci.com/upload/image/q1z6wks037.jpg?imageView2/0/w/960/h/960)\n\n让我们成为诗人！\n\n我们已经用Keras实现了莎士比亚诗歌生成器。运行以下单元格以加载所需的软件包和模型。这可能需要几分钟的时间。"},{"cell_type":"code","execution_count":13,"metadata":{"slideshow":{"slide_type":"slide"},"id":"95013944E80A431D8AF23EC779AAB2BA","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"Loading text data...\nCreating training set...\nnumber of training examples: 31412\nVectorizing training set...\nLoading model...\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/keras/engine/saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n  warnings.warn('Error in loading the saved optimizer '\n","name":"stderr"}],"source":"from __future__ import print_function\nfrom keras.callbacks import LambdaCallback\nfrom keras.models import Model, load_model, Sequential\nfrom keras.layers import Dense, Activation, Dropout, Input, Masking\nfrom keras.layers import LSTM\nfrom keras.utils.data_utils import get_file\nfrom keras.preprocessing.sequence import pad_sequences\nfrom shakespeare_utils import *\nimport sys\nimport io"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"D3CC9872D9A242D9A00B2EC0276934D4","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"为了节省你的时间，我们已经在莎士比亚的十四行诗[*\"The Sonnets\"*](shakespeare.txt)诗歌集上训练了大约1000个epoch的模型。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"430009895A99482E84B584DB7B56681E","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"让我们再训练模型完成一个新epoch，这也将花费几分钟。你可以运行`generate_output`，这将提示你输入小于40个字符的句子。这首诗将从你输入的句子开始，我们的RNN-Shakespeare将为你完成其余的部分！例如，尝试\"Forsooth this maketh no sense \"（不要输入引号）。根据是否在末尾加上空格，你的结果也可能会有所不同，两种方法都应尝试，也可以尝试其他输入法。\n"},{"cell_type":"code","execution_count":14,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"C4683AF3BE6D498398F1437D47DFAE12","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Epoch 1/1\n31412/31412 [==============================] - 138s 4ms/step - loss: 2.7218\n","name":"stdout"},{"output_type":"execute_result","metadata":{},"data":{"text/plain":"<keras.callbacks.History at 0x7fea566cf278>"},"transient":{},"execution_count":14}],"source":"print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n\nmodel.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"},{"cell_type":"code","execution_count":17,"metadata":{"slideshow":{"slide_type":"slide"},"id":"9454461B49A74867860B98F088FEC0BB","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: : to be or not to be","name":"stream"},{"output_type":"stream","text":"\n\nHere is your poem: \n\nto be or not to be.\n,\nthu all the dase widh more manthle to doing,\ndethought mine sunde thes it youl has lone love,\nthas nother miunter habll i proy my tond,\nastore self-efany nath's wordd,\nby holl give for true every brifl to thee,\nthe hatth love thoughtrild  \nshy bist the eyes in my sorled not see of,\nshy with rove mayst as my me whom must she trise.\nhis night bit mas my praire fired reon me.\ndo you khom whee a s","name":"stdout"}],"source":"# Run this cell to try with different inputs without having to re-train the model \ngenerate_output()"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"BCDFBD74714645598C805B0F7D315CD8","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"RNN-Shakespeare模型与你为恐龙名称构建的模型非常相似。唯一的区别是：\n- LSTM代替基本的RNN来捕获更远的依赖\n- 模型是更深的堆叠的LSTM模型（2层）\n- 使用Keras而不是python来简化代码\n\n如果你想了解更多信息，还可以在GitHub上查看Keras Team的文本生成实现：https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py \n\n祝贺你完成本笔记本作业！"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"912E807E9EE3492A8DFF4CAADE93965D","jupyter":{},"tags":[],"mdEditEnable":true,"trusted":true},"source":"**参考**:\n- This exercise took inspiration from Andrej Karpathy's implementation: https://gist.github.com/karpathy/d4dee566867f8291f086. To learn more about text generation, also check out Karpathy's [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n- For the Shakespearian poem generator, our implementation was based on the implementation of an LSTM text generator by the Keras team: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py \n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}