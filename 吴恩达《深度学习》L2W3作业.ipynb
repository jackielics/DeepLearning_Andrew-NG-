{"cells":[{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"67F7E6D17A13446CA2E7E10900DAA9DC","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"# TensorFlow教程\n\n欢迎来到本周的编程作业。 到目前为止，你一直使用numpy来构建神经网络。现在，我们将引导你使用深度学习框架，该框架将使你可以更轻松地构建神经网络。**TensorFlow，PaddlePaddle，Torch，Caffe，Keras**等机器学习框架可以极大地加快你的机器学习开发速度。所有这些框架也都有很多文档，你应该随时阅读学习。在此笔记本中，你将学习在TensorFlow中执行以下操作：\n\n- 初始化变量\n- 创建自己的会话（session）\n- 训练算法\n- 实现神经网络\n\n编程框架不仅可以缩短编码时间，而且有时还可以进行优化以加快代码速度。\n\n## 1 探索Tensorflow库\n\n首先，导入库：\n"},{"metadata":{"id":"6088E9FCBF534B1B8CC59F8F7CD18499","slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"/home/kesci/input/deeplearning63290\n","name":"stdout"}],"source":"cd ../input/deeplearning63290","execution_count":3},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"0B37ACD10D254EC083F74D890E12BAA8","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"import math\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n\n%matplotlib inline\nnp.random.seed(1)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"192044D2B853425D9A66A4C0CCF2C0AD","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"现在，你已经导入了库，我们将引导你完成其不同的应用程序。你将从一个示例开始：计算一个训练数据的损失。\n\n$$\nloss = \\mathcal{L}(\\hat{y}, y) = (\\hat y^{(i)} - y^{(i)})^2 \\tag{1}\n$$"},{"metadata":{"id":"67912290C80B45F1B4B86AE4C41CD8F2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## Method:\ntf.constant初始化常量\ntf.Variable初始化变量\ntf.Session创建会话执行操作\nsession.run打印输出\nsession.close关闭会话"},{"cell_type":"code","execution_count":5,"metadata":{"slideshow":{"slide_type":"slide"},"id":"C4EB801D86A44AD18006A707F35BD363","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n9\n","name":"stdout"}],"source":"y_hat = tf.constant(36, name='y_hat')            # Define y_hat constant. Set to 36.\ny = tf.constant(39, name='y')                    # Define y. Set to 39\n\nloss = tf.Variable((y - y_hat)**2, name='loss')  # Create a variable for the loss\n\ninit = tf.global_variables_initializer()         # When init is run later (session.run(init)),\n                                                 # the loss variable will be initialized and ready to be computed\nwith tf.Session() as session:                    # Create a session and print the output\n    session.run(init)                            # Initializes the variables\n    print(session.run(loss))                     # Prints the loss"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"B6F18036D4AE47D68395E2C462338F82","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"在TensorFlow中编写和运行程序包含以下步骤：\n\n1. 创建尚未执行的张量（变量）。\n1. 在这些张量之间编写操作。\n1. 初始化张量。\n1. 创建一个会话。\n1. 运行会话，这将运行你上面编写的操作。\n\n因此，当我们为损失创建变量时，我们仅将损失定义为其他数量的函数，但没有验证其值。为了验证它，我们必须运行`init = tf.global_variables_initializer（）`初始化损失变量，在最后一行中，我们终于能够验证`loss`的值并打印它。\n\n现在让我们看一个简单的例子。运行下面的单元格："},{"cell_type":"code","execution_count":6,"metadata":{"slideshow":{"slide_type":"slide"},"id":"59A86A3B20AA48318A117676C6582D78","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Tensor(\"Mul:0\", shape=(), dtype=int32)\n","name":"stdout"}],"source":"a = tf.constant(2)\nb = tf.constant(10)\nc = tf.multiply(a,b)\nprint(c)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"A5AFD77F763145D1B7D5467C44A35506","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"不出所料，看不到结果20！而是得到一个张量，是一个不具有shape属性且类型为“int32”的张量。你所做的所有操作都已放入“计算图”中，但你尚未运行此计算。为了实际将两个数字相乘，必须**创建一个会话并Session运行它**。"},{"cell_type":"code","execution_count":7,"metadata":{"slideshow":{"slide_type":"slide"},"id":"F51D4FEE01F4404D840C1B49CACC68E9","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"20\n","name":"stdout"}],"source":"sess = tf.Session()\nprint(sess.run(c))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"4FBE1FAA70034B699162B28B6C9BF45D","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"Great! 总而言之，**记住要初始化变量，创建一个会话并在该会话中运行操作**。\n\n接下来，你还必须了解 **placeholders(占位符)**。占位符是一个对象，你只能稍后指定其值。\n**要为占位符指定值，你可以使用\"feed dictionary\"（`feed_dict`变量）传入值**。在下面，我们为x创建了一个占位符，以允许我们稍后在运行会话时传递数字。"},{"cell_type":"code","execution_count":8,"metadata":{"slideshow":{"slide_type":"slide"},"id":"B326ACA6DF534BCA8DDBC9AF7DB96017","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"6\n","name":"stdout"}],"source":"# Change the value of x in the feed_dict\n\nx = tf.placeholder(tf.int64, name = 'x')\nprint(sess.run(2 * x, feed_dict = {x: 3}))# 为占位符x指定值3\nsess.close()"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"C2721CC116254F409B72D48B006DF8ED","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"当你首次定义`x`时，不必为其指定值。占位符只是一个变量，你在运行会话时才将数据分配给该变量。也就是说你在运行会话时向这些占位符“提供数据”。\n\n当你指定计算所需的操作时，你在告诉TensorFlow如何构造计算图。计算图可以具有一些占位符，你将在稍后指定它们的值。最后，在运行会话时，你要告诉TensorFlow执行计算图。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"67C292D8F17E41618E8F075A4C08B628","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"### 1.1 线性函数\n\n让我们开始此编程练习，计算以下方程式：$Y = WX + b$，其中$W$和$X$是随机矩阵，b是随机向量。\n\n**练习**：计算$WX + b$ ，其中$W, X$和$b$是从随机正态分布中得到的，W的维度为（4，3），X的维度为（3,1），b的维度为（4,1）。例如，下面是定义维度为（3,1）的常量X的方法：\n```python\nX = tf.constant(np.random.randn(3,1), name = \"X\")\n\n```\n你可能会发现以下函数很有用：\n- **tf.matmul(..., ...)进行矩阵乘法**\n- tf.add(..., ...)进行加法\n- np.random.randn(...)随机初始化"},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"6843C555EE974CCB875432D6831E3AC1","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: linear_function\n\ndef linear_function():\n    \"\"\"\n    Implements a linear function: \n            Initializes W to be a random tensor of shape (4,3)\n            Initializes X to be a random tensor of shape (3,1)\n            Initializes b to be a random tensor of shape (4,1)\n    Returns: \n    result -- runs the session for Y = WX + b \n    \"\"\"\n    \n    np.random.seed(1)\n    \n    ### START CODE HERE ### (4 lines of code)  \n    X = tf.constant(np.random.randn(3,1), name = \"X\")  \n    W = tf.constant(np.random.randn(4,3), name = \"W\")  \n    b = tf.constant(np.random.randn(4,1), name = \"b\")  \n    Y = tf.add(tf.matmul(W,X),b)  \n    ### END CODE HERE ###   \n      \n    # Create the session using tf.Session() and run it with sess.run(...) on the variable you want to calculate  \n      \n    ### START CODE HERE ###  \n    sess = tf.Session()  \n    # 运行会话，并打印输出\n    result = sess.run(Y)  \n    ### END CODE HERE ###   \n    \n    # close the session \n    sess.close()\n    # 关闭会话 \n    return result"},{"cell_type":"code","execution_count":10,"metadata":{"slideshow":{"slide_type":"slide"},"id":"CDE79616541746B9824A86CBB0269A9F","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"result = [[-2.15657382]\n [ 2.95891446]\n [-1.08926781]\n [-0.84538042]]\n","name":"stdout"}],"source":"print( \"result = \" + str(linear_function()))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"C2B4BA91EB944991819E74E40F595605","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nresult = [[-2.15657382]\n [ 2.95891446]\n [-1.08926781]\n [-0.84538042]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"8ECC6A076CE14144957E6050BB3146B8","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.2 计算Sigmoid\nGreat！你刚刚实现了线性函数。Tensorflow提供了各种常用的神经网络函数，例如`tf.sigmoid`和`tf.softmax`。对于本练习，让我们计算输入的sigmoid函数值。\n\n你将使用占位符变量`x`进行此练习。在运行会话时，应该使用feed字典传入输入`z`。在本练习中，你必须：\n（i）创建一个占位符`x`；\n（ii）使用`tf.sigmoid`定义计算Sigmoid所需的操作；\n（iii）然后运行该会话\n\n**练习**：实现下面的Sigmoid函数。你应该使用以下内容：\n\n- `tf.placeholder(tf.float32, name = \"...\")`\n- `tf.sigmoid(...)`\n- `sess.run(..., feed_dict = {x: z})`\n\n\n注意，在tensorflow中创建和使用会话有两种典型的方法：\n\n**Method 1:**\n```python\nsess = tf.Session()\n# Run the variables initialization (if needed), run the operations\nresult = sess.run(..., feed_dict = {...})\nsess.close() # Close the session\n```\n**Method 2:**\n```python\nwith tf.Session() as sess: \n    # run the variables initialization (if needed), run the operations\n    result = sess.run(..., feed_dict = {...})\n    # This takes care of closing the session for you :)\n```"},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"49C84EBEA39C48EC8BA562E623231F55","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: sigmoid\n\ndef sigmoid(z):\n    \"\"\"\n    Computes the sigmoid of z\n    \n    Arguments:\n    z -- input value, scalar or vector\n    \n    Returns: \n    results -- the sigmoid of z\n    \"\"\"\n    \n    ### START CODE HERE ### ( approx. 4 lines of code)  \n    # Create a placeholder for x. Name it 'x'.  \n    x = tf.placeholder(tf.float32, name = \"x\")  \n  \n    # compute sigmoid(x)  \n    sigmoid = tf.sigmoid(x)  \n  \n    # Create a session, and run it. Please use the method 2 explained above.   \n    # You should use a feed_dict to pass z's value to x.   \n    with tf.Session() as sess:  \n        # Run session and call the output \"result\"  \n        result = sess.run(sigmoid,feed_dict={x:z})   #actually,the sigmoid here is equal to tf.sigmoid(x)\n      \n    ### END CODE HERE ###  \n    \n    return result"},{"cell_type":"code","execution_count":12,"metadata":{"slideshow":{"slide_type":"slide"},"id":"4CDFC5BCF6C0473A89DB4EDDC92900E1","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"sigmoid(0) = 0.5\nsigmoid(12) = 0.9999938\n","name":"stdout"}],"source":"print (\"sigmoid(0) = \" + str(sigmoid(0)))\nprint (\"sigmoid(12) = \" + str(sigmoid(12)))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"7B4C7CA31BFC45C388A777B763050C10","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nsigmoid(0) = 0.5\nsigmoid(12) = 0.9999938"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"7172BAF0E98B47F6865A484B61558C99","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**总而言之，你知道如何**：\n1.创建占位符\n2.指定运算相对应的计算图\n3.创建会话\n4.如果需要指定占位符变量的值，使用feed字典运行会话。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"B5D7720166D64F22A0AB6E89152BA56F","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.3 计算损失\n\n你还可以使用内置函数来计算神经网络的损失。因此，对于i=1...m，无需编写代码来将其作为$a^{[2](i)}$和$y^{(i)}$的函数来计算：\n$$ \nJ = - \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log a^{ [2] (i)} + (1-y^{(i)})\\log (1-a^{ [2] (i)} )\\large )\\small\\tag{2} \n$$\n\n你可以使用tensorflow的一行代码中做到这一点！\n\n**练习**：实现**交叉熵损失**。你将使用的函数是：\n\n- **`tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...)`**\nlogits是预测值，labels是实际值\n\n你的代码应输入`z`，计算出sigmoid（得到`a`），然后计算出交叉熵损失$J$，所有这些操作都可以通过调用`tf.nn.sigmoid_cross_entropy_with_logits`来完成：\n\n$$\n-\\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log \\sigma(z^{[2](i)}) + (1-y^{(i)})\\log (1-\\sigma(z^{[2](i)})\\large )\\small\\tag{2}\n$$\n\n"},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"0F1B059D3C8A49AF820E61BF7CE88A25","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: cost\n\ndef cost(logits, labels):\n    \"\"\"\n    Computes the cost using the sigmoid cross entropy\n    \n    Arguments:\n    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n    labels -- vector of labels y (1 or 0) \n    \n    Note: What we've been calling \"z\" and \"y\" in this class are respectively called \"logits\" and \"labels\" \n    in the TensorFlow documentation. So logits will feed into z, and labels into y. \n    \n    Returns:\n    cost -- runs the session of the cost (formula (2))\n    \"\"\"\n    \n    ### START CODE HERE ###   \n      \n    # Create the placeholders for \"logits\" (z) and \"labels\" (y) (approx. 2 lines)  \n    z = tf.placeholder(tf.float32, name = \"z\")  \n    y = tf.placeholder(tf.float32, name = \"y\")  \n      \n    # Use the loss function (approx. 1 line)  \n    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z,labels=y)  \n      \n    # Create a session (approx. 1 line). See method 1 above.  \n    sess = tf.Session()  \n      \n    # Run the session (approx. 1 line).  \n    cost = sess.run(cost,feed_dict={z:logits,y:labels})  \n      \n    # Close the session (approx. 1 line). See method 1 above.  \n    sess.close()  \n      \n    ### END CODE HERE ###  \n    \n    return cost"},{"cell_type":"code","execution_count":14,"metadata":{"slideshow":{"slide_type":"slide"},"id":"AEDA62313CC040B0993F34DC4E15A899","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"cost = [1.0053872  1.0366409  0.41385433 0.39956614]\n","name":"stdout"}],"source":"logits = sigmoid(np.array([0.2,0.4,0.7,0.9]))\ncost = cost(logits, np.array([0,0,1,1]))\nprint (\"cost = \" + str(cost))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"0CB2A3520B9F4D2C80517566BF1DA5E9","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出** : \ncost = [1.0053872  1.0366409  0.41385433 0.39956614]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"1D46807FEF7C440E8B0E734AC5A910F0","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.4 使用独热(One Hot)编码\n\n在深度学习中，很多时候你会得到一个y向量，其数字范围从0到C-1，其中C是类的数量。例如C是4，那么你可能具有以下y向量，你将需要按以下方式对其进行转换：\n\n![Image Name](https://cdn.kesci.com/upload/image/q1obgaah6h.png?imageView2/0/w/960/h/960)\n\n这称为独热编码，因为在转换后的表示形式中，每一列中的一个元素正好是“hot”（设为1）。要以numpy格式进行此转换，你可能需要编写几行代码。在tensorflow中，你可以只使用一行代码：\n\n### - tf.one_hot(labels, depth, axis) \n\n**练习**：实现以下函数，以获取一个标签向量和**$C$（类的总数）**，并返回一个独热编码。使用`tf.one_hot（）`来做到这一点。"},{"cell_type":"code","execution_count":15,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"15F19533CA1446C380EB329ADE549799","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: one_hot_matrix\n\ndef one_hot_matrix(labels, C):\n    \"\"\"\n    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n                     will be 1. \n                     \n    Arguments:\n    labels -- vector containing the labels \n    C -- number of classes, the depth of the one hot dimension\n    \n    Returns: \n    one_hot -- one hot matrix\n    \"\"\"\n    \n    ### START CODE HERE ###  \n      \n    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)  \n    C = tf.constant(C, name = \"C\")  \n      \n    # Use tf.one_hot, be careful with the axis (approx. 1 line)  \n    one_hot_matrix = tf.one_hot(labels, C, axis=0)  \n      \n    # Create the session (approx. 1 line)  \n    sess = tf.Session()  \n      \n    # Run the session (approx. 1 line)  \n    one_hot = sess.run(one_hot_matrix)  \n      \n    # Close the session (approx. 1 line). See method 1 above.  \n    sess.close()  \n      \n    ### END CODE HERE ###  \n    \n    return one_hot"},{"cell_type":"code","execution_count":16,"metadata":{"slideshow":{"slide_type":"slide"},"id":"503B667DAE1E4FC88AC88085A9BA19EF","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"one_hot = [[0. 0. 0. 1. 0. 0.]\n [1. 0. 0. 0. 0. 1.]\n [0. 1. 0. 0. 1. 0.]\n [0. 0. 1. 0. 0. 0.]]\n","name":"stdout"}],"source":"labels = np.array([1,2,3,0,2,1])\none_hot = one_hot_matrix(labels, C = 4)\n# 一共有四类\nprint (\"one_hot = \" + str(one_hot))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"FF386A1BEEC74331885F389BFF596E57","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \none_hot = [[0. 0. 0. 1. 0. 0.]\n [1. 0. 0. 0. 0. 1.]\n [0. 1. 0. 0. 1. 0.]\n [0. 0. 1. 0. 0. 0.]]\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"CD0EAE6EFFB44B8087D3937C6C0E98EF","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.5 使用0和1初始化\n\n现在，你将学习如何初始化0和1的向量。 你将要调用的函数是**`tf.ones()`**。要使用零初始化，可以改用**`tf.zeros()`**。这些函数采用一个维度，并分别返回一个包含0和1的维度数组。\n\n**练习**：实现以下函数以获取维度并返回维度数组。\n\n - tf.ones(shape)"},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"9DEFA80A85D64C0E9E4DD92C8D418BBF","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: ones\n\ndef ones(shape):\n    \"\"\"\n    Creates an array of ones of dimension shape\n    \n    Arguments:\n    shape -- shape of the array you want to create\n        \n    Returns: \n    ones -- array containing only ones\n    \"\"\"\n    \n    ### START CODE HERE ###  \n      \n    # Create \"ones\" tensor using tf.ones(...). (approx. 1 line)  \n    ones = tf.ones(shape)  \n      \n    # Create the session (approx. 1 line)  \n    sess = tf.Session()  \n      \n    # Run the session to compute 'ones' (approx. 1 line)  \n    ones = sess.run(ones)  \n      \n    # Close the session (approx. 1 line). See method 1 above.  \n    sess.close()  \n      \n    ### END CODE HERE ###  \n    return ones  "},{"cell_type":"code","execution_count":18,"metadata":{"slideshow":{"slide_type":"slide"},"id":"BEC7919B2ED44F4390FBC64F6861B0DC","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"ones = [1. 1. 1.]\n","name":"stdout"}],"source":"print (\"ones = \" + str(ones([3])))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"9E94E799A9E8450E83E607AB33B33529","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出:**\nones = [1. 1. 1.]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"E611E61EBB8A451B84DBCE4167972899","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"# 使用Tensorflow构建你的第一个神经网络\n\n在这一部分作业中，你将使用tensorflow构建神经网络。请记住，实现tensorflow模型包含两个部分：\n\n- 创建计算图\n- 运行计算图\n\n让我们深入研究你要解决的问题！\n\n\n\n### 2.0 问题陈述：SIGNS 数据集\n\n一个下午，我们决定和一些朋友一起用计算机来解密手语。我们花了几个小时在白墙前拍照，并提出了以下数据集。现在，你的工作就是构建一种算法，以帮助语音障碍者和不懂手语的人的交流。\n\n- **训练集**：1080张图片（64 x 64像素）的手势表示从0到5的数字（每个数字180张图片）。\n- **测试集**：120张图片（64 x 64像素）的手势表示从0到5的数字（每个数字20张图片）。\n\n请注意，这是SIGNS数据集的子集。完整的数据集包含更多的手势。\n\n这是每个数字的示例，以及如何解释标签的方式。这些是原始图片，然后我们将图像分辨率降低到64 x 64像素。\n\n![Image Name](https://cdn.kesci.com/upload/image/q1o9x69csw.png?imageView2/0/w/960/h/960)\n\n**图 1**：SIGNS数据集\n\n运行以下代码以加载数据集。"},{"cell_type":"code","execution_count":19,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"DD1C854FC37F4D97AA6248782AF3E878","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# Loading the dataset\nX_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"2ABF1ABB789F48EF8A3B18C8F15F6C80","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"更改下面的索引并运行单元格以可视化数据集中的一些示例。"},{"cell_type":"code","execution_count":20,"metadata":{"slideshow":{"slide_type":"slide"},"id":"87192E9C2DE646278A79F073EDBAD85E","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"y = 5\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/87192E9C2DE646278A79F073EDBAD85E/qhrng9eomh.png\">"},"transient":{}}],"source":"# Example of a picture\nindex = 0\nplt.imshow(X_train_orig[index])\nprint (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"E9EE2A5F4E0A489F87E2B25959DAF1DA","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"通常先将图像数据集展平，然后除以255以对其进行归一化。最重要的是将每个标签转换为一个独热向量，如图1所示。运行下面的单元格即可转化。"},{"cell_type":"code","execution_count":21,"metadata":{"slideshow":{"slide_type":"slide"},"id":"FF3383353F73467E8A188DC77D5F5FBE","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"number of training examples = 1080\nnumber of test examples = 120\nX_train shape: (12288, 1080)\nY_train shape: (6, 1080)\nX_test shape: (12288, 120)\nY_test shape: (6, 120)\n","name":"stdout"}],"source":"# Flatten the training and test images\nX_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\nX_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n# Normalize image vectors\nX_train = X_train_flatten/255.\nX_test = X_test_flatten/255.\n# Convert training and test labels to one hot matrices\nY_train = convert_to_one_hot(Y_train_orig, 6)\nY_test = convert_to_one_hot(Y_test_orig, 6)\n\nprint (\"number of training examples = \" + str(X_train.shape[1]))\nprint (\"number of test examples = \" + str(X_test.shape[1]))\nprint (\"X_train shape: \" + str(X_train.shape))\nprint (\"Y_train shape: \" + str(Y_train.shape))\nprint (\"X_test shape: \" + str(X_test.shape))\nprint (\"Y_test shape: \" + str(Y_test.shape))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"B791A98D2BE047699B5DFD636760432A","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**注意** 12288 = $64 \\times 64 \\times 3$，每个图像均为正方形，64 x 64像素，其中3为RGB颜色。请确保理解这些数据的维度意义，然后再继续。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"F2D9238F7A474F35AB5C6D7512626800","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"**你的目标**是建立一种能够高精度识别符号的算法。为此，你将构建一个tensorflow模型，该模型与你先前在numpy中为猫识别构建的tensorflow模型几乎相同（但现在使用softmax输出）。这是将numpy实现的模型与tensorflow进行比较的好机会。\n\n**模型**为*LINEAR-> RELU-> LINEAR-> RELU-> LINEAR-> SOFTMAX *。 SIGMOID输出层已转换为SOFTMAX。SOFTMAX层将SIGMOID应用到两个以上的类。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"BC093945EE5F43B897FC28C50B8022CC","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"\n### 2.1 创建占位符\n\n你的第一个任务是为`X`和`Y`创建占位符，方便你以后在运行会话时传递训练数据。\n\n**练习**：实现以下函数以在tensorflow中创建占位符。"},{"cell_type":"code","execution_count":22,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"2AD5071788944A90850D27E561F3B636","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: create_placeholders\n\ndef create_placeholders(n_x, n_y):\n    \"\"\"\n    Creates the placeholders for the tensorflow session.\n    \n    Arguments:\n    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n    \n    Returns:\n    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n    \n    Tips:\n    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n      In fact, the number of examples during test/train is different.\n    \"\"\"\n\n    ### START CODE HERE ### (approx. 2 lines)  \n    X = tf.placeholder(shape=[n_x, None],dtype=tf.float32)  \n    Y = tf.placeholder(shape=[n_y, None],dtype=tf.float32)  \n    ### END CODE HERE ###  \n    \n    return X, Y"},{"cell_type":"code","execution_count":23,"metadata":{"slideshow":{"slide_type":"slide"},"id":"B89DA2942F8B4D1C8E83E6E0106F7C5A","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"X = Tensor(\"Placeholder:0\", shape=(12288, ?), dtype=float32)\nY = Tensor(\"Placeholder_1:0\", shape=(6, ?), dtype=float32)\n","name":"stdout"}],"source":"X, Y = create_placeholders(12288, 6)\nprint (\"X = \" + str(X))\nprint (\"Y = \" + str(Y))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"DF1AA9D1513E4D8EB362C876CAD99A71","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nX = Tensor(\"Placeholder:0\", shape=(12288, ?), dtype=float32)\nY = Tensor(\"Placeholder_1:0\", shape=(6, ?), dtype=float32)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"8AF5974356C0443B802912013909049C","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 2.2 初始化参数\n\n你的第二个任务是初始化tensorflow中的参数。\n\n**练习**：实现以下函数以初始化tensorflow中的参数。使用权重的Xavier初始化和偏差的零初始化。维度如下，对于W1和b1，你可以使用：\n\n```python\nW1 = tf.get_variable(\"W1\", [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\nb1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n```\n请使用`seed = 1`来确保你的结果与我们的结果相符。"},{"metadata":{"id":"E54BC8A5779D41B49A18DF8E92B748E2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"取出tf变量赋值：\ttf.get_variable\nXavier初始化:\t tf.contrib.layers.xavier_initializer\n零初始化:\t\t\t\ttf.zeros_initializer"},{"cell_type":"code","execution_count":24,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"DDA053C91AE34FDDAD311CB2AB5A3C8F","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: initialize_parameters\n\ndef initialize_parameters():\n    \"\"\"\n    Initializes parameters to build a neural network with tensorflow. The shapes are:\n                        W1 : [25, 12288]\n                        b1 : [25, 1]\n                        W2 : [12, 25]\n                        b2 : [12, 1]\n                        W3 : [6, 12]\n                        b3 : [6, 1]\n    \n    Returns:\n    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n    \"\"\"\n    \n    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n        \n    ### START CODE HERE ### (approx. 6 lines of code)  \n    W1 = tf.get_variable(\"W1\", [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))  \n    b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())  \n    W2 = tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))  \n    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())  \n    W3 = tf.get_variable(\"W3\", [6,12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))  \n    b3 = tf.get_variable(\"b3\", [6,1], initializer = tf.zeros_initializer())  \n    ### END CODE HERE ### \n\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2,\n                  \"W3\": W3,\n                  \"b3\": b3}\n    \n    return parameters"},{"cell_type":"code","execution_count":25,"metadata":{"slideshow":{"slide_type":"slide"},"id":"3DDEF9A483B44E6986D91D2D99041C57","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\nIf you depend on functionality not listed there, please file an issue.\n\nW1 = <tf.Variable 'W1:0' shape=(25, 12288) dtype=float32_ref>\nb1 = <tf.Variable 'b1:0' shape=(25, 1) dtype=float32_ref>\nW2 = <tf.Variable 'W2:0' shape=(12, 25) dtype=float32_ref>\nb2 = <tf.Variable 'b2:0' shape=(12, 1) dtype=float32_ref>\n","name":"stdout"}],"source":"tf.reset_default_graph()\nwith tf.Session() as sess:\n    parameters = initialize_parameters()\n    print(\"W1 = \" + str(parameters[\"W1\"]))\n    print(\"b1 = \" + str(parameters[\"b1\"]))\n    print(\"W2 = \" + str(parameters[\"W2\"]))\n    print(\"b2 = \" + str(parameters[\"b2\"]))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"0D62D6AEA7AC46E299296C2EDE82D03A","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nW1 = <tf.Variable 'W1:0' shape=(25, 12288) dtype=float32_ref>\nb1 = <tf.Variable 'b1:0' shape=(25, 1) dtype=float32_ref>\nW2 = <tf.Variable 'W2:0' shape=(12, 25) dtype=float32_ref>\nb2 = <tf.Variable 'b2:0' shape=(12, 1) dtype=float32_ref>"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"C74EE6E55C294795AD4456EA202828F4","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"如预期的那样，尚未对参数进行验证。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"A17E1BC6DA5A429DACFAA0E7E1281EBB","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"### 2.3 Tensorflow中的正向传播\n\n你现在将在tensorflow中实现正向传播模块。该函数将接收参数字典，并将完成正向传递。你将使用的函数是：\n\n- `tf.add（...，...）`进行加法\n- `tf.matmul（...，...）`进行矩阵乘法\n- `tf.nn.relu（...）`以应用ReLU激活\n\n**问题**：实现神经网络的正向传递。我们为你注释了numpy等式，以便你可以将tensorflow实现与numpy实现进行比较。重要的是要注意，前向传播在`z3`处停止。原因是在tensorflow中，最后的线性层输出作为计算损失函数的输入。因此，你不需要`a3`！\n"},{"cell_type":"code","execution_count":26,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"BFFFE85F3F224103858CAA8B75E86987","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: forward_propagation\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n                  the shapes are given in initialize_parameters\n\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    \"\"\"\n    \n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    \n    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:  \n    Z1 = tf.add(tf.matmul(W1,X),b1)                                              # Z1 = np.dot(W1, X) + b1  \n    A1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)  \n    Z2 = tf.add(tf.matmul(W2,A1),b2)                                              # Z2 = np.dot(W2, a1) + b2  \n    A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)  \n    Z3 = tf.add(tf.matmul(W3,A2),b3)                                              # Z3 = np.dot(W3,Z2) + b3  \n    ### END CODE HERE ###  \n    \n    return Z3"},{"cell_type":"code","execution_count":27,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"5C751DB8BFEA48548F65EC89E63DB558","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Z3 = Tensor(\"Add_2:0\", shape=(6, ?), dtype=float32)\n","name":"stdout"}],"source":"tf.reset_default_graph()\n\nwith tf.Session() as sess:\n    X, Y = create_placeholders(12288, 6)\n    parameters = initialize_parameters()\n    Z3 = forward_propagation(X, parameters)\n    print(\"Z3 = \" + str(Z3))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"9FD50C3562C94F0787D03B3A87882978","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nZ3 = Tensor(\"Add_2:0\", shape=(6, ?), dtype=float32)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"2ABAB36D5FC74333AFDE71D7BF95A4B3","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"你可能已经注意到，正向传播不会输出任何缓存。当我们开始进行传播时，你将在下面理解为什么。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"6B2C398F774B45B59BE5D74EEF941DA5","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"### 2.4 计算损失\n\n如前所述，使用以下方法很容易计算损失：\n```python\ntf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ..., labels = ...))\n```\n**问题**：实现以下损失函数。\n- 重要的是要知道`tf.nn.softmax_cross_entropy_with_logits`的\"`logits`\"和\"`labels`\"输入应具有一样的维度（数据数，类别数）。 因此，我们为你转换了Z3和Y。\n- 此外，`tf.reduce_mean`是对所以数据进行求和。"},{"cell_type":"code","execution_count":28,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"33DD46E9918047E08C040D1F377371D0","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: compute_cost \n\ndef compute_cost(Z3, Y):\n    \"\"\"\n    Computes the cost\n    \n    Arguments:\n    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n    Y -- \"true\" labels vector placeholder, same shape as Z3\n    \n    Returns:\n    cost - Tensor of the cost function\n    \"\"\"\n    \n    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n    \n    ### START CODE HERE ### (1 line of code)  \n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))  \n    ### END CODE HERE ###  \n    \n    return cost"},{"cell_type":"code","execution_count":29,"metadata":{"slideshow":{"slide_type":"slide"},"id":"E812A372616B450DA87225E48C8EEED7","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From <ipython-input-28-31bd7f36e96c>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\ncost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n","name":"stdout"}],"source":"tf.reset_default_graph()\n\nwith tf.Session() as sess:\n    X, Y = create_placeholders(12288, 6)\n    parameters = initialize_parameters()\n    Z3 = forward_propagation(X, parameters)\n    cost = compute_cost(Z3, Y)\n    print(\"cost = \" + str(cost))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"E020A5DDC0974288AFEC4CAC5678FAAE","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \ncost = Tensor(\"Mean:0\", shape=(), dtype=float32)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"2AB1F1DBFB8644A18B9F3BA9B9765982","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 2.5 反向传播和参数更新\n\n所有反向传播和参数更新均可使用1行代码完成，将这部分合并到模型中非常容易。\n\n计算损失函数之后，你将创建一个\"`optimizer`\"对象。运行tf.session时，必须与损失一起调用此对象。调用时，它将使用所选方法和学习率对给定的损失执行优化。\n\n例如，对于梯度下降，优化器将是：\n```python\noptimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n```\n\n要进行优化，你可以执行以下操作：\n```python\n_ , c = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n```\n\n通过相反顺序的tensorflow图来计算反向传播。从损失到输入。\n\n**注意**编码时，我们经常**使用`_`作为“throwaway”变量来存储以后不再需要使用的值**。这里`_`代表了我们不需要的`optimizer`的评估值（而 `c` 代表了 `cost`变量的值）。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"A46E1520F18E489B884A27463F5157E1","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"### 2.6 建立模型\n\n现在，将它们组合在一起！\n\n**练习**:调用之前实现的函数构建完整模型。"},{"cell_type":"code","execution_count":30,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"48A7D09D020F4504BE0533B36770A3B6","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n    \"\"\"\n    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n    \n    Arguments:\n    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n    learning_rate -- learning rate of the optimization\n    num_epochs -- number of epochs of the optimization loop\n    minibatch_size -- size of a minibatch\n    print_cost -- True to print the cost every 100 epochs\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    tf.set_random_seed(1)                             # to keep consistent results\n    seed = 3                                          # to keep consistent results\n    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n    n_y = Y_train.shape[0]                            # n_y : output size\n    costs = []                                        # To keep track of the cost\n    \n    # Create Placeholders of shape (n_x, n_y)\n    ### START CODE HERE ### (1 line)  \n    X, Y = create_placeholders(n_x, n_y)  \n    ### END CODE HERE ###  \n  \n    # Initialize parameters  \n    ### START CODE HERE ### (1 line)  \n    parameters = initialize_parameters()  \n    ### END CODE HERE ###  \n      \n    # Forward propagation: Build the forward propagation in the tensorflow graph  \n    ### START CODE HERE ### (1 line)  \n    Z3 = forward_propagation(X, parameters)  \n    ### END CODE HERE ###  \n      \n    # Cost function: Add cost function to tensorflow graph  \n    ### START CODE HERE ### (1 line)  \n    cost = compute_cost(Z3, Y)  \n    ### END CODE HERE ###  \n      \n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.  \n    ### START CODE HERE ### (1 line)  \n    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)  \n    ### END CODE HERE ###  \n      \n    # Initialize all the variables  \n    init = tf.global_variables_initializer()  \n  \n    # Start the session to compute the tensorflow graph  \n    with tf.Session() as sess:  \n          \n        # Run the initialization  \n        sess.run(init)  \n          \n        # Do the training loop  \n        for epoch in range(num_epochs):  \n  \n            epoch_cost = 0.                       # Defines a cost related to an epoch  \n            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set  \n            seed = seed + 1  \n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)  \n  \n            for minibatch in minibatches:  \n  \n                # Select a minibatch  \n                (minibatch_X, minibatch_Y) = minibatch  \n                  \n                # IMPORTANT: The line that runs the graph on a minibatch.  \n                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).  \n                ### START CODE HERE ### (1 line)  \n                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})  \n                ### END CODE HERE ###  \n                  \n                epoch_cost += minibatch_cost / num_minibatches  \n  \n            # Print the cost every epoch  \n            if print_cost == True and epoch % 100 == 0:  \n                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))  \n            if print_cost == True and epoch % 5 == 0:  \n                costs.append(epoch_cost)  \n                  \n        # plot the cost  \n        plt.plot(np.squeeze(costs))  \n        plt.ylabel('cost')  \n        plt.xlabel('iterations (per tens)')  \n        plt.title(\"Learning rate =\" + str(learning_rate))  \n        plt.show()  \n  \n        # lets save the parameters in a variable  \n        parameters = sess.run(parameters)  \n        print (\"Parameters have been trained!\")  \n  \n        # Calculate the correct predictions  \n        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))  \n  \n        # Calculate accuracy on the test set  \n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))  \n  \n        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))  \n        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))  \n          \n        return parameters  "},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"5EC373A0E54F451DB128B9353C3A169D","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"运行以下单元格来训练你的模型！在我们的机器上大约需要5分钟。 你的“100epoch后的损失”应为1.016458。如果不是，请不要浪费时间。单击笔记本电脑上方栏中的正方形（⬛），以中断训练，然后尝试更正你的代码。如果损失正确，请稍等片刻，然后在5分钟内回来！"},{"cell_type":"code","execution_count":29,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"B086473B8D2749EE90760C29CB005955","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Cost after epoch 0: 1.855702\nCost after epoch 100: 1.016458\nCost after epoch 200: 0.733102\nCost after epoch 300: 0.572941\nCost after epoch 400: 0.468769\nCost after epoch 500: 0.380960\nCost after epoch 600: 0.313826\nCost after epoch 700: 0.254241\nCost after epoch 800: 0.203784\nCost after epoch 900: 0.166471\nCost after epoch 1000: 0.141413\nCost after epoch 1100: 0.107604\nCost after epoch 1200: 0.086773\nCost after epoch 1300: 0.061032\nCost after epoch 1400: 0.050918\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/B086473B8D2749EE90760C29CB005955/q1oah4e074.png\">"},"transient":{}},{"output_type":"stream","text":"Parameters have been trained!\nTrain Accuracy: 0.9990741\nTest Accuracy: 0.725\n","name":"stdout"}],"source":"parameters = model(X_train, Y_train, X_test, Y_test)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"3C947183A53445C78D448F5424972E4F","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nTrain Accuracy: 0.9990741\nTest Accuracy: 0.725\n\nNice！你的算法可以识别出表示0到5之间数字的手势，准确度达到了71.7％。\n\n**评价**：\n- 你的模型足够强大，可以很好地拟合训练集。但是，鉴于训练和测试精度之间的差异，你可以尝试添加L2或dropout正则化以减少过拟合。\n- 将会话视为训练模型的代码块。每次你在小批次上运行会话时，它都会训练参数。总的来说，你已经运行了该会话多次（1500个epoch），直到获得训练有素的参数为止。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"7E1FEF8BDCA849388A8CA55F970926F8","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\t\t\n### 2.7 使用自己的图像进行测试（可选练习）\n\n祝贺你完成了此作业。现在，你可以拍张手的照片并查看模型的输出。要做到这一点：\n     1.单击此笔记本上部栏中的 \"File\" ，然后单击\"Open\"以在Coursera Hub上运行。\n     2.将图像添加到Jupyter Notebook的目录中，在 \"images\" 文件夹中\n     3.在以下代码中写下你的图片名称\n     4.运行代码，然后检查算法是否正确！"},{"cell_type":"code","execution_count":31,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"395B5A568CC346178F1F079A14059A22","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: `imread` is deprecated!\n`imread` is deprecated in SciPy 1.0.0.\nUse ``matplotlib.pyplot.imread`` instead.\n  # This is added back by InteractiveShellApp.init_path()\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `imresize` is deprecated!\n`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\nUse Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n  if sys.path[0] == '':\n","name":"stderr"},{"output_type":"stream","text":"Your algorithm predicts: y = 3\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/395B5A568CC346178F1F079A14059A22/q1obeky59e.png\">"},"transient":{}}],"source":"import scipy\nfrom PIL import Image\nfrom scipy import ndimage\n\n## START CODE HERE ## (PUT YOUR IMAGE NAME) \nmy_image = \"thumbs_up.jpg\"\n## END CODE HERE ##\n\n# We preprocess your image to fit your algorithm.\nfname = my_image\nimage = np.array(ndimage.imread(fname, flatten=False))\nmy_image = scipy.misc.imresize(image, size=(64,64)).reshape((1, 64*64*3)).T\nmy_image_prediction = predict(my_image, parameters)\n\nplt.imshow(image)\nprint(\"Your algorithm predicts: y = \" + str(np.squeeze(my_image_prediction)))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"C35AAEB280014CCB933820BA29345961","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"You indeed deserved a \"thumbs-up\" although as you can see the algorithm seems to classify it incorrectly. The reason is that the training set doesn't contain any \"thumbs-up\", so the model doesn't know how to deal with it! We call that a \"mismatched data distribution\" and it is one of the various of the next course on \"Structuring Machine Learning Projects\".\n\n尽管你看到算法似乎对它进行了错误分类，但你确实值得“竖起大拇指”。原因是训练集不包含任何“竖起大拇指”，因此模型不知道如何处理！ 我们称其为“数据不平衡”，它是下一章“构建机器学习项目”中的学习课程之一。"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"DD997422F2FE49688251819F6D6B3207","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**你应该记住**：\n- Tensorflow是深度学习中经常使用的编程框架\n- Tensorflow中的两个主要对象类别是张量和运算符。\n- 在Tensorflow中进行编码时，你必须执行以下步骤：\n     - 创建一个包含张量（变量，占位符...）和操作（tf.matmul，tf.add，...）的计算图\n     - 创建会话\n     - 初始化会话\n     - 运行会话以执行计算图\n- 你可以像在model（）中看到的那样多次执行计算图\n- 在“优化器”对象上运行会话时，将自动完成反向传播和优化。"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}