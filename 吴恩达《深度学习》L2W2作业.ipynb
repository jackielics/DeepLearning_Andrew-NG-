{"cells":[{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"43270893163045FA90E16EF6F07A7D04","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"# 优化算法\n到目前为止，你一直使用梯度下降来更新参数并使损失降至最低。 在本笔记本中，你将学习更多高级的优化方法，以加快学习速度，甚至可以使你的损失函数的获得更低的最终值。 一个好的优化算法可以使需要训练几天的网络，训练仅仅几个小时就能获得良好的结果。\n梯度下降好比在损失函数$J$上“下坡”。就像下图：\n![Image Name](https://cdn.kesci.com/upload/image/q1mfbtl11h.jpg?imageView2/0/w/960/h/960)\n\n**图1 **：**损失最小化好比在丘陵景观中寻找最低点**\n\n在训练的每个步骤中，你都按照一定的方向更新参数，以尝试到达最低点。\n\n**符号**：与往常一样，$\\frac{\\partial J}{\\partial a } =$ `da`适用于任何变量`a`。\n\n首先，请运行以下代码以导入所需的库。"},{"metadata":{"id":"1B359DD59C6D4CD489273C70F762CE88","slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"/home/kesci/input/deeplearning53365\n","name":"stdout"}],"source":"cd ../input/deeplearning53365","execution_count":1},{"cell_type":"code","execution_count":2,"metadata":{"slideshow":{"slide_type":"slide"},"id":"3524CBBFA522412B8C12C463AC941899","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"/home/kesci/input/deeplearning53365/opt_utils.py:76: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n  assert(parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n/home/kesci/input/deeplearning53365/opt_utils.py:77: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n  assert(parameters['W' + str(l)].shape == layer_dims[l], 1)\n","name":"stderr"}],"source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io\nimport math\nimport sklearn\nimport sklearn.datasets\n\nfrom opt_utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\nfrom opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\nfrom testCases import *\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"837F3137E69E41628FF2FD478BAC9538","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n## 1 梯度下降\n\n机器学习中一种简单的优化方法是梯度下降（gradient descent,GD）。当你对每个step中的所有$m$示例执行梯度计算步骤时，它也叫做“批量梯度下降”。\n\n**热身练习**：实现梯度下降更新方法。 对于$l = 1, ..., L$，梯度下降规则为：\n$$\nW^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{1}\n$$\n$$\nb^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{2}\n$$\n\n其中L是层数，$\\alpha$是学习率。所有参数都应存储在 `parameters`字典中。请注意，迭代器`l`在`for` 循环中从0开始，而第一个参数是$W^{[1]}$和$b^{[1]}$。编码时需要将`l` 转换为`l+1`。"},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"C39CC15217C24D148F1D3A57E642886A","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: update_parameters_with_gd\n\ndef update_parameters_with_gd(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using one step of gradient descent\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters to be updated:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients to update each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    learning_rate -- the learning rate, scalar.\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(parameters) // 2 # number of layers in the neural networks\n\n    # Update rule for each parameter\n    for l in range(L):\n        ### START CODE HERE ### (approx. 2 lines)\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] -learning_rate*grads[\"db\" + str(l+1)]\n        ### END CODE HERE ###\n        \n    return parameters"},{"cell_type":"code","execution_count":4,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"B094E16D993046618590CD62B9741BA2","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"W1 = [[ 1.63535156 -0.62320365 -0.53718766]\n [-1.07799357  0.85639907 -2.29470142]]\nb1 = [[ 1.74604067]\n [-0.75184921]]\nW2 = [[ 0.32171798 -0.25467393  1.46902454]\n [-2.05617317 -0.31554548 -0.3756023 ]\n [ 1.1404819  -1.09976462 -0.1612551 ]]\nb2 = [[-0.88020257]\n [ 0.02561572]\n [ 0.57539477]]\n","name":"stdout"}],"source":"parameters, grads, learning_rate = update_parameters_with_gd_test_case()\n\nparameters = update_parameters_with_gd(parameters, grads, learning_rate)\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"3D7B751E28FB40908D2823E2C34C5FB3","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nW1 = [[ 1.63535156 -0.62320365 -0.53718766]\n [-1.07799357  0.85639907 -2.29470142]]\nb1 = [[ 1.74604067]\n [-0.75184921]]\nW2 = [[ 0.32171798 -0.25467393  1.46902454]\n [-2.05617317 -0.31554548 -0.3756023 ]\n [ 1.1404819  -1.09976462 -0.1612551 ]]\nb2 = [[-0.88020257]\n [ 0.02561572]\n [ 0.57539477]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"DE3384C6505C41928E457A41F4AD38FD","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"它的一种变体是随机梯度下降（SGD），它相当于mini版的批次梯度下降，其中每个mini-batch只有一个数据示例。刚刚实现的更新规则不会更改。不同的是，SGD一次仅在一个训练数据上计算梯度，而不是在整个训练集合上计算梯度。下面的代码示例说明了随机梯度下降和（批量）梯度下降之间的区别。\n\n- **(Batch) Gradient Descent**:\n\n``` python\nX = data_input\nY = labels\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    # Forward propagation\n    a, caches = forward_propagation(X, parameters)\n    # Compute cost.\n    cost = compute_cost(a, Y)\n    # Backward propagation.\n    grads = backward_propagation(a, caches, parameters)\n    # Update parameters.\n    parameters = update_parameters(parameters, grads)\n        \n```\n\n\n- **Stochastic Gradient Descent**:\n\n```python\nX = data_input\nY = labels\nparameters = initialize_parameters(layers_dims)\nfor i in range(0, num_iterations):\n    for j in range(0, m):\n        # Forward propagation\n        a, caches = forward_propagation(X[:,j], parameters)\n        # Compute cost\n        cost = compute_cost(a, Y[:,j])\n        # Backward propagation\n        grads = backward_propagation(a, caches, parameters)\n        # Update parameters.\n        parameters = update_parameters(parameters, grads)\n```\n\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"4334B58C47FA4115BCCF4C09A1E2EBA1","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"对于随机梯度下降，在更新梯度之前，只使用1个训练样例。当训练集大时，SGD可以更新的更快。但是这些参数会向最小值“摆动”而不是平稳地收敛。下图是一个演示例子：\n\n![Image Name](https://cdn.kesci.com/upload/image/q1mffyt41s.png?imageView2/0/w/960/h/960)\n\n**图 1**: **SGD vs GD**\n“+”表示损失的最小值。 SGD造成许多振荡以达到收敛。但是每个step中，计算SGD比使用GD更快，因为它仅使用一个训练示例（相对于GD的整个批次）。\n\n**注意**：实现SGD总共需要3个for循环：\n1.迭代次数\n2.$m$个训练数据\n3.各层上（要更新所有参数，从$(W^{[1]},b^{[1]})$到$(W^{[L]},b^{[L]})$)\n\n实际上，如果你既不使用整个训练集也不使用一个训练示例来执行每次更新，则通常会得到更快的结果。小批量梯度下降法在每个步骤中使用中间数量的示例。通过小批量梯度下降，你可以遍历小批量，而不是遍历各个训练示例。\n\n![Image Name](https://cdn.kesci.com/upload/image/q1mfh0auie.png?imageView2/0/w/960/h/960)\n\n**图 2**：**SGD vs Mini-Batch GD**\n“+”表示损失的最小值。在优化算法中使用mini-batch批处理通常可以加快优化速度。\n\n**你应该记住**：\n- 梯度下降，小批量梯度下降和随机梯度下降之间的差异是用于执行一个更新步骤的数据数量。\n- 必须调整超参数学习率$\\alpha$。\n- 在小批量的情况下，通常它会胜过梯度下降或随机梯度下降（尤其是训练集较大时）。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"F3A3BE14965E497087B2E2DF1F396300","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n## 2 Mini-Batch 梯度下降\n\n让我们学习如何从训练集（X，Y）中构建小批次数据。\n\n分两个步骤：\n- **Shuffle**：如下所示，创建训练集（X，Y）的随机打乱版本。X和Y中的每一列代表一个训练示例。注意，随机打乱是在X和Y之间同步完成的。这样，在随机打乱之后，X的$i^{th}$列就是对应于Y中$i^{th}$标签的示例。打乱步骤可确保该示例将随机分为不同小批。\n\n![Image Name](https://cdn.kesci.com/upload/image/q1mfiulpgt.png?imageView2/0/w/960/h/960)\n\n- **Partition**：将打乱后的（X，Y）划分为大小为`mini_batch_size`（此处为64）的小批处理。请注意，训练示例的数量并不总是可以被`mini_batch_size`整除。最后的小批量可能较小，但是你不必担心，当最终的迷你批处理小于完整的`mini_batch_size`时，它将如下图所示：\n\n![Image Name](https://cdn.kesci.com/upload/image/q1mfj65sh0.png?imageView2/0/w/960/h/960)\n\n**练习**：实现`random_mini_batches`。我们为你编码好了shuffling部分。为了帮助你实现partitioning部分，我们为你提供了以下代码，用于选择$1^{st}$和$2^{nd}$小批次的索引：\n```python\nfirst_mini_batch_X = shuffled_X[:, 0 : mini_batch_size]\nsecond_mini_batch_X = shuffled_X[:, mini_batch_size : 2 * mini_batch_size]\n...\n```\n\n请注意，最后一个小批次的结果可能小于`mini_batch_size=64`。令$\\lfloor s \\rfloor$代表$s$向下舍入到最接近的整数（在Python中为`math.floor（s）`）。如果示例总数不是`mini_batch_size = 64`的倍数，则将有$\\lfloor \\frac{m}{mini\\_batch\\_size}\\rfloor$个带有完整示例的小批次，数量为64最终的一批次中的示例将是($m-mini_\\_batch_\\_size \\times \\lfloor \\frac{m}{mini\\_batch\\_size}\\rfloor$)。"},{"metadata":{"id":"34D7265204D2487DBF4CE584AF4A3303","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"### Method:\nnp.random.permutation():将序列元素洗牌，改变顺序"},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"36A08F99834F4ABBB228C5ED4FE1DE0C","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: random_mini_batches\n\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n        \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    # 将序列[1,m]洗牌\n    shuffled_X = X[:, permutation]\n    # shuffled为洗牌后重新排布的序列（输入数据）\n    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        ### START CODE HERE ### (approx. 2 lines)\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n        ### END CODE HERE ###\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n# 下述过程处理可能存在的最后一个集\n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        ### START CODE HERE ### (approx. 2 lines)\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n        ### END CODE HERE ###\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n\n    return mini_batches"},{"cell_type":"code","execution_count":7,"metadata":{"slideshow":{"slide_type":"slide"},"id":"17FF02A4E677411583E1D5D93FCBF8BC","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"shape of the 1st mini_batch_X: (12288, 64)\nshape of the 2nd mini_batch_X: (12288, 64)\nshape of the 3rd mini_batch_X: (12288, 20)\nshape of the 1st mini_batch_Y: (1, 64)\nshape of the 2nd mini_batch_Y: (1, 64)\nshape of the 3rd mini_batch_Y: (1, 20)\nmini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]\n","name":"stdout"}],"source":"X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()\nmini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)\n\nprint (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\nprint (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\nprint (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\nprint (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\nprint (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \nprint (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\nprint (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"CFB450BC1ED5439E8E81A62D573E4515","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nshape of the 1st mini_batch_X: (12288, 64)\nshape of the 2nd mini_batch_X: (12288, 64)\nshape of the 3rd mini_batch_X: (12288, 20)\nshape of the 1st mini_batch_Y: (1, 64)\nshape of the 2nd mini_batch_Y: (1, 64)\nshape of the 3rd mini_batch_Y: (1, 20)\nmini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"E88952D3478049ED8AEA5122B6BC6B28","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**你应该记住**：\n- Shuffling和Partitioning是构建小批次数据所需的两个步骤\n- 通常选择2的幂作为最小批量大小，例如16、32、64、128。并与CPU/GPU内存相对应"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"F5B03266F70B43E08B109BC4608F0F82","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 3 Momentum 动量梯度下降法\n\n因为小批量梯度下降仅在看到示例的子集后才进行参数更新，所以更新的方向具有一定的差异，因此小批量梯度下降所采取的路径将“朝着收敛”振荡。利用冲量则可以减少这些振荡。\n\n冲量考虑了过去的梯度以平滑更新。我们将先前梯度的“方向”存储在变量$v$中。这将是先前步骤中梯度的指数加权平均值，你也可以将$v$看作是下坡滚动的球的“速度”，根据山坡的坡度/坡度的方向来提高速度（和冲量）。\n\n![Image Name](https://cdn.kesci.com/upload/image/q1mfknrcuj.png?imageView2/0/w/960/h/960)\n\n**图 3**：红色箭头显示了带冲量的小批次梯度下降步骤所采取的方向。蓝点表示每一步的梯度方向（相对于当前的小批量）。让梯度影响$v$而不是仅遵循梯度，然后朝$v$的方向迈出一步。\n\n**练习**：初始化速度。速度$v$是一个Python字典，需要使用零数组进行初始化。它的键与grads词典中的键相同，即：\n为$l =1,...,L$：\n```python\nv[\"dW\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l+1)])\nv[\"db\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l+1)])\n```\n\n**注意**：迭代器l在for循环中从0开始，而第一个参数是v[\"dW1\"]和v[\"db1\"]（在上标中为“1”）。这就是为什么我们在“for”循环中将`l`转换为`l+1`的原因。"},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"0859C6E26759401C8D7CD64AC3BD9BFE","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: initialize_velocity\n\ndef initialize_velocity(parameters):\n    \"\"\"\n    Initializes the velocity as a python dictionary with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    \n    Returns:\n    v -- python dictionary containing the current velocity.\n                    v['dW' + str(l)] = velocity of dWl\n                    v['db' + str(l)] = velocity of dbl\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    \n    # Initialize velocity\n    for l in range(L):\n        ### START CODE HERE ### (approx. 2 lines)\n        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n        ### END CODE HERE ###\n        \n    return v"},{"cell_type":"code","execution_count":9,"metadata":{"slideshow":{"slide_type":"slide"},"id":"D3029BA469064D59833FCB278D95DA45","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"v[\"dW1\"] = [[0. 0. 0.]\n [0. 0. 0.]]\nv[\"db1\"] = [[0.]\n [0.]]\nv[\"dW2\"] = [[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\nv[\"db2\"] = [[0.]\n [0.]\n [0.]]\n","name":"stdout"}],"source":"parameters = initialize_velocity_test_case()\n\nv = initialize_velocity(parameters)\nprint(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\nprint(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\nprint(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\nprint(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"1F86BB502B4A48FC8D0B81D933C35BE4","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nv[\"dW1\"] = [[0. 0. 0.]\n [0. 0. 0.]]\nv[\"db1\"] = [[0.]\n [0.]]\nv[\"dW2\"] = [[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\nv[\"db2\"] = [[0.]\n [0.]\n [0.]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"6546C7E1E04C4F9085BF3D9E22B7C294","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**练习**：实现带冲量的参数更新。冲量更新规则是，对于$l = 1, ..., L$: \n\n$$\n\\begin{cases}\nv_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\nW^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n\\end{cases}\\tag{3}\n$$\n\n$$\n\\begin{cases}\nv_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\nb^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n\\end{cases}\\tag{4}\n$$\n\n其中L是层数，$\\beta$是动量，$\\alpha$是学习率。所有参数都应存储在`parameters`字典中。请注意，迭代器`l`在`for`循环中从0开始，而第一个参数是$W^{[1]}$和$b^{[1]}$（在上标中为“1”）。因此，编码时需要将`l`转化至`l+1`。"},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"10CAF6320C15478183A472AC325C677A","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: update_parameters_with_momentum\n\ndef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n    \"\"\"\n    Update parameters using Momentum\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- python dictionary containing the current velocity:\n                    v['dW' + str(l)] = ...\n                    v['db' + str(l)] = ...\n    beta -- the momentum hyperparameter, scalar\n    learning_rate -- the learning rate, scalar\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- python dictionary containing your updated velocities\n    \"\"\"\n\n    L = len(parameters) // 2 # number of layers in the neural networks\n    \n    # Momentum update for each parameter\n    for l in range(L):\n        \n        ### START CODE HERE ### (approx. 4 lines)\n        # compute velocities\n        v[\"dW\" + str(l + 1)] = beta*v[\"dW\" + str(l + 1)]+(1-beta)*grads['dW' + str(l+1)]\n        v[\"db\" + str(l + 1)] = beta*v[\"db\" + str(l + 1)]+(1-beta)*grads['db' + str(l+1)]\n        # update parameters\n        parameters[\"W\" + str(l + 1)] = parameters['W' + str(l+1)] - learning_rate*v[\"dW\" + str(l + 1)] \n        parameters[\"b\" + str(l + 1)] = parameters['b' + str(l+1)] - learning_rate*v[\"db\" + str(l + 1)] \n        ### END CODE HERE ###\n        \n    return parameters, v"},{"cell_type":"code","execution_count":11,"metadata":{"slideshow":{"slide_type":"slide"},"id":"08C68B6165DD450281821BF6FB0E6DF3","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"W1 = [[ 1.62544598 -0.61290114 -0.52907334]\n [-1.07347112  0.86450677 -2.30085497]]\nb1 = [[ 1.74493465]\n [-0.76027113]]\nW2 = [[ 0.31930698 -0.24990073  1.4627996 ]\n [-2.05974396 -0.32173003 -0.38320915]\n [ 1.13444069 -1.0998786  -0.1713109 ]]\nb2 = [[-0.87809283]\n [ 0.04055394]\n [ 0.58207317]]\nv[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n [ 0.05024943  0.09008559 -0.06837279]]\nv[\"db1\"] = [[-0.01228902]\n [-0.09357694]]\nv[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n [-0.03967535 -0.06871727 -0.08452056]\n [-0.06712461 -0.00126646 -0.11173103]]\nv[\"db2\"] = [[0.02344157]\n [0.16598022]\n [0.07420442]]\n","name":"stdout"}],"source":"parameters, grads, v = update_parameters_with_momentum_test_case()\n\nparameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\nprint(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\nprint(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\nprint(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\nprint(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"04B38ACF2A8C4B348642AD0059D123D0","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nW1 = [[ 1.62544598 -0.61290114 -0.52907334]\n [-1.07347112  0.86450677 -2.30085497]]\nb1 = [[ 1.74493465]\n [-0.76027113]]\nW2 = [[ 0.31930698 -0.24990073  1.4627996 ]\n [-2.05974396 -0.32173003 -0.38320915]\n [ 1.13444069 -1.0998786  -0.1713109 ]]\nb2 = [[-0.87809283]\n [ 0.04055394]\n [ 0.58207317]]\nv[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n [ 0.05024943  0.09008559 -0.06837279]]\nv[\"db1\"] = [[-0.01228902]\n [-0.09357694]]\nv[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n [-0.03967535 -0.06871727 -0.08452056]\n [-0.06712461 -0.00126646 -0.11173103]]\nv[\"db2\"] = [[0.02344157]\n [0.16598022]\n [0.07420442]]"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"B27BAB7ADA794696926F990DC1088D48","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n**注意**：\n- 速度用零初始化。因此，该算法将花费一些迭代来“提高”速度并开始采取更大的步骤。\n- **如果$\\beta = 0$，则它变为没有冲量的标准梯度下降。**\n\n**怎样选择$\\beta$?**\n\n- **冲量$\\beta$越大，更新越平滑**，因为我们对过去的梯度的考虑也更多。但是，如果$\\beta$太大，也可能使更新变得过于平滑。\n- $\\beta$的常用值范围是0.8到0.999。如果你不想调整它，则$\\beta = 0.9$通常是一个合理的默认值。\n- 调整模型的最佳$\\beta$可能需要尝试几个值，以了解在降低损失函数$J$的值方面最有效的方法。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"25975AC06E9C42338187F9632C609EC0","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**你应该记住**：\n- 冲量将过去的梯度考虑在内，以平滑梯度下降的步骤。它可以应用于批量梯度下降，小批次梯度下降或随机梯度下降。\n- 必须调整冲量超参数$\\beta$和学习率$\\alpha$。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"6A1B9C78D32C44E5A677F258227316C7","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 4 Adam\n\nAdam是训练神经网络最有效的优化算法之一。它结合了**RMSProp和Momentum**的优点。\n\n**Adam原理**\n1.计算过去**梯度**的指数加权平均值，并将其存储在变量$v$（使用偏差校正之前）和$v^{corrected}$ （使用偏差校正）中。\n2.计算过去**梯度的平方**的指数加权平均值，并将其存储在变量$s$（偏差校正之前）和$s^{corrected}$（偏差校正中）中。\n3.组合“1”和“2”的信息，在一个方向上更新参数。\n\n对于$l = 1, ..., L$，更新规则为：\n\n$$\n\\begin{cases}\nv_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\nv^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\ns_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\ns^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\nW^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n\\end{cases}\n$$\n\n其中：\n- t计算出Adam采取的步骤数\n- L是层数\n- $\\beta_1$和$\\beta_2$是控制两个指数加权平均值的超参数。\n- $\\alpha$是学习率\n- $\\varepsilon$是**一个很小的数字，以避免被零除**\n\n和之前一样，我们将所有参数存储在`parameters`字典中\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"CC2C369F51674A518B169242567587FE","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**练习**：初始化跟踪过去信息的Adam变量$v, s$ \n\n**说明**：变量$v, s$是需要用零数组初始化的python字典。它们的key与`grads`的key相同，即：\n对于$l = 1, ..., L$：\n\n```python\nv[\"dW\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l+1)])\nv[\"db\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l+1)])\ns[\"dW\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l+1)])\ns[\"db\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l+1)])\n\n```"},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"C07ACA426C894B9FBD6D7C3F4F5640C5","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: initialize_adam\n\ndef initialize_adam(parameters) :\n    \"\"\"\n    Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    \n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n    ### START CODE HERE ### (approx. 4 lines)\n        v[\"dW\" + str(l + 1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n        v[\"db\" + str(l + 1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n        s[\"dW\" + str(l + 1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n        s[\"db\" + str(l + 1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n        ### END CODE HERE ###\n    \n    return v, s"},{"cell_type":"code","execution_count":13,"metadata":{"slideshow":{"slide_type":"slide"},"id":"8631DA3C070D4CEF8D315E25D1D86F6A","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"v[\"dW1\"] = [[0. 0. 0.]\n [0. 0. 0.]]\nv[\"db1\"] = [[0.]\n [0.]]\nv[\"dW2\"] = [[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\nv[\"db2\"] = [[0.]\n [0.]\n [0.]]\ns[\"dW1\"] = [[0. 0. 0.]\n [0. 0. 0.]]\ns[\"db1\"] = [[0.]\n [0.]]\ns[\"dW2\"] = [[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\ns[\"db2\"] = [[0.]\n [0.]\n [0.]]\n","name":"stdout"}],"source":"parameters = initialize_adam_test_case()\n\nv, s = initialize_adam(parameters)\nprint(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\nprint(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\nprint(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\nprint(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\nprint(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\nprint(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\nprint(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\nprint(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"893D596C0E20436F91351112A38B48AA","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nv[\"dW1\"] = [[0. 0. 0.]\n [0. 0. 0.]]\nv[\"db1\"] = [[0.]\n [0.]]\nv[\"dW2\"] = [[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\nv[\"db2\"] = [[0.]\n [0.]\n [0.]]\ns[\"dW1\"] = [[0. 0. 0.]\n [0. 0. 0.]]\ns[\"db1\"] = [[0.]\n [0.]]\ns[\"dW2\"] = [[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\ns[\"db2\"] = [[0.]\n [0.]\n [0.]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"F0C8514A6AA345BBA7F3198DA20D926D","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n**练习**：用Adam实现参数更新。回想一下一般的更新规则是，对于$l = 1, ..., L$: \n\n$$\n\\begin{cases}\nv_{W^{[l]}} = \\beta_1 v_{W^{[l]}} + (1 - \\beta_1) \\frac{\\partial J }{ \\partial W^{[l]} } \\\\\nv^{corrected}_{W^{[l]}} = \\frac{v_{W^{[l]}}}{1 - (\\beta_1)^t} \\\\\ns_{W^{[l]}} = \\beta_2 s_{W^{[l]}} + (1 - \\beta_2) (\\frac{\\partial J }{\\partial W^{[l]} })^2 \\\\\ns^{corrected}_{W^{[l]}} = \\frac{s_{W^{[l]}}}{1 - (\\beta_2)^t} \\\\\nW^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{W^{[l]}}}{\\sqrt{s^{corrected}_{W^{[l]}}}+\\varepsilon}\n\\end{cases}$$\n\n**注意**:迭代器 `l` 在 `for` 循环中从0开始，而第一个参数是$W^{[1]}$和$b^{[1]}$。编码时需要将`l`转换为 `l+1` 。"},{"cell_type":"code","execution_count":14,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"333C1E5A8875453A8876444A6BFD4392","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: update_parameters_with_adam\n\ndef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n    \"\"\"\n    Update parameters using Adam\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    learning_rate -- the learning rate, scalar.\n    beta1 -- Exponential decay hyperparameter for the first moment estimates \n    beta2 -- Exponential decay hyperparameter for the second moment estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    \"\"\"\n    \n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n    \n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n        ### START CODE HERE ### (approx. 2 lines)\n        v[\"dW\" + str(l + 1)] = beta1*v[\"dW\" + str(l + 1)] +(1-beta1)*grads['dW' + str(l+1)]\n        v[\"db\" + str(l + 1)] = beta1*v[\"db\" + str(l + 1)] +(1-beta1)*grads['db' + str(l+1)]\n        ### END CODE HERE ###\n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        ### START CODE HERE ### (approx. 2 lines)\n        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)]/(1-(beta1)**t)\n        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)]/(1-(beta1)**t)\n        ### END CODE HERE ###\n\n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        ### START CODE HERE ### (approx. 2 lines)\n        s[\"dW\" + str(l + 1)] =beta2*s[\"dW\" + str(l + 1)] + (1-beta2)*(grads['dW' + str(l+1)]**2)\n        s[\"db\" + str(l + 1)] = beta2*s[\"db\" + str(l + 1)] + (1-beta2)*(grads['db' + str(l+1)]**2)\n        ### END CODE HERE ###\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n        ### START CODE HERE ### (approx. 2 lines)\n        s_corrected[\"dW\" + str(l + 1)] =s[\"dW\" + str(l + 1)]/(1-(beta2)**t)\n        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)]/(1-(beta2)**t)\n        ### END CODE HERE ###\n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n        ### START CODE HERE ### (approx. 2 lines)\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)]-learning_rate*(v_corrected[\"dW\" + str(l + 1)]/np.sqrt( s_corrected[\"dW\" + str(l + 1)]+epsilon))\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)]-learning_rate*(v_corrected[\"db\" + str(l + 1)]/np.sqrt( s_corrected[\"db\" + str(l + 1)]+epsilon))\n        ### END CODE HERE ###\n        \n    return parameters, v, s"},{"cell_type":"code","execution_count":15,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"A0446775A9514962A4E5455EF2A72701","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"W1 = [[ 1.63178673 -0.61919778 -0.53561312]\n [-1.08040999  0.85796626 -2.29409733]]\nb1 = [[ 1.75225313]\n [-0.75376553]]\nW2 = [[ 0.32648046 -0.25681174  1.46954931]\n [-2.05269934 -0.31497584 -0.37661299]\n [ 1.14121081 -1.09245036 -0.16498684]]\nb2 = [[-0.88529978]\n [ 0.03477238]\n [ 0.57537385]]\nv[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n [ 0.05024943  0.09008559 -0.06837279]]\nv[\"db1\"] = [[-0.01228902]\n [-0.09357694]]\nv[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n [-0.03967535 -0.06871727 -0.08452056]\n [-0.06712461 -0.00126646 -0.11173103]]\nv[\"db2\"] = [[0.02344157]\n [0.16598022]\n [0.07420442]]\ns[\"dW1\"] = [[0.00121136 0.00131039 0.00081287]\n [0.0002525  0.00081154 0.00046748]]\ns[\"db1\"] = [[1.51020075e-05]\n [8.75664434e-04]]\ns[\"dW2\"] = [[7.17640232e-05 2.81276921e-04 4.78394595e-04]\n [1.57413361e-04 4.72206320e-04 7.14372576e-04]\n [4.50571368e-04 1.60392066e-07 1.24838242e-03]]\ns[\"db2\"] = [[5.49507194e-05]\n [2.75494327e-03]\n [5.50629536e-04]]\n","name":"stdout"}],"source":"parameters, grads, v, s = update_parameters_with_adam_test_case()\nparameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = 2)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))\nprint(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\nprint(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\nprint(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\nprint(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\nprint(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\nprint(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\nprint(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\nprint(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"F4178E8A419443168D69EF6DC3A05A9F","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\n\nW1 = [[ 1.63178673 -0.61919778 -0.53561312]\n [-1.08040999  0.85796626 -2.29409733]]\nb1 = [[ 1.75225313]\n [-0.75376553]]\nW2 = [[ 0.32648046 -0.25681174  1.46954931]\n [-2.05269934 -0.31497584 -0.37661299]\n [ 1.14121081 -1.09245036 -0.16498684]]\nb2 = [[-0.88529978]\n [ 0.03477238]\n [ 0.57537385]]\nv[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n [ 0.05024943  0.09008559 -0.06837279]]\nv[\"db1\"] = [[-0.01228902]\n [-0.09357694]]\nv[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n [-0.03967535 -0.06871727 -0.08452056]\n [-0.06712461 -0.00126646 -0.11173103]]\nv[\"db2\"] = [[0.02344157]\n [0.16598022]\n [0.07420442]]\ns[\"dW1\"] = [[0.00121136 0.00131039 0.00081287]\n [0.0002525  0.00081154 0.00046748]]\ns[\"db1\"] = [[1.51020075e-05]\n [8.75664434e-04]]\ns[\"dW2\"] = [[7.17640232e-05 2.81276921e-04 4.78394595e-04]\n [1.57413361e-04 4.72206320e-04 7.14372576e-04]\n [4.50571368e-04 1.60392066e-07 1.24838242e-03]]\ns[\"db2\"] = [[5.49507194e-05]\n [2.75494327e-03]\n [5.50629536e-04]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"8B96FBB61D8A409E87551F84FC52EF52","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"现在，你学习了三种有效的优化算法（小批次梯度下降，冲量，Adam）。让我们使用每个优化器来实现一个模型，并观察其中的差异。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"C8F5F8C0B3E54290871CE13C986D7171","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"## 5 不同优化算法的模型\n\n我们使用“moons”数据集来测试不同的优化方法。（该数据集被命名为“月亮”，因为两个类别的数据看起来有点像月牙。）"},{"cell_type":"code","execution_count":16,"metadata":{"slideshow":{"slide_type":"slide"},"id":"72B9993F04454D889F68DE9D21C50071","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/72B9993F04454D889F68DE9D21C50071/qhrhcwqjfc.png\">"},"transient":{}}],"source":"train_X, train_Y = load_dataset()"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"C2EE060329B64A0EB37A353459372E5D","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"我们已经实现了一个三层的神经网络。你将使用以下方法进行训练：\n- 小批次 **Gradient Descent**:它将调用你的函数：\n     - `update_parameters_with_gd（）`\n- 小批次 **冲量**：它将调用你的函数：\n     - `initialize_velocity（）`和`update_parameters_with_momentum（）`\n- 小批次 **Adam**：它将调用你的函数：\n     - `initialize_adam（）`和`update_parameters_with_adam（）`"},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"FF7DA29B31924713B782BC3AB89A2572","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n    \"\"\"\n    3-layer neural network model which can be run in different optimizer modes.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    layers_dims -- python list, containing the size of each layer\n    learning_rate -- the learning rate, scalar.\n    mini_batch_size -- the size of a mini batch\n    beta -- Momentum hyperparameter\n    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n    num_epochs -- number of epochs\n    print_cost -- True to print the cost every 1000 epochs\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n\n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost\n            cost = compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n                                                               t, learning_rate, beta1, beta2,  epsilon)\n        \n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"D76C9AECA2304C0B8584CBC27D7C5C9B","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n现在，你将依次使用3种优化方法来运行此神经网络。\n\n### 5.1 Mini-Batch GD小批量梯度下降\n\n运行以下代码以查看模型如何进行小批量梯度下降。"},{"cell_type":"code","execution_count":18,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"3D5AA8121A4347509DCD230A85541330","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Cost after epoch 0: 0.690736\nCost after epoch 1000: 0.685273\nCost after epoch 2000: 0.647072\nCost after epoch 3000: 0.619525\nCost after epoch 4000: 0.576584\nCost after epoch 5000: 0.607243\nCost after epoch 6000: 0.529403\nCost after epoch 7000: 0.460768\nCost after epoch 8000: 0.465586\nCost after epoch 9000: 0.464518\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/3D5AA8121A4347509DCD230A85541330/qhrhi0uagr.png\">"},"transient":{}},{"output_type":"stream","text":"Accuracy: 0.7966666666666666\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/3D5AA8121A4347509DCD230A85541330/qhrhi0mnbr.png\">"},"transient":{}}],"source":"# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Gradient Descent optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"AEC559A20A2042008EAE93E54EDBD0B4","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n### 5.2 带冲量的小批量梯度下降\n\n运行以下代码，以查看模型如何使用冲量。因为此示例相对简单，所以使用冲量的收益很小。但是对于更复杂的问题，你可能会看到更大的收获。"},{"cell_type":"code","execution_count":19,"metadata":{"slideshow":{"slide_type":"slide"},"id":"1AEEECFF91FB46CF9A868425972C55B4","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Cost after epoch 0: 0.690741\nCost after epoch 1000: 0.685341\nCost after epoch 2000: 0.647145\nCost after epoch 3000: 0.619594\nCost after epoch 4000: 0.576665\nCost after epoch 5000: 0.607324\nCost after epoch 6000: 0.529476\nCost after epoch 7000: 0.460936\nCost after epoch 8000: 0.465780\nCost after epoch 9000: 0.464740\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/1AEEECFF91FB46CF9A868425972C55B4/qhrhj0zto3.png\">"},"transient":{}},{"output_type":"stream","text":"Accuracy: 0.7966666666666666\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/1AEEECFF91FB46CF9A868425972C55B4/qhrhj0jlze.png\">"},"transient":{}}],"source":"# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = \"momentum\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Momentum optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"8507A110D86D41599EDED59BF90022A4","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 5.3 Adam模式的小批量梯度下降\n\n运行以下代码以查看使用Adam的模型表现"},{"cell_type":"code","execution_count":20,"metadata":{"slideshow":{"slide_type":"slide"},"id":"68615E851FAE4CBA8858865EADFBB891","jupyter":{},"tags":[],"trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"Cost after epoch 0: 0.690552\nCost after epoch 1000: 0.185501\nCost after epoch 2000: 0.150830\nCost after epoch 3000: 0.074454\nCost after epoch 4000: 0.125959\nCost after epoch 5000: 0.104344\nCost after epoch 6000: 0.100676\nCost after epoch 7000: 0.031652\nCost after epoch 8000: 0.111973\nCost after epoch 9000: 0.197940\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/68615E851FAE4CBA8858865EADFBB891/qhrhkfhbia.png\">"},"transient":{}},{"output_type":"stream","text":"Accuracy: 0.94\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/68615E851FAE4CBA8858865EADFBB891/qhrhkf9t79.png\">"},"transient":{}}],"source":"# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n# Plot decision boundary\nplt.title(\"Model with Adam optimization\")\naxes = plt.gca()\naxes.set_xlim([-1.5,2.5])\naxes.set_ylim([-1,1.5])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"027BFC727ABC4B4E992A35AE30E1B226","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 5.4 总结\n\n|优化方法|准确度|模型损失|\n| ------------------- | -------- | ------------ |\n|Gradient descent | 79.70％|振荡|\n|Momentum | 79.70％|振荡|\n|Adam  | 94％|更光滑|\n\n冲量通常会有所帮助，但是鉴于学习率低和数据集过于简单，其影响几乎可以忽略不计。同样，你看到损失的巨大波动是因为对于优化算法，某些小批处理比其他小批处理更为困难。\n\n另一方面，Adam明显胜过小批次梯度下降和冲量。如果你在此简单数据集上运行更多epoch，则这三种方法都将产生非常好的结果。但是，Adam收敛得更快。\n\nAdam的优势包括：\n- 相对较低的内存要求（尽管高于梯度下降和带冲量的梯度下降）\n- 即使很少调整超参数，通常也能很好地工作（$\\alpha$除外）"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"FD0E313E932545708F327E13F47F8D60","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"**参考**：\n\n- Adam paper: https://arxiv.org/pdf/1412.6980.pdf"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"DFF7B67815C94EC79E972F6C3047AB78","jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":""}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}