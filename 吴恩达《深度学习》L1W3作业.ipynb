{"cells":[{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"39A15432E82F4F988CF8A0C41D1C3935","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"# 用1层隐藏层的神经网络分类二维数据\n\n欢迎来到第3周的编程作业。 现在是时候建立你的第一个神经网络了，它将具有一层隐藏层。 你将看到此模型与你使用逻辑回归实现的模型之间的巨大差异。\n\n**你将学到如何：**\n- 实现具有单个隐藏层的2分类神经网络\n- 使用具有非线性激活函数的神经元，例如tanh\n- 计算交叉熵损失\n- 实现前向和后向传播"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"1883376E4E8045F7BBAE93BA7E2B2F00","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 1- 安装包##\n\n让我们首先导入在作业过程中需要的所有软件包。\n- [numpy](www.numpy.org)是Python科学计算的基本包。\n- [sklearn](http://scikit-learn.org/stable/)提供了用于数据挖掘和分析的简单有效的工具。\n- [matplotlib](http://matplotlib.org) 是在Python中常用的绘制图形的库。\n- testCases提供了一些测试示例用以评估函数的正确性\n- planar_utils提供了此作业中使用的各种函数"},{"metadata":{"id":"6F93D0995BE54D8BAF78ED460D136322","slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"/home/kesci/input/deeplearning24054\n","name":"stdout"}],"source":"cd ../input/deeplearning24054","execution_count":1},{"cell_type":"code","execution_count":26,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"9DE389FC5FCE44E58537D3230FDEB043","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# Package imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom testCases import *\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\nfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n\n%matplotlib inline\n\nnp.random.seed(1) # set a seed so that the results are consistent"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"2FBA21FA40A0483A935CDF3C8667B3C2","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 2- 数据集##\n\n首先，让我们获取处理的数据集。 以下代码会将“flower” 2分类数据集加载到变量 `X` 和 `Y`中。"},{"cell_type":"code","execution_count":27,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"0AAD31285D904EF89CD8EAD1AB5534F8","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"X, Y = load_planar_dataset() "},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"9A91E6605AFC4C5DA81095657DCD48E0","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"使用matplotlib可视化数据集。 数据看起来像是带有一些红色（标签y = 0）和一些蓝色（y = 1）点的“花”。 我们的目标是建立一个适合该数据的分类模型。"},{"cell_type":"code","execution_count":28,"metadata":{"slideshow":{"slide_type":"slide"},"id":"C7BA05618F6B4B609BF103B8ABD57E10","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"<matplotlib.collections.PathCollection at 0x7f7c09ac1da0>"},"transient":{},"execution_count":28},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/C7BA05618F6B4B609BF103B8ABD57E10/qhkwxm8bsc.png\">"},"transient":{}}],"source":"# Visualize the data:\nplt.scatter(X[0, :], X[1, :], c=Y.reshape(X[0,:].shape), s=40, cmap=plt.cm.Spectral)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"33635FE0E4524D84A7E760907EBA9F25","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"现在你有：\n   - 包含特征（x1，x2）的numpy数组（矩阵）X\n   - 包含标签（红色：0，蓝色：1）的numpy数组（向量）Y。\n\n首先让我们深入地了解一下我们的数据。\n\n**练习**：数据集中有多少个训练示例？ 另外，变量“ X”和“ Y”的“shape”是什么？\n\n**提示**：如何获得numpy数组的shape维度？ [(help)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html)"},{"cell_type":"code","execution_count":29,"metadata":{"slideshow":{"slide_type":"slide"},"id":"E1C6058DA03A400880DB2013CA536098","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"The shape of X is: (2, 400)\nThe shape of Y is: (1, 400)\nI have m = 400 training examples!\n","name":"stdout"}],"source":"### START CODE HERE ### (≈ 3 lines of code)\nshape_X = X.shape\nshape_Y = Y.shape\n\nm = shape_X[1]  # training set size\n### END CODE HERE ###\n\nprint ('The shape of X is: ' + str(shape_X))\nprint ('The shape of Y is: ' + str(shape_Y))\nprint ('I have m = %d training examples!' % (m))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"5DBA35CFB7764B9F98568DD719FDA6E5","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nThe shape of X is: (2, 400)\nThe shape of Y is: (1, 400)\nI have m = 400 training examples!"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"116F2D4E6AB54284B9A764F79C61F3BF","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 3- 简单Logistic回归\n\n在构建完整的神经网络之前，首先让我们看看逻辑回归在此问题上的表现。 你可以使用sklearn的内置函数来执行此操作。 运行以下代码以在数据集上训练逻辑回归分类器。\n**该节用到了较多机器学习相关知识！！！**"},{"cell_type":"code","execution_count":30,"metadata":{"slideshow":{"slide_type":"slide"},"id":"DF6BD9BAD8C942E881280DE065D9DA93","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n  warnings.warn(CV_WARNING, FutureWarning)\n","name":"stderr"}],"source":"# Train the logistic regression classifier\nclf = sklearn.linear_model.LogisticRegressionCV();\nclf.fit(X.T, Y.T);"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"9F7191AC4D054B2F877D483C8AA62086","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"现在，你可以运行下面的代码以绘制此模型的决策边界："},{"cell_type":"code","execution_count":31,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"8B93212BF1674229824A91919A1CB5F1","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/8B93212BF1674229824A91919A1CB5F1/qhkwxwd9lv.png\">"},"transient":{}}],"source":"# Plot the decision boundary for logistic regression\nplot_decision_boundary(lambda x: clf.predict(x), X, Y)\nplt.title(\"Logistic Regression\")\n\n# Print accuracy\nLR_predictions = clf.predict(X.T)\nprint ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n       '% ' + \"(percentage of correctly labelled datapoints)\")"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"DC56E5032A9D4D2B869C724955DCAF8C","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nAccuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"DAB4499E833E45188557A33EEC7D33F8","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**Interpretation**: The dataset is not linearly separable, so logistic regression doesn't perform well. Hopefully a neural network will do better. Let's try this now!\n\n**说明**：由于数据集不是线性可分类的，因此逻辑回归效果不佳。 让我们试试是否神经网络会做得更好吧！"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"CABE302E9C95456B81621AB09F721147","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 4- 神经网络模型\n\n从上面我们可以得知Logistic回归不适用于“flower数据集”。现在你将训练带有单个隐藏层的神经网络。\n\n**这是我们的模型：**\n\n![Image Name](https://cdn.kesci.com/upload/image/q17ipqoyrg.png?imageView2/0/w/960/h/960)\n\n\n**数学原理：**\n\n例如$x^{(i)}$:\n$$\nz^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}\n$$ \n$$\na^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}\n$$\n$$\nz^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}\n$$\n$$\n\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}\n$$\n$$\ny^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}\n$$\n\n根据所有的预测数据，你还可以如下计算损失$J$: \n$$\nJ = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}\n$$\n\n**提示**：\n建立神经网络的一般方法是：\n1.定义神经网络结构（输入单元数，隐藏单元数等）。\n2.初始化模型的参数\n3.循环：\n - 实现前向传播\n - 计算损失\n - 后向传播以获得梯度\n - 更新参数（梯度下降）\n\n我们通常会构建辅助函数来计算第1-3步，然后将它们合并为`nn_model()`函数。一旦构建了`nn_model()`并学习了正确的参数，就可以对新数据进行预测。\n\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"A0166452AB784D28972CCD51C701EA41","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 4.1- 定义神经网络结构####\n\n**练习**：定义三个变量：\n     - n_x：输入层的大小\n     - n_h：隐藏层的大小（单层，将该层单元数设置为4）\n     - n_y：输出层的大小\n\n**提示**：使用shape来找到n_x和n_y。 另外，将隐藏层大小硬编码为4。"},{"cell_type":"code","execution_count":32,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"BE09DA11D96A44348F6E7519CADEDA96","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: layer_sizes\n\ndef layer_sizes(X, Y):\n    \"\"\"\n    Arguments:\n    X -- input dataset of shape (input size, number of examples)\n    Y -- labels of shape (output size, number of examples)\n    \n    Returns:\n    n_x -- the size of the input layer\n    n_h -- the size of the hidden layer\n    n_y -- the size of the output layer\n    \"\"\"\n    ### START CODE HERE ### (≈ 3 lines of code)\n    n_x = X.shape[0] # size of input layer\n    n_h = 4\n    n_y = Y.shape[0] # size of output layer\n    ### END CODE HERE ###\n    return (n_x, n_h, n_y)"},{"cell_type":"code","execution_count":33,"metadata":{"slideshow":{"slide_type":"slide"},"id":"A284DE307E4B4F98BE183F82718600C9","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"The size of the input layer is: n_x = 5\nThe size of the hidden layer is: n_h = 4\nThe size of the output layer is: n_y = 2\n","name":"stdout"}],"source":"X_assess, Y_assess = layer_sizes_test_case()\n(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)\nprint(\"The size of the input layer is: n_x = \" + str(n_x))\nprint(\"The size of the hidden layer is: n_h = \" + str(n_h))\nprint(\"The size of the output layer is: n_y = \" + str(n_y))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"C14BA1500E28465FB710C6A283F94689","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**（仅用于评估刚刚编码的函数，并不代表实际网络大小）。\n输入层的大小为：n_x = 5\n隐藏层的大小为：n_h = 4\n输出层的大小为：n_y = 2"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"E413569EF82745C882D62635B85DE92A","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n### 4.2- 初始化模型的参数####\n\n**练习**：实现函数 `initialize_parameters()`。\n\n**说明**：\n- 请确保参数大小正确。 如果需要，也可参考上面的神经网络图。\n- 使用随机值初始化**权重矩阵**。\n     - 使用：`np.random.randn（a，b）* 0.01`随机初始化维度为（a，b）的矩阵。\n- 将偏差向量初始化为零。\n     - 使用：`np.zeros((a,b))` 初始化维度为（a，b）零的矩阵。\n### 设置 seed 相当于选择一条世界线，一系列的固定的值"},{"cell_type":"code","execution_count":34,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"3F6BC7DB42D34CE9850094E2154FB191","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: initialize_parameters\n\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    params -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"\n    \n    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n    \n    ### START CODE HERE ### (≈ 4 lines of code)\n    W1 = np.random.randn(n_h,n_x) * 0.01\n    # 行数为输出个数，列数为输入个数\n    b1 = np.zeros((n_h,1))\n    W2 = np.random.randn(n_y,n_h) * 0.01\n    b2 = np.zeros((n_y,1))\n    ### END CODE HERE ###\n    \n    assert (W1.shape == (n_h, n_x))\n    assert (b1.shape == (n_h, 1))\n    assert (W2.shape == (n_y, n_h))\n    assert (b2.shape == (n_y, 1))\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters"},{"cell_type":"code","execution_count":35,"metadata":{"slideshow":{"slide_type":"slide"},"id":"9DA57210E61944FF86920A0F63D909A5","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"W1 = [[-0.00416758 -0.00056267]\n [-0.02136196  0.01640271]\n [-0.01793436 -0.00841747]\n [ 0.00502881 -0.01245288]]\nb1 = [[0.]\n [0.]\n [0.]\n [0.]]\nW2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]\nb2 = [[0.]]\n","name":"stdout"}],"source":"n_x, n_h, n_y = initialize_parameters_test_case()\n\nparameters = initialize_parameters(n_x, n_h, n_y)\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"3BC63BE46C4A472585F01974B4E81B36","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nW1 = [[-0.00416758 -0.00056267]\n [-0.02136196  0.01640271]\n [-0.01793436 -0.00841747]\n [ 0.00502881 -0.01245288]]\nb1 = [[0.]\n [0.]\n [0.]\n [0.]]\nW2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]\nb2 = [[0.]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"299B968D6E08401C8C6616C13ACF21AC","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 4.3- 循环####\n\n**问题**：实现`forward_propagation（）`。\n\n**说明**：\n- 在上方查看分类器的数学表示形式。\n- 你可以使用内置在笔记本中的`sigmoid()`函数。\n- 你也可以使用numpy库中的`np.tanh（）`函数。\n- 必须执行以下步骤：\n     1.使用`parameters [“ ..”]`从字典“ parameters”（这是`initialize_parameters（）`的输出）中检索出每个参数。\n     2.实现正向传播，计算$Z^{[1]}, A^{[1]}, Z^{[2]}$ 和 $A^{[2]}$ （所有训练数据的预测结果向量）。\n- 向后传播所需的值存储在`cache`中， `cache`将作为反向传播函数的输入。"},{"cell_type":"code","execution_count":36,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"6A4475FA841E4AC28800D38CF32D04F7","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: forward_propagation\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Argument:\n    X -- input data of size (n_x, m)\n    parameters -- python dictionary containing your parameters (output of initialization function)\n    \n    Returns:\n    A2 -- The sigmoid output of the second activation\n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n    \"\"\"\n    # Retrieve each parameter from the dictionary \"parameters\"\n    ### START CODE HERE ### (≈ 4 lines of code)\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    ### END CODE HERE ###\n    \n    # Implement Forward Propagation to calculate A2 (probabilities)\n    ### START CODE HERE ### (≈ 4 lines of code)\n    Z1 = np.dot(W1,X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2,A1) + b2\n    A2 = sigmoid(Z2)\n    ### END CODE HERE ###\n    \n    assert(A2.shape == (1, X.shape[1]))\n    \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache"},{"cell_type":"code","execution_count":37,"metadata":{"slideshow":{"slide_type":"slide"},"id":"241246ABCA9A4D17AA1247F256845BB4","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"-0.0004997557777419913 -0.000496963353231779 0.00043818745095914653 0.500109546852431\n[[-0.00616586  0.0020626   0.0034962 ]\n [-0.05229879  0.02726335 -0.02646869]\n [-0.02009991  0.00368692  0.02884556]\n [ 0.02153007 -0.01385322  0.02600471]]\n[[-0.00616578  0.0020626   0.00349619]\n [-0.05225116  0.02725659 -0.02646251]\n [-0.02009721  0.0036869   0.02883756]\n [ 0.02152675 -0.01385234  0.02599885]]\n[[ 0.00092281 -0.00056678  0.00095853]]\n[[0.5002307  0.49985831 0.50023963]]\n","name":"stdout"}],"source":"X_assess, parameters = forward_propagation_test_case()\n\nA2, cache = forward_propagation(X_assess, parameters)\n\n# Note: we use the mean here just to make sure that your output matches ours. \nprint(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))\n#检验各参数的维度，从而可知n_x=3,n_h=4,n_y=1\nprint(cache['Z1'])\nprint(cache['A1'])\nprint(cache['Z2'])\nprint(cache['A2'])"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"CD6FC5558E4941BF80CE88465F855EE0","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\n-0.0004997557777419913 -0.000496963353231779 0.00043818745095914653 0.500109546852431"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"A3730B63190B408980628A7E88424468","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"现在，你已经计算了包含每个示例的$a^{[2](i)}$ 的 $A^{[2]}$ （在Python变量“`A2`”中），其中，你可以计算损失函数 如下：\n\n$$\nJ = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}\n$$\n\n**练习**：实现`compute_cost（）`以计算损失$J$的值。\n\n**说明**：\n- 有很多种方法可以实现交叉熵损失。 我们为你提供了实现方法：\n$- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})$:\n```python\nlogprobs = np.multiply(np.log(A2),Y)\ncost = - np.sum(logprobs)                # no need to use a for loop!\n```\n\n（你也可以使用np.multiply()然后使用np.sum()或直接使用np.dot()）。"},{"cell_type":"code","execution_count":38,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"82503303AFA44C12BA07740A71D114E1","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: compute_cost\n\ndef compute_cost(A2, Y, parameters):\n    \"\"\"\n    Computes the cross-entropy cost given in equation (13)\n    \n    Arguments:\n    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n    \n    Returns:\n    cost -- cross-entropy cost given equation (13)\n    \"\"\"\n    \n    m = Y.shape[1] # number of example\n\n    # Compute the cross-entropy cost\n     ### START CODE HERE ### (≈ 2 lines of code)\n    logprobs = Y*np.log(A2) + (1-Y)* np.log(1-A2)\n    cost = -1/m * np.sum(logprobs)\n    ### END CODE HERE ###\n    \n    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n                                # E.g., turns [[17]] into 17 \n    assert(isinstance(cost, float))\n    \n    return cost"},{"cell_type":"code","execution_count":39,"metadata":{"slideshow":{"slide_type":"slide"},"id":"1267C379B1964A169D0D67611EECD90E","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"cost = 0.6929198937761265\n","name":"stdout"}],"source":"A2, Y_assess, parameters = compute_cost_test_case()\n\nprint(\"cost = \" + str(compute_cost(A2, Y_assess, parameters)))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"EF1506C663444F1C82CDA5C2800CA0D1","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\ncost = 0.6929198937761265"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"9C159CA9766647B189089DE9BC0C8025","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"现在，通过使用在正向传播期间计算的缓存，你可以实现后向传播。\n\n**问题**：实现函数`backward_propagation（）`。\n\n**说明**：\n反向传播通常是**深度学习中最难（最数学）**的部分。为了帮助你更好地了解，我们提供了反向传播课程的幻灯片。你将要使用此幻灯片右侧的六个方程式以构建向量化实现。\n\n![Image Name](https://cdn.kesci.com/upload/image/q17hcd4yra.png?imageView2/0/w/960/h/960)\n\n$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$\n\n$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}} a^{[1](i)T}$\n\n$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$\n\n$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1](i)2})$\n\n$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}} X^T$\n\n$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$\n\n- 请注意，$*$ 表示元素乘法。\n- 你将使用在深度学习中很常见的编码表示方法：\n    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n- 提示：\n    -要计算dZ1，你首先需要计算$g^{[1]'}(Z^{[1]})$。由于$g^{[1]}(.)$ 是tanh激活函数，因此如果$a = g^{[1]}(z)$ 则$g^{[1]'}(z) = 1-a^2$。所以你可以使用`(1 - np.power(A1, 2))`计算$g^{[1]'}(Z^{[1]})$。"},{"cell_type":"code","execution_count":40,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"BDDB4288DE1C4DC285D8E1E358532008","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: backward_propagation\n\ndef backward_propagation(parameters, cache, X, Y):\n    \"\"\"\n    Implement the backward propagation using the instructions above.\n    \n    Arguments:\n    parameters -- python dictionary containing our parameters \n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n    X -- input data of shape (2, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    \n    Returns:\n    grads -- python dictionary containing your gradients with respect to different parameters\n    \"\"\"\n    m = X.shape[1]\n    \n    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n    ### START CODE HERE ### (≈ 2 lines of code)\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    ### END CODE HERE ###\n        \n    # Retrieve also A1 and A2 from dictionary \"cache\".\n    ### START CODE HERE ### (≈ 2 lines of code)\n    A1 = cache[\"A1\"]\n    A2 = cache[\"A2\"]\n    ### END CODE HERE ###\n    \n    # Backward propagation: calculate dW1, db1, dW2, db2. \n    ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)\n    dZ2= A2 - Y\n    dW2 = 1 / m * np.dot(dZ2,A1.T)\n    db2 = 1 / m * np.sum(dZ2,axis=1,keepdims=True)\n    dZ1 = np.dot(W2.T,dZ2) * (1-np.power(A1,2))\n    dW1 = 1 / m * np.dot(dZ1,X.T)\n    db1 = 1 / m * np.sum(dZ1,axis=1,keepdims=True)\n    ### END CODE HERE ###\n    \n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads"},{"cell_type":"code","execution_count":41,"metadata":{"slideshow":{"slide_type":"slide"},"id":"9AF54D93879E4B31823EDFB3CCE38BC5","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"dW1 = [[ 0.01018708 -0.00708701]\n [ 0.00873447 -0.0060768 ]\n [-0.00530847  0.00369379]\n [-0.02206365  0.01535126]]\ndb1 = [[-0.00069728]\n [-0.00060606]\n [ 0.000364  ]\n [ 0.00151207]]\ndW2 = [[ 0.00363613  0.03153604  0.01162914 -0.01318316]]\ndb2 = [[0.06589489]]\n","name":"stdout"}],"source":"parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n\ngrads = backward_propagation(parameters, cache, X_assess, Y_assess)\nprint (\"dW1 = \"+ str(grads[\"dW1\"]))\nprint (\"db1 = \"+ str(grads[\"db1\"]))\nprint (\"dW2 = \"+ str(grads[\"dW2\"]))\nprint (\"db2 = \"+ str(grads[\"db2\"]))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"A38CB58DC34D40DD8874FF2BB2179439","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\ndW1 = [[ 0.01018708 -0.00708701]\n [ 0.00873447 -0.0060768 ]\n [-0.00530847  0.00369379]\n [-0.02206365  0.01535126]]\ndb1 = [[-0.00069728]\n [-0.00060606]\n [ 0.000364  ]\n [ 0.00151207]]\ndW2 = [[ 0.00363613  0.03153604  0.01162914 -0.01318316]]\ndb2 = [[0.06589489]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"98122BEFDD18492687F6376338E1E0D7","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n**问题**：实现参数更新。 使用梯度下降，你必须使用（dW1，db1，dW2，db2）才能更新（W1，b1，W2，b2）。\n\n**一般的梯度下降规则**：$\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$其中$\\alpha$是学习率，而$\\theta$ 代表一个参数。\n\n**图示**：具有良好的学习速率（收敛）和较差的学习速率（发散）的梯度下降算法。 图片由Adam Harley提供。\n\n![Image Name](https://cdn.kesci.com/upload/image/q17hh4otzu.gif?imageView2/0/w/960/h/960)\n\n![Image Name](https://cdn.kesci.com/upload/image/q17hharbth.gif?imageView2/0/w/960/h/960)\n\n\n"},{"cell_type":"code","execution_count":42,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"529FEFD7AB6D4B7A8F2BE4A604C64EF2","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: update_parameters\n\ndef update_parameters(parameters, grads, learning_rate = 1.2):\n    \"\"\"\n    Updates parameters using the gradient descent update rule given above\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients \n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    # Retrieve each parameter from the dictionary \"parameters\"\n    ### START CODE HERE ### (≈ 4 lines of code)\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    ### END CODE HERE ###\n    \n    # Retrieve each gradient from the dictionary \"grads\"\n    ### START CODE HERE ### (≈ 4 lines of code)\n    dW1 = grads[\"dW1\"]\n    db1 = grads[\"db1\"]\n    dW2 = grads[\"dW2\"]\n    db2 = grads[\"db2\"]\n    ## END CODE HERE ###\n    \n    # Update rule for each parameter\n    ### START CODE HERE ### (≈ 4 lines of code)\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    ### END CODE HERE ###\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters"},{"cell_type":"code","execution_count":43,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"F3D76CE09DDC4E128D1B3732B04BA2DF","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"W1 = [[-0.00643025  0.01936718]\n [-0.02410458  0.03978052]\n [-0.01653973 -0.02096177]\n [ 0.01046864 -0.05990141]]\nb1 = [[-1.02420756e-06]\n [ 1.27373948e-05]\n [ 8.32996807e-07]\n [-3.20136836e-06]]\nW2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]\nb2 = [[0.00010457]]\n","name":"stdout"}],"source":"parameters, grads = update_parameters_test_case()\nparameters = update_parameters(parameters, grads)\n\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"E8C531ED61A04156B7E688C74B657282","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\ndW1 = [[ 0.01018708 -0.00708701]\n [ 0.00873447 -0.0060768 ]\n [-0.00530847  0.00369379]\n [-0.02206365  0.01535126]]\ndb1 = [[-0.00069728]\n [-0.00060606]\n [ 0.000364  ]\n [ 0.00151207]]\ndW2 = [[ 0.00363613  0.03153604  0.01162914 -0.01318316]]\ndb2 = [[0.06589489]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"441FB397BE564E5C86A4821ACF88D4B3","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n### 4.4- 在nn_model（）中集成4.1、4.2和4.3部分中的函数 ####\n\n**问题**：在nn_model（）中建立你的神经网络模型。\n\n**说明**：神经网络模型必须以正确的顺序组合先前构建的函数。"},{"cell_type":"code","execution_count":44,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"335987758CC54FA492CBD4682E2E5649","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: nn_model\n\ndef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n    \"\"\"\n    Arguments:\n    X -- dataset of shape (2, number of examples)\n    Y -- labels of shape (1, number of examples)\n    n_h -- size of the hidden layer\n    num_iterations -- Number of iterations in gradient descent loop\n    print_cost -- if True, print the cost every 1000 iterations\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    \n    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n    ### START CODE HERE ### (≈ 5 lines of code)\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    ### END CODE HERE ###\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n         \n        ### START CODE HERE ### (≈ 4 lines of code)\n        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n        A2, cache = forward_propagation(X, parameters)\n        \n        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n        cost = compute_cost(A2, Y, parameters)\n \n        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n        grads = backward_propagation(parameters, cache, X, Y)\n \n        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n        parameters = update_parameters(parameters, grads)\n        \n        ### END CODE HERE ###\n        \n        # Print the cost every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    return parameters"},{"cell_type":"code","execution_count":46,"metadata":{"slideshow":{"slide_type":"slide"},"id":"D055F9E90E4F4A8B848161E41EAD3121","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log\n","name":"stderr"},{"output_type":"stream","text":"W1 = [[-4.18503197  5.33214315]\n [-7.52988635  1.24306559]\n [-4.19302427  5.32627154]\n [ 7.52984762 -1.24308746]]\nb1 = [[ 2.32926944]\n [ 3.79460252]\n [ 2.33002498]\n [-3.79466751]]\nW2 = [[-6033.83668723 -6008.12983227 -6033.10091631  6008.06624417]]\nb2 = [[-52.66610924]]\n","name":"stdout"}],"source":"X_assess, Y_assess = nn_model_test_case()\n\nparameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=False)\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"4E41F8F9B09244A88B6B7953AB723B52","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nW1 = [[-4.18503197  5.33214315]\n [-7.52988635  1.24306559]\n [-4.19302427  5.32627154]\n [ 7.52984762 -1.24308746]]\nb1 = [[ 2.32926944]\n [ 3.79460252]\n [ 2.33002498]\n [-3.79466751]]\nW2 = [[-6033.83668723 -6008.12983227 -6033.10091631  6008.06624417]]\nb2 = [[-52.66610924]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"9889CAABA1C8433B91D2A5DEBA79E028","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 4.5- 预测\n\n**问题**：使用你的模型通过构建predict()函数进行预测。\n使用正向传播来预测结果。\n\n**提示**：predictions = $y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases}\n      1 & \\text{if}\\ activation > 0.5 \\\\\n      0 & \\text{otherwise}\n    \\end{cases}$  \n    \n例如，如果你想基于阈值将矩阵X设为0和1，则可以执行以下操作： ```X_new = (X > threshold)```"},{"cell_type":"code","execution_count":48,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"19AFD7AAE7B442B49955CF9EDF31F8D3","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: predict\n\ndef predict(parameters, X):\n    \"\"\"\n    Using the learned parameters, predicts a class for each example in X\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (n_x, m)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n    \"\"\"\n    \n    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n  ### START CODE HERE ### (≈ 2 lines of code)\n    A2, cache = forward_propagation(X, parameters)\n    predictions = np.round(A2)\n    ### END CODE HERE ###\n    \n    return predictions"},{"cell_type":"code","execution_count":49,"metadata":{"slideshow":{"slide_type":"slide"},"id":"E10FF326CAE746E6A6257F123FB31401","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"predictions mean = 0.6666666666666666\n","name":"stdout"}],"source":"parameters, X_assess = predict_test_case()\n\npredictions = predict(parameters, X_assess)\nprint(\"predictions mean = \" + str(np.mean(predictions)))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"113DEA0020164FDF8442C1450ACED587","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \npredictions mean = 0.6666666666666666"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"F9C06F1D7BCE4F1597AC6A258AA7FEFF","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"现在运行模型以查看其如何在二维数据集上运行。 运行以下代码以使用含有$n_h$隐藏单元的单个隐藏层测试模型。"},{"cell_type":"code","execution_count":50,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"CB068684F93C4A2A8AE816EB492CDCBE","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Cost after iteration 0: 0.693048\nCost after iteration 1000: 0.288083\nCost after iteration 2000: 0.254385\nCost after iteration 3000: 0.233864\nCost after iteration 4000: 0.226792\nCost after iteration 5000: 0.222644\nCost after iteration 6000: 0.219731\nCost after iteration 7000: 0.217504\nCost after iteration 8000: 0.219467\nCost after iteration 9000: 0.218561\n","name":"stdout"},{"output_type":"execute_result","metadata":{},"data":{"text/plain":"Text(0.5, 1.0, 'Decision Boundary for hidden layer size 4')"},"transient":{},"execution_count":50},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/CB068684F93C4A2A8AE816EB492CDCBE/qhkx3gr9wi.png\">"},"transient":{}}],"source":"# Build a model with a n_h-dimensional hidden layer\nparameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n\n# Plot the decision boundary\nplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\nplt.title(\"Decision Boundary for hidden layer size \" + str(4))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"8687E42D10F4474E88C842B18380D2FD","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nCost after iteration 9000: 0.218561"},{"cell_type":"code","execution_count":24,"metadata":{"slideshow":{"slide_type":"slide"},"id":"93049DED91EC41848D8020264BD272F7","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Accuracy: 90%\n","name":"stdout"}],"source":"# Print accuracy\npredictions = predict(parameters, X)\nprint ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"1C312E1811CB47BB87D1EC89416D9E07","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nAccuracy: 90%"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"CB8C70D69C084ED3885530EF4BD71EBD","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"与Logistic回归相比，准确性确实更高。 该模型学习了flower的叶子图案！ 与逻辑回归不同，神经网络甚至能够学习非线性的决策边界。\n\n现在，让我们尝试几种不同的隐藏层大小。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"83D23050A1CF44BC8E3CDE29CDAB8213","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"### 4.6- 调整隐藏层大小（可选练习）###\n\n运行以下代码（可能需要1-2分钟）， 你将观察到不同大小隐藏层的模型的不同表现。"},{"cell_type":"code","execution_count":51,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"F70D5A23097642688F6245327ACE9DD7","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Accuracy for 1 hidden units: 67.5 %\nAccuracy for 2 hidden units: 67.25 %\nAccuracy for 3 hidden units: 90.75 %\nAccuracy for 4 hidden units: 90.5 %\nAccuracy for 5 hidden units: 91.25 %\nAccuracy for 10 hidden units: 90.25 %\nAccuracy for 20 hidden units: 90.5 %\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 1152x2304 with 7 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/F70D5A23097642688F6245327ACE9DD7/qhkx6wpw8s.png\">"},"transient":{}}],"source":"# This may take about 2 minutes to run\n\nplt.figure(figsize=(16, 32))\nhidden_layer_sizes = [1, 2, 3, 4, 5, 10, 20]\nfor i, n_h in enumerate(hidden_layer_sizes):\n    plt.subplot(5, 2, i+1)\n    plt.title('Hidden Layer of size %d' % n_h)\n    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n    predictions = predict(parameters, X)\n    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"4869F5D7B7E64483A996133AD2AD44CC","mdEditEnable":true,"jupyter":{},"tags":[],"trusted":true},"source":"**说明**：\n- 较大的模型（具有更多隐藏的单元）能够更好地拟合训练集，直到最终最大的模型**过拟合数据**为止。\n- 隐藏层的最佳大小似乎在**n_h = 5**左右。的确，此值似乎很好地拟合了数据，而又不会引起明显的过度拟合。\n- 稍后你还将学习正则化，帮助构建更大的模型（例如n_h = 50）而不会过度拟合。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"E6899BC169694A6E813FEF82FB5B4678","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"**可选问题**：\n\n**注意**：请单击右上角的\"Submit Assignment\"蓝色按钮以提交作业。\n\n如果你愿意，可以探索一些可选的问题：\n- 将tanh激活函数更改为sigmoid或ReLU会发生什么？\n- 调整学习率会发生什么？\n- 如果我们更改数据集该怎么办？ （请参阅下面的第5部分！）"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"5ADB0C5A9DE842E0805D25C115A8E5B9","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**你学习了以下几点：**\n- 建立具有隐藏层的完整神经网络\n- 善用非线性单位\n- 实现正向传播和反向传播，并训练神经网络\n- 了解不同隐藏层大小（包括过度拟合）的影响。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"0A079694CCB244A18A4DC8ED61F41D98","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"## 5- 模型在其他数据集上的性能"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"4AC9D6D50DA442519061F4AEAE80D623","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"如果需要，可以为以下每个数据集重新运行整个笔记本（除去数据集部分）。"},{"cell_type":"code","execution_count":27,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"B726E458BAD648828A18A3AC3706C12D","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/B726E458BAD648828A18A3AC3706C12D/q17hlyhva3.png\">"},"transient":{}}],"source":"# Datasets\nnoisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n\ndatasets = {\"noisy_circles\": noisy_circles,\n            \"noisy_moons\": noisy_moons,\n            \"blobs\": blobs,\n            \"gaussian_quantiles\": gaussian_quantiles}\n\n### START CODE HERE ### (choose your dataset)\ndataset = \"gaussian_quantiles\"\n### END CODE HERE ###\n\nX, Y = datasets[dataset]\nX, Y = X.T, Y.reshape(1, Y.shape[0])\n\n# make blobs binary\nif dataset == \"blobs\":\n    Y = Y%2\n\n# Visualize the data\nplt.scatter(X[0, :], X[1, :], c=Y.reshape(X[0,:].shape), s=40, cmap=plt.cm.Spectral);"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"8356E1E535CF4F6B88CB7DA485F35F36","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"恭喜你完成了此编程作业！\n\n参考:\n- http://scs.ryerson.ca/~aharley/neural-networks/\n- http://cs231n.github.io/neural-networks-case-study/"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}