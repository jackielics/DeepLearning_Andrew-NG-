{"cells":[{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"4F6EC8ADDEBA42BD81616F1171C3470B","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"# 卷积神经网络的应用\n\n欢迎来到课程4的第二项作业！在此笔记本中，你将：\n\n- 实现模型构建所需的辅助函数\n- 使用TensorFlow实现功能全面的ConvNet\n\n**完成此作业后，你将能够：**\n\n- 用TensorFlow构建和训练ConvNet解决分类问题\n\n我们在这里假设你已经熟悉TensorFlow。如果不是，请先学习课程2第三周的*TensorFlow教程*（“*改善深度神经网络*”）。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"147C8B05D611493693B519BDFEF47520","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 1 TensorFlow模型\n\n在上一项作业中，你使用numpy构建了辅助函数，以了解卷积神经网络背后的机制。实际上现在大多数深度学习的应用都是使用编程框架构建的，框架具有许多内置函数，你可以轻松地调用它们。\n\n和之前一样，我们将从加载包开始。"},{"metadata":{"id":"A29986C823704CED82443FB4801B3EA2","slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"/home/kesci/input/deeplerning95172\n","name":"stdout"}],"source":"cd /home/kesci/input/deeplerning95172","execution_count":1},{"cell_type":"code","execution_count":2,"metadata":{"slideshow":{"slide_type":"slide"},"id":"9894EB9782E2463C8C936EBF29A77CC4","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"import math\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom cnn_utils import *\n\n%matplotlib inline\nnp.random.seed(1)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"DDB2B8D291924761867CA9444408EC9F","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"运行以下单元格以加载要使用的“SIGNS”数据集。"},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"62178E1E3E0A405B996D1F0001CD32DC","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# Loading the data (signs)\nX_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"B3FACA52E4444A8487691E1EF4826430","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"SIGNS数据集是6个手势符号的图片集，这些符号表示从0到5的数字。\n\n![Image Name](https://cdn.kesci.com/upload/image/q1q2tsejam.png?imageView2/0/w/960/h/960)\n\n以下单元格将显示数据集中标记图像的示例。随时更改`index`的值，然后重新运行以查看不同的示例。"},{"cell_type":"code","execution_count":4,"metadata":{"slideshow":{"slide_type":"slide"},"id":"8DA5478647424BCEBA150369DB9FA66C","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"y = 2\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/8DA5478647424BCEBA150369DB9FA66C/q1q2u37q2p.png\">"},"transient":{}}],"source":"# Example of a picture\nindex = 6\nplt.imshow(X_train_orig[index])\nprint (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"554C87E76C054DFC80F45396579C9BB2","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"在课程2中，你已经为此数据集构建了一个全连接的网络。但是由于这是图像数据集，因此应用ConvNet将更自然。\n\n首先，让我们检查数据的维度。"},{"cell_type":"code","execution_count":5,"metadata":{"slideshow":{"slide_type":"slide"},"id":"C37BE72DE594459D8E714ACB2699CD67","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"number of training examples = 1080\nnumber of test examples = 120\nX_train shape: (1080, 64, 64, 3)\nY_train shape: (1080, 6)\nX_test shape: (120, 64, 64, 3)\nY_test shape: (120, 6)\n","name":"stdout"}],"source":"X_train = X_train_orig/255.\nX_test = X_test_orig/255.\nY_train = convert_to_one_hot(Y_train_orig, 6).T\nY_test = convert_to_one_hot(Y_test_orig, 6).T\nprint (\"number of training examples = \" + str(X_train.shape[0]))\nprint (\"number of test examples = \" + str(X_test.shape[0]))\nprint (\"X_train shape: \" + str(X_train.shape))\nprint (\"Y_train shape: \" + str(Y_train.shape))\nprint (\"X_test shape: \" + str(X_test.shape))\nprint (\"Y_test shape: \" + str(Y_test.shape))\nconv_layers = {}"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"D0D38B222C5645FF84D35777699BA09A","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.1 创建占位符\n\nTensorFlow需要为运行会话时输入的数据创建占位符。\n\n**练习**：实现以下函数为输入图像X和输出Y创建占位符。暂时不用定义训练数据的数量。为此，你可以使用 \"None\" 作为批次大小，稍后灵活地选择它。因此，X的维度应为 **[None, n_H0, n_W0, n_C0]**，Y的尺寸应为 **[None, n_y]**。 [提示](https://www.tensorflow.org/api_docs/python/tf/placeholder)。"},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"EDA5A6675D7645099F54AD866CA0E802","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: create_placeholders\n\ndef create_placeholders(n_H0, n_W0, n_C0, n_y):\n    \"\"\"\n    Creates the placeholders for the tensorflow session.\n    \n    Arguments:\n    n_H0 -- scalar, height of an input image\n    n_W0 -- scalar, width of an input image\n    n_C0 -- scalar, number of channels of the input\n    n_y -- scalar, number of classes\n        \n    Returns:\n    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n    \"\"\"\n\n    ### START CODE HERE ### (≈2 lines)\n    X = tf.placeholder(tf.float32, shape=(None, n_H0, n_W0, n_C0))\n    Y = tf.placeholder(tf.float32,shape=(None,n_y))\n    ### END CODE HERE ###\n    \n    return X, Y"},{"cell_type":"code","execution_count":7,"metadata":{"slideshow":{"slide_type":"slide"},"id":"A6B254D5117D4607B7D4E72BD54F7262","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"X = Tensor(\"Placeholder:0\", shape=(?, 64, 64, 3), dtype=float32)\nY = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=float32)\n","name":"stdout"}],"source":"X, Y = create_placeholders(64, 64, 3, 6)\nprint (\"X = \" + str(X))\nprint (\"Y = \" + str(Y))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"98ED2D0D8F224959BC08B6EFAE4875AD","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**\nX = Tensor(\"Placeholder:0\", shape=(?, 64, 64, 3), dtype=float32)\nY = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=float32)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"920335E1B9DF4CA38D57ACE1D53EACE7","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.2 初始化参数\n\n你将使用`tf.contrib.layers.xavier_initializer（seed = 0）`初始化权重/滤波器$W1$和$W2$。你无需担心偏差变量，因为TensorFlow函数可以处理偏差。还要注意你只会为conv2d函数初始化权重/滤波器，TensorFlow将自动初始化全连接部分的层。在本作业的后面，我们将详细讨论。\n\n**练习**:实现initialize_parameters(),下面提供了每组过滤器的尺寸。\n**提示**：在Tensorflow中初始化维度为[1,2,3,4]的参数$W$，使用：\n```python\nW = tf.get_variable(\"W\", [1,2,3,4], initializer = ...)\n```\n[More Info](https://www.tensorflow.org/api_docs/python/tf/get_variable)。"},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"33768F825EE646899CD3ACD61B703B92","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: initialize_parameters\n\ndef initialize_parameters():\n    \"\"\"\n    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n                        W1 : [4, 4, 3, 8]\n                        W2 : [2, 2, 8, 16]\n    Returns:\n    parameters -- a dictionary of tensors containing W1, W2\n    \"\"\"\n    \n    tf.set_random_seed(1)                              # so that your \"random\" numbers match ours\n        \n    ### START CODE HERE ### (approx. 2 lines of code)\n    W1 =tf.get_variable('W1',[4,4,3,8],initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n    W2 = tf.get_variable('W2',[2,2,8,16],initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n    ### END CODE HERE ###\n    parameters = {\"W1\": W1,\n                  \"W2\": W2}\n    \n    return parameters"},{"cell_type":"code","execution_count":9,"metadata":{"slideshow":{"slide_type":"slide"},"id":"47BA6D8AA66044BC9177D5D3277853F3","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\nIf you depend on functionality not listed there, please file an issue.\n\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nW1 = [ 0.00131723  0.1417614  -0.04434952  0.09197326  0.14984085 -0.03514394\n -0.06847463  0.05245192]\nW2 = [-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n","name":"stdout"}],"source":"tf.reset_default_graph()\nwith tf.Session() as sess_test:\n    parameters = initialize_parameters()\n    init = tf.global_variables_initializer()\n    sess_test.run(init)\n    print(\"W1 = \" + str(parameters[\"W1\"].eval()[1,1,1]))\n    print(\"W2 = \" + str(parameters[\"W2\"].eval()[1,1,1]))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"EA24CF8942C64513845DE854B264740C","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出:**\nW1 = [ 0.00131723  0.1417614  -0.04434952  0.09197326  0.14984085 -0.03514394\n -0.06847463  0.05245192]\nW2 = [-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n -0.22779644 -0.1601823  -0.16117483 -0.10286498]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"ACA47AD6059E4523862E44B83C51E3B1","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.3 正向传播\n\n在TensorFlow中，有内置函数为你执行卷积步骤。\n\n- **tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = 'SAME'):** 给定输入$X$和一组滤波器$W1$，函数将使用$W1$的滤波器卷积X。第三个输入([1,f,f,1])表示输入的每个维度(m, n_H_prev, n_W_prev, n_C_prev)的步幅。你可以在[here](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)阅读完整的文档。\n\n- **tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME'):** 给定输入A，此函数使用大小为（f，f）的窗口和大小为（s，s）的步幅在每个窗口上进行最大池化。你可以在 [here](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool)阅读完整的文档。\n\n- **tf.nn.relu(Z1):** 计算Z1的ReLU激活输出（可以是任何形状）。你可以在 [here](https://www.tensorflow.org/api_docs/python/tf/nn/relu)阅读完整的文档。\n\n- **tf.contrib.layers.flatten(P)**: 给定输入P，此函数将每个示例展平为一维向量，同时保持批量大小。它返回维度为[batch_size，k]的展平张量。你可以在 [here](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten)阅读完整的文档。\n\n- **tf.contrib.layers.fully_connected(F, num_outputs):** 给定展平的输入F，它将返回用全连接层计算出的输出。你可以在 [here](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected)阅读完整的文档。\n\n在上面的最后一个函数（`tf.contrib.layers.fully_connected`）中，全连接层会自动初始化图中的权重，并在训练模型时继续对其进行训练。因此，初始化参数时无需初始化这些权重。\n\n**练习**：\n\n实现下面的`forward_propagation`函数以构建以下模型：`CONV2D-> RELU-> MAXPOOL-> CONV2D-> RELU-> MAXPOOL-> FLATTEN-> FULLYCONNECTED`。使用上面那些函数。\n\t \n具体地，我们将在所有步骤中使用以下参数：\n     - Conv2D：步幅为1，填充为“SAME”\n     - ReLU\n     - Max pool：使用8x8的滤波器和8x8的步幅，填充为“SAME”\n     - Conv2D：步幅为1，填充为“SAME”\n     - ReLU\n     - Max pool：使用4x4的滤波器和4x4的步幅，填充为“SAME”\n     - 展平之前的输出。\n     - FULLYCONNECTED（FC）层：应用不含非线性激活函数的全连接层。请勿在此处调用softmax。这将在输出层中产生6个神经元，然后将其传递给softmax。在TensorFlow中，softmax和cost函数被合并为一个函数，在计算损失时将调用另一个函数。"},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"4BCD24B4187343828675210485909F5F","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: forward_propagation\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Implements the forward propagation for the model:\n    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n\n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n                  the shapes are given in initialize_parameters\n\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    \"\"\"\n\n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    \n    ### START CODE HERE ###\n    # CONV2D: stride of 1, padding 'SAME'\n    Z1 = tf.nn.conv2d(X,W1, strides = [1,1,1,1], padding = 'SAME')\n    # RELU\n    A1 = tf.nn.relu(Z1)\n    # MAXPOOL: window 8x8, sride 8, padding 'SAME'\n    P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n    # CONV2D: filters W2, stride 1, padding 'SAME'\n    Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = 'SAME')\n    # RELU\n    A2 = tf.nn.relu(Z2)\n    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n    P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n    # FLATTEN\n    P2 = tf.contrib.layers.flatten(P2)\n    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\"\n    Z3 = tf.contrib.layers.fully_connected(P2, num_outputs = 6, activation_fn=None)\n    ### END CODE HERE ###\n\n\n    return Z3"},{"cell_type":"code","execution_count":11,"metadata":{"slideshow":{"slide_type":"slide"},"id":"5C14B62913444E839487ACCD0E5F2692","collapsed":false,"scrolled":true,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.flatten instead.\nZ3 = [[ 1.4416984  -0.24909666  5.450499   -0.2618962  -0.20669907  1.3654671 ]\n [ 1.4070846  -0.02573211  5.08928    -0.48669922 -0.40940708  1.2624859 ]]\n","name":"stdout"}],"source":"tf.reset_default_graph()\n\nwith tf.Session() as sess:\n    np.random.seed(1)\n    X, Y = create_placeholders(64, 64, 3, 6)\n    parameters = initialize_parameters()\n    Z3 = forward_propagation(X, parameters)\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    a = sess.run(Z3, {X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\n    print(\"Z3 = \" + str(a))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"8798B05661304369891DE7849FC3A821","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nZ3 = [[ 1.4416984  -0.24909666  5.450499   -0.2618962  -0.20669907  1.3654671 ]\n [ 1.4070846  -0.02573211  5.08928    -0.48669922 -0.40940708  1.2624859 ]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"71F153E836E440BCBD4BD25E59856463","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.4 计算损失\n\n在下面实现损失函数的计算，你可能会发现以下两个函数很有帮助：\n\n- **tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y):** 计算softmax熵损失，该函数会计算softmax激活函数以及由此产生的损失。你可以在 [here](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)查看完整的文档。\n- **tf.reduce_mean:** 计算张量各维度上元素的均值，用它来对所有训练示例的损失求和，以获得总损失，你可以在[here](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)查看完整的文档。\n\n**练习**：使用上面的函数计算下述损失。"},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"5B7EFE39D907481E96DFB25D3FAC1F1F","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: compute_cost \n\ndef compute_cost(Z3, Y):\n    \"\"\"\n    Computes the cost\n    \n    Arguments:\n    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n    Y -- \"true\" labels vector placeholder, same shape as Z3\n    \n    Returns:\n    cost - Tensor of the cost function\n    \"\"\"\n    \n    ### START CODE HERE ### (1 line of code)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))\n    ### END CODE HERE ###\n    \n    return cost"},{"cell_type":"code","execution_count":13,"metadata":{"slideshow":{"slide_type":"slide"},"id":"72206CB05B6746BE8086622FFEF31247","collapsed":false,"scrolled":true,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From <ipython-input-12-ca0f1caf42c5>:16: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\ncost = 4.6648693\n","name":"stdout"}],"source":"tf.reset_default_graph()\n\nwith tf.Session() as sess:\n    np.random.seed(1)\n    X, Y = create_placeholders(64, 64, 3, 6)\n    parameters = initialize_parameters()\n    Z3 = forward_propagation(X, parameters)\n    cost = compute_cost(Z3, Y)\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    a = sess.run(cost, {X: np.random.randn(4,64,64,3), Y: np.random.randn(4,6)})\n    print(\"cost = \" + str(a))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"2395B3BB384548B2A998BBC8F92E54BC","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \ncost =2.91034"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"3CAB38F16CDD4F7C8E48F9BDC52B3D54","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.5 构建模型\n\n最后，你将合并以上实现的辅助函数以构建模型并在SIGNS数据集上对其进行训练。\n\n你已经在课程2的“优化算法”编程作业中实现了`random_mini_batches()`，记住此函数返回的是一个小批次的处理列表。\n\n**练习**：完成以下函数：\n\n以下模型应：\n\n- 创建占位符\n- 初始化参数\n- 正向传播\n- 计算损失\n- 创建优化函数\n\n最后，你将创建一个会话并为num_epochs运行一个for循环，获取小批次处理，然后针对每个小批次运行优化函数。\n[Hint for initializing the variables](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer) "},{"cell_type":"code","execution_count":14,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"332B69BED66B49F5BE691C240B4AC71D","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: model\n\ndef model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n          num_epochs = 100, minibatch_size = 64, print_cost = True):\n    \"\"\"\n    Implements a three-layer ConvNet in Tensorflow:\n    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n    \n    Arguments:\n    X_train -- training set, of shape (None, 64, 64, 3)\n    Y_train -- test set, of shape (None, n_y = 6)\n    X_test -- training set, of shape (None, 64, 64, 3)\n    Y_test -- test set, of shape (None, n_y = 6)\n    learning_rate -- learning rate of the optimization\n    num_epochs -- number of epochs of the optimization loop\n    minibatch_size -- size of a minibatch\n    print_cost -- True to print the cost every 100 epochs\n    \n    Returns:\n    train_accuracy -- real number, accuracy on the train set (X_train)\n    test_accuracy -- real number, testing accuracy on the test set (X_test)\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n    seed = 3                                          # to keep results consistent (numpy seed)\n    (m, n_H0, n_W0, n_C0) = X_train.shape             \n    n_y = Y_train.shape[1]                            \n    costs = []                                        # To keep track of the cost\n    \n    # Create Placeholders of the correct shape\n    ### START CODE HERE ### (1 line)\n    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n    ### END CODE HERE ###\n\n    # Initialize parameters\n    ### START CODE HERE ### (1 line)\n    parameters = initialize_parameters()\n    ### END CODE HERE ###\n    \n    # Forward propagation: Build the forward propagation in the tensorflow graph\n    ### START CODE HERE ### (1 line)\n    Z3 = forward_propagation(X, parameters)\n    ### END CODE HERE ###\n    \n    # Cost function: Add cost function to tensorflow graph\n    ### START CODE HERE ### (1 line)\n    cost = compute_cost(Z3, Y)\n    ### END CODE HERE ###\n    \n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n    ### START CODE HERE ### (1 line)\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n    ### END CODE HERE ###\n    \n    # Initialize all the variables globally\n    init = tf.global_variables_initializer()\n     \n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n        \n        # Run the initialization\n        sess.run(init)\n        \n        # Do the training loop\n        for epoch in range(num_epochs):\n\n            minibatch_cost = 0.\n            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n            seed = seed + 1\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            for minibatch in minibatches:\n\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                # IMPORTANT: The line that runs the graph on a minibatch.\n                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n                ### START CODE HERE ### (1 line)\n                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})\n                ### END CODE HERE ###\n                \n                minibatch_cost += temp_cost / num_minibatches\n                \n\n            # Print the cost every epoch\n            if print_cost == True and epoch % 5 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n            if print_cost == True and epoch % 1 == 0:\n                costs.append(minibatch_cost)\n        \n        \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # Calculate the correct predictions\n        predict_op = tf.argmax(Z3, 1)\n        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n        \n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n        print(accuracy)\n        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n        print(\"Train Accuracy:\", train_accuracy)\n        print(\"Test Accuracy:\", test_accuracy)\n                \n        return train_accuracy, test_accuracy, parameters"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"BF1E53D78DDA41BC89EF1F8BFAC5506E","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"运行以下单元格以训练模型100个epoch。检查第0和第5个阶段之后的损失是否与我们的输出匹配。如果不是，请停止单元格并检查你的代码！"},{"cell_type":"code","execution_count":15,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"C838332E4477484C8CC2564D82FC6E09","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Cost after epoch 0: 1.921332\nCost after epoch 5: 1.904156\nCost after epoch 10: 1.904309\nCost after epoch 15: 1.904477\nCost after epoch 20: 1.901876\nCost after epoch 25: 1.784077\nCost after epoch 30: 1.681052\nCost after epoch 35: 1.618207\nCost after epoch 40: 1.597972\nCost after epoch 45: 1.566707\nCost after epoch 50: 1.554486\nCost after epoch 55: 1.502187\nCost after epoch 60: 1.461035\nCost after epoch 65: 1.304477\nCost after epoch 70: 1.201501\nCost after epoch 75: 1.144230\nCost after epoch 80: 1.098368\nCost after epoch 85: 1.077411\nCost after epoch 90: 1.043173\nCost after epoch 95: 1.022620\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/C838332E4477484C8CC2564D82FC6E09/q1q37jvhm1.png\">"},"transient":{}},{"output_type":"stream","text":"Tensor(\"Mean_1:0\", shape=(), dtype=float32)\nTrain Accuracy: 0.6638889\nTest Accuracy: 0.55\n","name":"stdout"}],"source":"_, _, parameters = model(X_train, Y_train, X_test, Y_test)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"36D44E394A3E4A22A8FEC7935BF2B28F","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \n尽管它可能无法完美匹配，但是你的预期输出应该接近我们的预期，并且你的损失值应该降低。\nCost after epoch 0: 1.921332\nCost after epoch 5: 1.904156\n...\nCost after epoch 0: 1.921332\nCost after epoch 5: 1.904156\n\n\nTrain Accuracy: 0.6638889\nTest Accuracy: 0.55"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"A6002CC1E6CE4D5991BA953A67D74E9E","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"Nice！你已经完成了作业并建立了一个模型，该模型可以在测试集上以几乎80％的精度识别SIGN手势，如果你愿意，可以随时使用此数据集。实际上，你可以通过花费更多时间调整超参数或使用正则化来提高其准确性（因为该模型显然具有很高的方差）。"},{"cell_type":"code","execution_count":16,"metadata":{"slideshow":{"slide_type":"slide"},"id":"9BBF4784DFCC4C6FA9CE0F48C4479DF5","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: `imread` is deprecated!\n`imread` is deprecated in SciPy 1.0.0.\nUse ``matplotlib.pyplot.imread`` instead.\n  \n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: `imresize` is deprecated!\n`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\nUse Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n  This is separate from the ipykernel package so we can avoid doing imports until\n","name":"stderr"},{"output_type":"execute_result","metadata":{},"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f1aa584fe80>"},"transient":{},"execution_count":16},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/9BBF4784DFCC4C6FA9CE0F48C4479DF5/q1q3wn9wf7.png\">"},"transient":{}}],"source":"fname = \"thumbs_up.jpg\"\nimage = np.array(ndimage.imread(fname, flatten=False))\nmy_image = scipy.misc.imresize(image, size=(64,64))\nplt.imshow(my_image)"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}