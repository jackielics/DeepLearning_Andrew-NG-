{"cells":[{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"1A3AF5455F3D44B3887C3983BEA1CB48","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"# 人脸识别 - the Happy House\n\n欢迎来到第4周的作业1！在此次作业中，你将学习构建人脸识别系统。\n此笔记本中许多内容想法都来自于[FaceNet](https://arxiv.org/pdf/1503.03832.pdf)。\n在视频教程中，我们还提到了[DeepFace](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)。\n\n人脸识别问题通常分为两类：\n\n- **人脸验证**：比如在某些机场，系统通过扫描你的护照，然后确认你（携带护照的人）是本人，从而通过海关。也比如使用脸部解锁的手机。通常这一类是1：1匹配的问题。\n- **人脸识别**：例如讲座显示了一个百度员工进入办公室的人脸识别[视频](https://www.youtube.com/watch?v=wr4rx0Spihs)。此类则是1：K匹配问题。\n\nFaceNet网络将人脸图像编码为128个数字向量并学习，通过比较两个这样的向量，以确定两个图片是否是同一个人。\n    \n\n**在此作业中，你将：**\n\n- 实现triplet loss 损失函数\n- 使用预先训练的模型将人脸图像映射为128维编码\n- 使用这些编码实现人脸验证和人脸识别\n\n在本练习中，我们将使用预训练的模型，该模型使用\"channels first\"来表示ConvNet激活，而不是像先前的编程作业一样使用\"channels last\"。换句话说，一批图像将具有$(m, n_C, n_H, n_W)$的维度 而非$(m, n_H, n_W, n_C)$。这两种方式在开源实现中都有相当大的吸引力。深度学习中也没有统一的标准。\n\n首先让我们加载所需的软件包。\n"},{"metadata":{"id":"4B3BFB81ABBA4E68BAE5C582F7DB0D38","slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":true,"jupyter":{},"tags":[],"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"/home/kesci/input/deeplearning113246\n","name":"stdout"}],"source":"cd /home/kesci/input/deeplearning113246","execution_count":1},{"cell_type":"code","execution_count":2,"metadata":{"slideshow":{"slide_type":"slide"},"id":"551D9912F8C74CCE8FBF7C2DA1772E03","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\nMatplotlib is building the font cache using fc-list. This may take a moment.\n","name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"threshold must be numeric and non-NAN, try sys.maxsize for untruncated representation","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-7bae747a1a03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36mset_printoptions\u001b[0;34m(precision, threshold, edgeitems, linewidth, suppress, nanstr, infstr, formatter, sign, floatmode, **kwarg)\u001b[0m\n\u001b[1;32m    244\u001b[0m     opt = _make_options_dict(precision, threshold, edgeitems, linewidth,\n\u001b[1;32m    245\u001b[0m                              \u001b[0msuppress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnanstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                              floatmode, legacy)\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0;31m# formatter is always reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'formatter'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/arrayprint.py\u001b[0m in \u001b[0;36m_make_options_dict\u001b[0;34m(precision, threshold, edgeitems, linewidth, suppress, nanstr, infstr, sign, formatter, floatmode, legacy)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# forbid the bad threshold arg suggested by stack overflow, gh-12351\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             raise ValueError(\"threshold must be numeric and non-NAN, try \"\n\u001b[0m\u001b[1;32m     94\u001b[0m                              \"sys.maxsize for untruncated representation\")\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: threshold must be numeric and non-NAN, try sys.maxsize for untruncated representation"]}],"source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\nfrom keras.layers.merge import Concatenate\nfrom keras.layers.core import Lambda, Flatten, Dense\nfrom keras.initializers import glorot_uniform\nfrom keras.engine.topology import Layer\nfrom keras import backend as K\nK.set_image_data_format('channels_first')\nimport cv2\nimport os\nimport numpy as np\nfrom numpy import genfromtxt\nimport pandas as pd\nimport tensorflow as tf\nfrom fr_utils import *\nfrom inception_blocks_v2 import *\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\nnp.set_printoptions(threshold=np.nan)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"357989529ADA4BDDB1F478951DF9495D","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 0 人脸验证\n\n在人脸验证中，你将获得两张图像，并且必须确定它们是否属于同一个人。最简单的方法是逐像素比较两个图像。如果原始图像之间的距离小于选定的阈值，则可能是同一个人！\n\n![Image Name](https://cdn.kesci.com/upload/image/q1yuur1uj2.png?imageView2/0/w/960/h/960)\n\n**图 1**"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"DBE08BDBCCC54071B29B28AD04C6CE1F","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"当然，此算法的性能确实很差，因为像素值会由于光照，人脸方向，甚至头部位置的微小变化等因素而急剧变化。\n\n你会发现，可以编码$f(img)$而不是使用原始图像，这样对该编码进行逐元素比较就可以更准确地判断两张图片是否属于同一个人。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"9781ED52E6C840CD84578FBA91F46FE1","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"## 1 将人脸图像编码为128维向量\n\n### 1.1 使用ConvNet计算编码\n\nFaceNet模型需要大量训练数据并需要很长时间去训练。因此，按照深度学习中的常规应用做法，我们加载已经训练好的权重。网络结构遵循[Szegedy *et al.*](https://arxiv.org/abs/1409.4842)中的Inception模型。我们提供了初始网络实现。你可以查看文件`inception_blocks.py`以了解其实现方式（转到Jupyter笔记本顶部的\"File->Open...\"）。\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"0FEA5231ADBF4BBABB5E10F6F5A4A2EF","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"你需要知道的关键事项是：\n\n- 该网络使用96x96尺寸的RGB图像作为输入。具体来说，输入一张人脸图像（或一批$m$人脸图像）作为维度为$(m, n_C, n_H, n_W) = (m, 3, 96, 96)$ 的张量\n- 输出维度为$(m, 128)$的矩阵，该矩阵将每个输入的面部图像编码为128维向量\n\n运行下面的单元格以创建人脸图像模型。"},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"1901EFA0F9CC40068ED45150676403C1","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n","name":"stdout"}],"source":"FRmodel = faceRecoModel(input_shape=(3, 96, 96))"},{"cell_type":"code","execution_count":4,"metadata":{"slideshow":{"slide_type":"slide"},"id":"59B514B6406644018E74DF92DF8E5370","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Total Params: 3743280\n","name":"stdout"}],"source":"print(\"Total Params:\", FRmodel.count_params())"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"BD685571480540E8871088FE889F47EE","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**：\nTotal Params: 3743280"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"97DF9D87602D4BAE8331768AD5DDF40A","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"\n通过使用128个神经元组成的全连接层作为最后一层，该模型可确保输出是大小为128的编码向量。然后，使用该编码比较两个人脸图像，如下所示：\n![Image Name](https://cdn.kesci.com/upload/image/q1yuxatz7o.png?imageView2/0/w/960/h/960)\n\n**图 2**：通过计算两种编码和阈值之间的距离，以确定两张图片是否代表同一个人\n\n如果满足以下条件，编码将是一种不错的选择：\n- 同一个人的两张图像的编码彼此非常相似\n- 不同人的两幅图像的编码差距明显\n\ntriplet loss损失函数促进此实现，它尝试将“同一个人（锚点和正向）”的两个图像的编码“推”得更近，同时将另外一个人（锚点，负向）的两个图像的编码“拉”得更远。\n\n![Image Name](https://cdn.kesci.com/upload/image/q1yuxywaid.png?imageView2/0/w/960/h/960)\n\n**图 3**：\n在下一部分中，我们将从左到右调用图片：锚点（A），正向（P），负向（N）"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"ED4855D68A30440C84F1C4DD24552CB7","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 1.2 三元组损失\n\n对于图像$x$，其编码表示为$f(x)$，其中$f$是神经网络的计算函数。\n![Image Name](https://cdn.kesci.com/upload/image/q1yv306dba.png?imageView2/0/w/960/h/960)\n我们在模型的末尾添加一个标准化步骤，以使$\\mid \\mid f(x) \\mid \\mid_2 = 1$（意味着编码向量应为范数1）。\n\n训练将使用三组图像 $(A, P, N)$：\n\n- A是“锚示例”图像：人的照片。\n- P是“正示例”图像：与锚示例图像相同的人的照片。\n- N是“负示例”图像：与锚示例图像不同的人的照片。\n\n这些图像是从我们的训练集中选取的。我们使用$(A^{(i)}, P^{(i)}, N^{(i)})$来表示第$i$个训练示例。\n\n如果你想确定一个人的图像$A^{(i)}$比负例图像$N^{(i)}$更接近正例图像$P^{(i)}$ 至少要保证$\\alpha$：\n\n$$\n\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 + \\alpha < \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2\n$$\n\n因此，你需要最小化以下\"triplet cost\"：\n\n$$\n\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\underbrace{\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha \\large ] \\small_+ \\tag{3}\n$$\n\n在这里，我们使用符号\"$[z]_+$\"表示$max(z,0)$。\n\n注意：\n- 项（1）是给定三元组的锚示例“A”与正示例“P”之间的平方距离；期望最小化的值。\n- 项（2）是给定三元组的锚示例“A”和负示例“N”之间的平方距离，期望该值相对较大，因此在它前面有一个负号是有意义的。\n- $\\alpha$称为边距。它是一个超参数可以手动调节。我们将使用$\\alpha = 0.2$。\n\n大多数实现方法还需对编码向量进行标准化以使其范数等于1（即$\\mid \\mid f(img)\\mid \\mid_2$=1）\n\n**练习**：实现公式（3）定义的三元组损失。包含4个步骤：\n1. 计算“锚示例”和“正示例”编码之间的距离：$\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2$\n1. 计算“锚示例”和“负示例”编码之间的距离：$\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$\n1. 根据每个训练示例计算公式：\n$\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid - \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2 + \\alpha$\n1. 通过将最大值取为零并对训练示例求和来计算完整公式：\n$$\n\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 - \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2+ \\alpha \\large ] \\small_+ \\tag{3}\n$$\n\n一些有用的函数：`tf.reduce_sum()`, `tf.square()`, `tf.subtract()`, `tf.add()`, `tf.maximum()`。\n对于步骤1和步骤2，你需要加上$\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2$ and $\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$ ，而在第4步中，你需要将训练示例求总。\n"},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"BCBA7AA6981B4E879016BF7AC5422240","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: triplet_loss\n\ndef triplet_loss(y_true, y_pred, alpha = 0.2):\n    \"\"\"\n    Implementation of the triplet loss as defined by formula (3)\n    \n    Arguments:\n    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n    y_pred -- python list containing three objects:\n            anchor -- the encodings for the anchor images, of shape (None, 128)\n            positive -- the encodings for the positive images, of shape (None, 128)\n            negative -- the encodings for the negative images, of shape (None, 128)\n    \n    Returns:\n    loss -- real number, value of the loss\n    \"\"\"\n    \n    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n    \n    ### START CODE HERE ### (≈ 4 lines)\n    # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n    pos_dist =  tf.reduce_sum(tf.square(tf.subtract(anchor, positive)))# ,axis=-1 528.143\n    # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n    neg_dist =  tf.reduce_sum(tf.square(tf.subtract(anchor, negative)))\n    # Step 3: subtract the two previous distances and add alpha.\n    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist),alpha)\n    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n    ### END CODE HERE ###\n    #如果加上axis=-1，计算出的loss就和expected output相同，但是不能通过作业验证\n    return loss"},{"cell_type":"code","execution_count":6,"metadata":{"slideshow":{"slide_type":"slide"},"id":"E9CE211248484B419524D44E379BA82A","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"loss = 350.02716\n","name":"stdout"}],"source":"with tf.Session() as test:\n    tf.set_random_seed(1)\n    y_true = (None, None, None)\n    y_pred = (tf.random_normal([3, 128], mean=6, stddev=0.1, seed = 1),\n              tf.random_normal([3, 128], mean=1, stddev=1, seed = 1),\n              tf.random_normal([3, 128], mean=3, stddev=4, seed = 1))\n    loss = triplet_loss(y_true, y_pred)\n    \n    print(\"loss = \" + str(loss.eval()))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"F5BC779EC40C47F99215AABB5764091A","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nloss=528.143\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"71901D3DDD2446139C68B71FC2BDCCA5","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 2 加载训练后的模型\n\n通过最小化三元组损失来训练FaceNet。但是由于训练需要大量数据和计算，因此在这里我们不会从头开始进行训练。我们加载以前训练好的模型。使用以下单元格加载模型；这将花费几分钟才能运行。"},{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"4AF1E315DAC34962989C12ECF00F9501","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\nload_weights_from_FaceNet(FRmodel)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"8FEF78A9BF274C269233193278FBC8FF","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"这是三个示例的编码距离：\n\n![Image Name](https://cdn.kesci.com/upload/image/q1yv5rn07y.png?imageView2/0/w/960/h/960)\n\n**图 4**：\n三人的编码距离输出示例\n\n现在，让我们使用此模型执行人脸验证和人脸识别！"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"82D7F569E2D94049AFFF634B72E19346","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 3 模型应用"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"7DEB3B573741469F81C3286EDC470A52","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"回到 the Happy House（数据集介绍可参考[L4W2KT作业](https://www.kesci.com/home/project/5ddf8f59ca27f8002c4ad2ed)）！ 自从你在较早的任务中实现了对房子的幸福感识别以来，居民就过着幸福的生活。\n\n但是，有几个问题不断出现：快乐之家变得如此高兴，以至于附近的每个快乐的人都在你的客厅里闲逛。房屋变得很拥挤，这对里面的居民产生了负面影响。所有其他快乐的人也在吃你的食物。\n\n因此，你决定更改门禁政策，不让随机快乐的人进入，即使他们Happy！相反，你想构建一个“人脸验证”系统，以便仅允许指定列表中的人员进入。要被录取，每个人都必须刷一张ID卡（识别卡）才能触发门上的面部识别系统，然后检查他们是否是本人。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"55049044E9BE45A2853AF4BFF9D851D4","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"### 3.1 人脸验证\n\n让我们建立一个数据库，其中包含允许进入幸福屋的人的编码向量。我们使用`img_to_encoding(image_path, model)`生成编码，它基本上在指定的图像上运行模型的正向传播。\n\n运行以下代码以构建数据库（以python字典表示）。该数据库将每个人的姓名映射为其面部的128维编码。"},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"914180B99BF443C4916BDDF48FA91B5B","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"database = {}\ndatabase[\"danielle\"] = img_to_encoding(\"images/danielle.png\", FRmodel)\ndatabase[\"younes\"] = img_to_encoding(\"images/younes.jpg\", FRmodel)\ndatabase[\"tian\"] = img_to_encoding(\"images/tian.jpg\", FRmodel)\ndatabase[\"andrew\"] = img_to_encoding(\"images/andrew.jpg\", FRmodel)\ndatabase[\"kian\"] = img_to_encoding(\"images/kian.jpg\", FRmodel)\ndatabase[\"dan\"] = img_to_encoding(\"images/dan.jpg\", FRmodel)\ndatabase[\"sebastiano\"] = img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\ndatabase[\"bertrand\"] = img_to_encoding(\"images/bertrand.jpg\", FRmodel)\ndatabase[\"kevin\"] = img_to_encoding(\"images/kevin.jpg\", FRmodel)\ndatabase[\"felix\"] = img_to_encoding(\"images/felix.jpg\", FRmodel)\ndatabase[\"benoit\"] = img_to_encoding(\"images/benoit.jpg\", FRmodel)\ndatabase[\"arnaud\"] = img_to_encoding(\"images/arnaud.jpg\", FRmodel)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"879761F6094E457A87FE29BF7EAE4FB7","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"现在，当有人出现在你的前门并刷他们的身份证时，你可以在数据库中查找他们的编码，并用它来检查站在前门的人是否是本人。\n\n**练习**：实现 verify() 函数，该函数检查前门摄像头拍摄到的图片（`image_path`）是否是本人。你需要执行以下步骤：\n1. 从image_path计算图像的编码\n1. 计算此编码和存储在数据库中的身份图像的编码的距离\n1. 如果距离小于0.7，打开门，否则不要打开。\n\n如上所述，你应该使用L2距离（np.linalg.norm）。（注意：在此实现中，将L2距离而不是L2距离的平方与阈值0.7进行比较。）"},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"3830C14E6E4743E8A577DFB0FB846362","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: verify\n\ndef verify(image_path, identity, database, model):\n    \"\"\"\n    Function that verifies if the person on the \"image_path\" image is \"identity\".\n    \n    Arguments:\n    image_path -- path to an image\n    identity -- string, name of the person you'd like to verify the identity. Has to be a resident of the Happy house.\n    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n    model -- your Inception model instance in Keras\n    \n    Returns:\n    dist -- distance between the image_path and the image of \"identity\" in the database.\n    door_open -- True, if the door should open. False otherwise.\n    \"\"\"\n    \n    ### START CODE HERE ###\n\n    # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line)\n    encoding = img_to_encoding(image_path,model)\n\n    # Step 2: Compute distance with identity's image (≈ 1 line)\n    dist = np.linalg.norm(encoding-database[identity])\n\n    # Step 3: Open the door if dist < 0.7, else don't open (≈ 3 lines)\n    if dist<0.7:\n        print(\"It's \" + str(identity) + \", welcome home!\")\n        door_open = True\n    else:\n        print(\"It's not \" + str(identity) + \", please go away\")\n        door_open = False\n\n    ### END CODE HERE ###\n        \n    return dist, door_open"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"6474DBEDE1CF47E59C26189DDB590BB5","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"尤恩斯（Younes）试图进入快乐之家，然后相机为他拍照（\"images/camera_0.jpg\"）。让我们在这张图片上运行你的验证算法：\n\n<img src=\"images/camera_0.jpg\" style=\"width:100px;height:100px;\">"},{"cell_type":"code","execution_count":10,"metadata":{"slideshow":{"slide_type":"slide"},"id":"5B6E529FBDA84E8D8E9B5C98182A9AFB","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"It's younes, welcome home!\n","name":"stdout"},{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(0.67100745, True)"},"transient":{},"execution_count":10}],"source":"verify(\"images/camera_0.jpg\", \"younes\", database, FRmodel)"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"B9DFCC76CB5A44BFB5BC1FEAF6559B5D","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nIt's younes, welcome home!\n(0.67100745, True)"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"78FBDA63278F46F18843844FB1AB4C62","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"上周末破坏水族馆的Benoit已被禁止进入房屋，并已从数据库中删除。他偷了Kian的身份证，然后回到屋子里，试图把自己打扮成Kian。 前门摄像头拍摄了Benoit的照片（\"images/camera_2.jpg\"）。让我们运行验证算法来检查benoit是否可以进入。\n<img src=\"images/camera_2.jpg\" style=\"width:100px;height:100px;\">"},{"cell_type":"code","execution_count":11,"metadata":{"slideshow":{"slide_type":"slide"},"id":"CDC5C9F9AEC6493FBC4C673DA7ED6C5C","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"It's not kian, please go away\n","name":"stdout"},{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(0.8580015, False)"},"transient":{},"execution_count":11}],"source":"verify(\"images/camera_2.jpg\", \"kian\", database, FRmodel)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"7BF6224F31614B158E6B3F70D7EF4108","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nIt's not kian, please go away\n(0.8580015, False)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"2DD3BF9B146240BB8F58AF38C4ED453E","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 3.2 人脸识别\n\n你的人脸验证系统在大部分情况下运行良好。但是自从肯恩（Kian）的身份证被盗以来，那天晚上他回到家中时，他进不了门了！\n\n为了减少这种恶作剧，你想将人脸验证系统更改为人脸识别系统。这样，不再需要携带身份证。授权人员可以走到房屋前，前门将为他们解锁！\n\n为此，你将实现一个人脸识别系统，该系统将图像作为输入，并确定该图像是否是授权人员之一。与以前的人脸验证系统不同，我们将不再获得一个人的名字作为其他输入。\n\n**练习**：实现`who_is_it()`，你需要执行以下步骤：\n1. 从image_path计算图像的目标编码\n1. 从数据库中查找与目标编码距离最短的编码。\n    - 将`min_dist`变量初始化为足够大的数字（100）。这将帮助你跟踪最接近输入编码的编码。\n    - 遍历数据库字典的名称和编码。循环使用`for (name, db_enc) in database.items()`。\n        - 计算目标“编码”与数据库中当前“编码”之间的L2距离。\n        - 如果此距离小于min_dist，则将min_dist设置为dist，并将identity设置为name。"},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"E935B083CD17495B8BCC57AFC7430F88","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: who_is_it\n\ndef who_is_it(image_path, database, model):\n    \"\"\"\n    Implements face recognition for the happy house by finding who is the person on the image_path image.\n    \n    Arguments:\n    image_path -- path to an image\n    database -- database containing image encodings along with the name of the person on the image\n    model -- your Inception model instance in Keras\n    \n    Returns:\n    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n    identity -- string, the name prediction for the person on image_path\n    \"\"\"\n    \n    ### START CODE HERE ### \n\n    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)\n    encoding = img_to_encoding(image_path,model)\n\n    ## Step 2: Find the closest encoding ##\n\n    # Initialize \"min_dist\" to a large value, say 100 (≈1 line)\n    min_dist = 100\n\n    # Loop over the database dictionary's names and encodings.\n    for (name, db_enc) in database.items():\n\n        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database. (≈ 1 line)\n        dist = np.linalg.norm(encoding-db_enc)\n\n        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n        if dist<min_dist:\n            min_dist = dist\n            identity = name\n\n    ### END CODE HERE ###\n    \n    if min_dist > 0.7:\n        print(\"Not in the database.\")\n    else:\n        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n        \n    return min_dist, identity"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"66016E5507E745B19C87013154BAB6B2","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"尤恩斯（Younes）在前门，相机为他拍照（\"images/camera_0.jpg\"）。让我们看看你的who_it_is()算法是否可以识别Younes。"},{"cell_type":"code","execution_count":13,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"952A85B03416459E8F8011EDC4497552","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"it's younes, the distance is 0.67100745\n","name":"stdout"},{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(0.67100745, 'younes')"},"transient":{},"execution_count":13}],"source":"who_is_it(\"images/camera_0.jpg\", database, FRmodel)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"F8CE7B4852DE4D37801956E6A65AAAC1","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\nit's younes, the distance is 0.67100745\n(0.67100745, 'younes')"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"4E09DD8F041F4170B079CFB96DB7A4C0","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"你可以将\"`camera_0.jpg`\"（younes的图片）更改为\"`camera_1.jpg`\" （bertrand的图片），然后查看结果。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"54369B8767D248499048DF5FD226E4A4","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"你的快乐之家运作良好。它只允许经过授权的人员进入，而人们也不再需要携带身份证！\n\n现在你已经了解了最新的人脸识别系统是如何工作的。\n\n尽管我们不会在这里实现它，但是这里有一些方法可以进一步改进算法：\n- 将每个人的更多图像（在不同的光照条件下，在不同的日子等拍摄的图像）放入数据库中。然后给定新图像，将新面孔与人物的多张图片进行比较以提高准确性。\n- 裁剪仅包含脸部的图像，并减少脸部周围的“边框”区域。该预处理去除了面部周围的一些无关像素，并且还使算法更加健壮。\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"97443308EC4B4F6B8D0783A685B6E636","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**你应该记住**：\n- 人脸验证解决了更简单的1：1匹配问题；人脸识别则解决了更难的1：K匹配问题。\n- 三元组损失是用于训练神经网络以学习面部图像编码的有效损失函数。\n- 相同的编码可用于验证和识别。通过测量两个图像的编码之间的距离，可以确定它们是否是同一个人的照片。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"1C328230A46247EE864606CE6D4F2409","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"恭喜你完成此作业！\n"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"3406628B270343CC840993A3894514C5","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"### 参考:\n\n- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n- Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf) \n- The pretrained model we use is inspired by Victor Sy Wang's implementation and was loaded using his code: https://github.com/iwantooxxoox/Keras-OpenFace.\n- Our implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet \n\n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}