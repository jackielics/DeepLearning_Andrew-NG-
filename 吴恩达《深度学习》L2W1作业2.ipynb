{"cells":[{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"CB209E90EA7541AA80737679A13F9BEC","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"# 正则化\n\n欢迎来到本周的第二次作业。 \n深度学习模型具有很高的灵活性和能力，如果训练数据集不够大，**将会造成一个严重的问题--过拟合**。尽管它在训练集上效果很好，但是学到的网络**不能应用到测试集中！**\n\n**你将学习：** 在深度学习模型中使用正则化。\n\n首先导入要使用的包。"},{"metadata":{"id":"8D62F49135BA49508CD34905018821EB","slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"/home/kesci/input/deeplearning34288\n","name":"stdout"}],"source":"cd ../input/deeplearning34288","execution_count":2},{"cell_type":"code","execution_count":3,"metadata":{"slideshow":{"slide_type":"slide"},"id":"4107473570C645028DBBB2C89E02692F","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"/home/kesci/input/deeplearning34288/reg_utils.py:85: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n  assert(parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n/home/kesci/input/deeplearning34288/reg_utils.py:86: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n  assert(parameters['W' + str(l)].shape == layer_dims[l], 1)\n","name":"stderr"}],"source":"# import packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec\nfrom reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters\nimport sklearn\nimport sklearn.datasets\nimport scipy.io\nfrom testCases import *\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"E8ABFE76EA2C4C8B960AA6EA30364678","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**问题陈述**：你刚刚被法国足球公司聘为AI专家。他们希望你推荐预测法国守门员将球踢出的位置，以便法国队的球员可以用头将球击中。\n\n![Image Name](https://cdn.kesci.com/upload/image/q1asazcc3a.png?imageView2/0/w/960/h/960)\n\n**图1**：\n**足球场**\n守门员将球踢到空中，每支球队的球员都在尽力用头击球\n\n\n他们为你提供了法国过去10场比赛的二维数据集。"},{"cell_type":"code","execution_count":5,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"CDD575669C804B8884D8ACF1E1BA8E89","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/CDD575669C804B8884D8ACF1E1BA8E89/qhmq1ytrhm.png\">"},"transient":{}}],"source":"train_X, train_Y, test_X, test_Y = load_2D_dataset()"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"9EC7E933F7914A7C88B400DAB4D1F591","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"数据中每个点对应于足球场上的位置，在该位置上，法国守门员从足球场左侧射出球后，足球运动员用他/她的头部击中了球。\n- 如果圆点为蓝色，则表示法国球员设法用头部将球击中\n- 如果圆点为红色，则表示另一支球队的球员用头撞球\n\n**你的目标**：运用深度学习模型预测守门员应将球踢到球场上的位置。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"52AE1684976345BB8C4E8EC43066464F","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"**数据集分析**：该数据集含有噪声，但看起来一条将左上半部分（蓝色）与右下半部分（红色）分开的对角线会很比较有效。\n\n你将首先尝试非正则化模型。然后学习如何对其进行正则化，并决定选择哪种模型来解决法国足球公司的问题。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"BFDD0CB9BCA449B9949A71F35CC49907","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"## 1 非正则化模型\n\n你将使用以下神经网络（已为你实现），可以如下使用此模型：\n- 在*regularization mode*中，通过`lambd`将输入设置为非零值。我们使用`lambd`代替`lambda`，因为`lambda`是Python中的保留关键字。\n- 在*dropout mode*中，将`keep_prob`设置为小于1的值\n\n首先，你将尝试不进行任何正则化的模型。然后，你将实现：\n- *L2 正则化* 函数：`compute_cost_with_regularization()`和`backward_propagation_with_regularization()`\n- *Dropout* 函数：`forward_propagation_with_dropout()`和`backward_propagation_with_dropout()`\n\n在每个部分中，你都将使用正确的输入来运行此模型，以便它调用已实现的函数。查看以下代码以熟悉该模型。"},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"02ABEAD0DEBD400E9C2E58C71280FEF1","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n    \"\"\"\n    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n    learning_rate -- learning rate of the optimization\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- If True, print the cost every 10000 iterations\n    lambd -- regularization hyperparameter, scalar\n    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n    \n    Returns:\n    parameters -- parameters learned by the model. They can then be used to predict.\n    \"\"\"\n        \n    grads = {}\n    costs = []                            # to keep track of the cost\n    m = X.shape[1]                        # number of examples\n    layers_dims = [X.shape[0], 20, 3, 1]\n    \n    # Initialize parameters dictionary.\n    # 前面介绍过三种初始化参数的方式\n    parameters = initialize_parameters(layers_dims)\n\n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n        if keep_prob == 1:\n            a3, cache = forward_propagation(X, parameters)\n        elif keep_prob < 1:\n            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n        # 需要知道how to drop-out\n        # Cost function\n        if lambd == 0:\n            cost = compute_cost(a3, Y)\n        else:\n            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n            \n        # Backward propagation.\n        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, \n                                            # but this assignment will only explore one at a time\n        if lambd == 0 and keep_prob == 1:\n            grads = backward_propagation(X, Y, cache)\n        elif lambd != 0:\n            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n        elif keep_prob < 1:\n            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n        \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the loss every 10000 iterations\n        if print_cost and i % 10000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n        if print_cost and i % 1000 == 0:\n            costs.append(cost)\n    \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (x1,000)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"3ED21A73F8814D4C9845BE333528C7DC","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"让我们在不进行任何正则化的情况下训练模型，并观察训练/测试集的准确性。"},{"cell_type":"code","execution_count":7,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"FBE302FF15E14275A686B3FF0B890416","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Cost after iteration 0: 0.6557412523481002\nCost after iteration 10000: 0.1632998752572417\nCost after iteration 20000: 0.13851642423284755\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/FBE302FF15E14275A686B3FF0B890416/qhmq27ztb6.png\">"},"transient":{}},{"output_type":"stream","text":"On the training set:\nAccuracy: 0.9478672985781991\nOn the test set:\nAccuracy: 0.915\n","name":"stdout"}],"source":"parameters = model(train_X, train_Y)\nprint (\"On the training set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)"},{"metadata":{"id":"E482A33C0EE744A38A71681706F85F05","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"## 思考\nWHY 加入了正则化之后，迭代之初COST下降得如此之快？"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"AD73F686654E49CBBB403D327D5A7304","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"训练精度为94.8％，而测试精度为91.5％。这是**基准模型**的表现（你将观察到正则化对该模型的影响）。运行以下代码以绘制模型的决策边界。"},{"cell_type":"code","execution_count":6,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"9B6DEE169B6447008820D6EAC4B61F9A","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/9B6DEE169B6447008820D6EAC4B61F9A/q1aso5ofx2.png\">"},"transient":{}}],"source":"plt.title(\"Model without regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"2E179DF3BFB643A7874456DE01F44658","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"非正则化模型显然过度拟合了训练集，拟合了一些**噪声点**！现在让我们看一下减少过拟合的两种手段。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"34E0CC13C09D4D099E34E21AAC3E0DC0","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 2 L2正则化\n\n避免过拟合的标准方法称为 **L2正则化**，它将损失函数从：\n$$\nJ = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}\n$$\n修改到：\n$$\nJ_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}\n$$\n\n让我们修改损失并观察结果。\n\n**练习**：实现`compute_cost_with_regularization（）`，以计算公式（2）的损失。要计算$\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$ ，请使用：\n```python\nnp.sum(np.square(Wl))\n```\n请注意，你必须对$W^{[1]}$，$W^{[2]}$和$W^{[3]}$执行上述代码操作，然后将三个项相加并乘以$\\frac{1}{m}\\frac{\\lambda}{2}$。"},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"C8A9DF4B5B4F455F8EE9D1F8C37E4FF7","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: compute_cost_with_regularization\n\ndef compute_cost_with_regularization(A3, Y, parameters, lambd):\n    \"\"\"\n    Implement the cost function with L2 regularization. See formula (2) above.\n    \n    Arguments:\n    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    parameters -- python dictionary containing parameters of the model\n    \n    Returns:\n    cost - value of the regularized loss function (formula (2))\n    \"\"\"\n    m = Y.shape[1]\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    W3 = parameters[\"W3\"]\n    \n    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n    \n    ### START CODE HERE ### (approx. 1 line)\n    L2_regularization_cost = (1./m*lambd/2)*(np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))\n    ### END CODER HERE ###\n    \n    cost = cross_entropy_cost + L2_regularization_cost\n    \n    return cost"},{"cell_type":"code","execution_count":9,"metadata":{"slideshow":{"slide_type":"slide"},"id":"A34C7F6CDADB44B0BE4F220FD1066BB0","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"cost = 1.7864859451590758\n","name":"stdout"}],"source":"A3, Y_assess, parameters = compute_cost_with_regularization_test_case()\n\nprint(\"cost = \" + str(compute_cost_with_regularization(A3, Y_assess, parameters, lambd = 0.1)))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"9FBE77E1AA05408B8B5435E192EE5EEC","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \ncost = 1.7864859451590758"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"29C07E55C630449E9FEF8CFB44E09E31","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"当然，因为你更改了损失，所以还必须更改反向传播！ 必须针对新损失函数计算所有梯度。\n\n**练习**：实现正则化后的反向传播。更改仅涉及dW1，dW2和dW3。对于每一个，你必须添加正则化项的梯度($\\frac{d}{dW} ( \\frac{1}{2}\\frac{\\lambda}{m}  W^2) = \\frac{\\lambda}{m} W$)。"},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"F9DCD242E50A454A99A8E4A8806EC80B","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: backward_propagation_with_regularization\n\ndef backward_propagation_with_regularization(X, Y, cache, lambd):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n\n    Arguments:\n    X -- input dataset, of shape (input size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation()\n    lambd -- regularization hyperparameter, scalar\n\n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n\n    m = X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n\n    dZ3 = A3 - Y\n\n    ### START CODE HERE ### (approx. 1 line)\n    dW3 = 1./m * np.dot(dZ3, A2.T) + lambd/m * W3\n    ### END CODE HERE ###\n    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n\n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n    ### START CODE HERE ### (approx. 1 line)\n    dW2 = 1./m * np.dot(dZ2, A1.T) + lambd/m * W2\n    ### END CODE HERE ###\n    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n\n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n    ### START CODE HERE ### (approx. 1 line)\n    dW1 = 1./m * np.dot(dZ1, X.T) + lambd/m * W1\n    ### END CODE HERE ###\n    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n\n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n\n    return gradients"},{"cell_type":"code","execution_count":10,"metadata":{"slideshow":{"slide_type":"slide"},"id":"FE08BE486CF44F728EF2B169D4DC6DB1","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"dW1 = [[-0.25604646  0.12298827 -0.28297129]\n [-0.17706303  0.34536094 -0.4410571 ]]\ndW2 = [[ 0.79276486  0.85133918]\n [-0.0957219  -0.01720463]\n [-0.13100772 -0.03750433]]\ndW3 = [[-1.77691347 -0.11832879 -0.09397446]]\n","name":"stdout"}],"source":"X_assess, Y_assess, cache = backward_propagation_with_regularization_test_case()\n\ngrads = backward_propagation_with_regularization(X_assess, Y_assess, cache, lambd = 0.7)\nprint (\"dW1 = \"+ str(grads[\"dW1\"]))\nprint (\"dW2 = \"+ str(grads[\"dW2\"]))\nprint (\"dW3 = \"+ str(grads[\"dW3\"]))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"94A3EB6F374C488A94B56C9C541A4B6A","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**:\ndW1 = [[-0.25604646  0.12298827 -0.28297129]\n [-0.17706303  0.34536094 -0.4410571 ]]\ndW2 = [[ 0.79276486  0.85133918]\n [-0.0957219  -0.01720463]\n [-0.13100772 -0.03750433]]\ndW3 = [[-1.77691347 -0.11832879 -0.09397446]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"4196AC738672487489DBE56B2EA70AD7","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"现在让我们使用L2正则化$(\\lambda = 0.7)$运行的模型。`model（）`函数将调用：\n- `compute_cost_with_regularization`代替`compute_cost`\n- `backward_propagation_with_regularization`代替`backward_propagation`"},{"cell_type":"code","execution_count":11,"metadata":{"scrolled":false,"slideshow":{"slide_type":"slide"},"id":"B1D9E86511C14DAFBD84B93D7D239B2C","collapsed":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Cost after iteration 0: 0.6974484493131264\nCost after iteration 10000: 0.2684918873282239\nCost after iteration 20000: 0.2680916337127301\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/B1D9E86511C14DAFBD84B93D7D239B2C/q1asr5p8x1.png\">"},"transient":{}},{"output_type":"stream","text":"On the train set:\nAccuracy: 0.9383886255924171\nOn the test set:\nAccuracy: 0.93\n","name":"stdout"}],"source":"parameters = model(train_X, train_Y, lambd = 0.7)\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"A9F27658BB2D49788C19694CE1FFEDC9","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"Nice！测试集的准确性提高到93％。你成功拯救了法国足球队！\n\n模型不再过拟合训练数据了。让我们绘制决策边界看一下。"},{"cell_type":"code","execution_count":12,"metadata":{"slideshow":{"slide_type":"slide"},"id":"1E8CEFC44CE6499E9220F0F0F424893E","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/1E8CEFC44CE6499E9220F0F0F424893E/q1asrgaphp.png\">"},"transient":{}}],"source":"plt.title(\"Model with L2-regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"E662C14C5C0E4614B99A7AEA7C27ADF8","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**观察**：\n- $\\lambda$ 的值是你可以调整开发集的超参数。\n- L2正则化使决策边界更**平滑**。如果$\\lambda$ 太大，则也可能“过度平滑”，从而使模型偏差较高。\n\n**L2正则化的原理**：\n\nL2正则化基于以下假设：权重较小的模型比权重较大的模型更简单。因此，通过对损失函数中权重的平方值进行惩罚，可以将所有权重驱动为较小的值。比重太大会使损失过高！这将导致模型更平滑，输出随着输入的变化而变化得更慢。\n\n**你应该记住** L2正则化的影响：\n- 损失计算：\n    - 正则化条件会添加到损失函数中\n- 反向传播函数：\n    - 有关权重矩阵的渐变中还有其他术语\n- 权重最终变小（“权重衰减”）：\n    - 权重被推到较小的值。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"50F3277DDC7143E28E16904CE6C77D00","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 3 Dropout\n\n最后，**Dropout**是广泛用于深度学习的正则化技术。\n**它会在每次迭代中随机关闭一些神经元。** \n观看这两个video，看看这它是什么意思！\n\n```\n要了解Dropout，可以思考与朋友进行以下对话：\n- 朋友：“为什么你需要所有神经元来训练你的网络以分类图像？”。\n- 你：“因为每个神经元都有权重，并且可以学习图像的特定特征/细节/形状。我拥有的神经元越多，模型学习的特征就越丰富！”\n- 朋友：“我知道了，但是你确定你的神经元学习的是不同的特征而不是全部相同的特征吗？”\n- 你：“这是个好问题……同一层中的神经元实际上并不关联。应该绝对有可能让他们学习相同的图像特征/形状/形式/细节...这是多余的。为此应该有一个解决方案。”\n```\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q1au928m1v.gif?imageView2/0/w/960/h/960)\n\n图2：Dropout第二个隐藏层。\n在每次迭代中，你以概率$1 - keep\\_prob$或以概率$keep\\_prob$（此处为50％）关闭此层的每个神经元。关闭的神经元对迭代的正向和反向传播均无助于训练。\n\n![Image Name](https://cdn.kesci.com/upload/image/q1aua8h2gs.gif?imageView2/0/w/960/h/960)\n\n图3：Dropout第一和第三隐藏层。\n$1^{st}$层：我们平均关闭了40％的神经元。$3^{rd}$ 层：我们平均关闭了20％的神经元。\n\n当你关闭某些神经元时，实际上是在修改模型。Dropout背后的想法是，在每次迭代中，你将训练仅使用神经元子集的不同模型。通过Dropout，你的神经元对另一种特定神经元的激活变得不那么敏感，因为另一神经元可能随时关闭。\n\n### 3.1 带有Dropout的正向传播\n\n**练习**：实现带有Dropout的正向传播。你正在使用3层的神经网络，并将为第一和第二隐藏层添加Dropout。我们不会将Dropout应用于输入层或输出层。\n\n**说明**：\n关闭第一层和第二层中的某些神经元。为此，将执行4个步骤：\n1. 在讲座中，我们讨论了使用`np.random.rand（）`创建与$a^{[1]}$形状相同的变量$d^{[1]}$的方法，以随机获取0到1之间的数。在这里，你将使用向量化的实现，创建一个与$A^{[1]}$的矩阵维度相同的随机矩阵$D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}] $。 \n2. 通过对$D^{[1]}$中的值进行阈值设置，将$D^{[1]}$的每个条目设置为0（概率为`1-keep_prob`）或1（概率为`keep_prob`）。提示：将矩阵X的所有条目设置为0（如果概率小于0.5）或1（如果概率大于0.5），则可以执行：`X = (X < 0.5)`。注意0和1分别对应False和True。\n3. 将$A^{[1]}$设置为$A^{[1]} * D^{[1]}$（关闭一些神经元）。你可以将$D^{[1]}$ 视为**掩码，这样当它与另一个矩阵相乘时，关闭某些值**。\n4. 将$A^{[1]}$除以`keep_prob`。通过这样做，你可以确保损失结果仍具有与dropout相同的期望值。（此技术也称为反向dropout）"},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"5E6A6D0C184A40F7932D2073D4D3167F","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: forward_propagation_with_dropout\n\ndef forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n    \"\"\"\n    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n    \n    Arguments:\n    X -- input dataset, of shape (2, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n                    W1 -- weight matrix of shape (20, 2)\n                    b1 -- bias vector of shape (20, 1)\n                    W2 -- weight matrix of shape (3, 20)\n                    b2 -- bias vector of shape (3, 1)\n                    W3 -- weight matrix of shape (1, 3)\n                    b3 -- bias vector of shape (1, 1)\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n    cache -- tuple, information stored for computing the backward propagation\n    \"\"\"\n    \n    np.random.seed(1)\n    \n    # retrieve parameters\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    \n    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n    Z1 = np.dot(W1, X) + b1\n    A1 = relu(Z1)\n    ### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. \n    D1 = np.random.rand(A1.shape[0],A1.shape[1])                                         # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n    D1 = D1 < keep_prob                                      # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n    A1 = A1 * D1                                         # Step 3: shut down some neurons of A1\n    A1 = A1 / keep_prob                                        # Step 4: scale the value of neurons that haven't been shut down\n    ### END CODE HERE ###\n    Z2 = np.dot(W2, A1) + b2\n    A2 = relu(Z2)\n    ### START CODE HERE ### (approx. 4 lines)\n    D2 = np.random.rand(A2.shape[0],A2.shape[1])                                         # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n    D2 = D2 < keep_prob                                         # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)\n    A2 = A2 * D2                                         # Step 3: shut down some neurons of A2\n    A2 = A2 / keep_prob                                      # Step 4: scale the value of neurons that haven't been shut down\n    ### END CODE HERE ###\n    Z3 = np.dot(W3, A2) + b3\n    A3 = sigmoid(Z3)\n\n    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n\n    return A3, cache"},{"cell_type":"code","execution_count":14,"metadata":{"slideshow":{"slide_type":"slide"},"id":"951BC126312D4226839BD0562E8F8148","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"A3 = [[0.36974721 0.00305176 0.04565099 0.49683389 0.36974721]]\n","name":"stdout"}],"source":"X_assess, parameters = forward_propagation_with_dropout_test_case()\n\nA3, cache = forward_propagation_with_dropout(X_assess, parameters, keep_prob = 0.7)\nprint (\"A3 = \" + str(A3))"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"A36AD87CCCCA4E119BC6EC2FF0B6A948","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \nA3 = [[0.36974721 0.00305176 0.04565099 0.49683389 0.36974721]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"9B5333A187934B99BEE6184912C5E724","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"### 3.2 带有dropout的反向传播\n\n**练习**：实现带有dropout的反向传播。和之前一样，训练一个3层的网络。使用存储在缓存中的掩码$D^{[1]}$和$D^{[2]}$，添加dropout到第一和第二个隐藏层。\n\n**说明**：\n带有dropout的反向传播实现上非常容易。你将必须执行2个步骤：\n1.你先前通过在`A1`上应用掩码$D^{[1]}$来关闭正向传播过程中的某些神经元。在反向传播中，你将必须将相同的掩码$D^{[1]}$重新应用于`dA1`来关闭相同的神经元。\n2.在正向传播过程中，你已将`A1`除以`keep_prob`。 因此，在反向传播中，必须再次将`dA1`除以`keep_prob`（计算的解释是，如果$A^{[1]}$被`keep_prob`缩放，则其派生的$dA^{[1]}$也由相同的`keep_prob`缩放）。\n"},{"cell_type":"code","execution_count":15,"metadata":{"collapsed":false,"slideshow":{"slide_type":"slide"},"id":"914AAF073DB74D718A37A9C2763F49EB","scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[],"source":"# GRADED FUNCTION: backward_propagation_with_dropout\n\ndef backward_propagation_with_dropout(X, Y, cache, keep_prob):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added dropout.\n    \n    Arguments:\n\n    X -- input dataset, of shape (2, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation_with_dropout()\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    dW3 = 1./m * np.dot(dZ3, A2.T)\n    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n    dA2 = np.dot(W3.T, dZ3)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    dA2 = dA2 * D2              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n    dA2 = dA2 / keep_prob           # Step 2: Scale the value of neurons that haven't been shut down\n    ### END CODE HERE ###\n    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n    dW2 = 1./m * np.dot(dZ2, A1.T)\n    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n\n    dA1 = np.dot(W2.T, dZ2)\n    ### START CODE HERE ### (≈ 2 lines of code)\n    dA1 = dA1 * D1              # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n    dA1 = dA1 / keep_prob             # Step 2: Scale the value of neurons that haven't been shut down\n    ### END CODE HERE ###\n    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n    dW1 = 1./m * np.dot(dZ1, X.T)\n    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n\n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients"},{"cell_type":"code","execution_count":16,"metadata":{"slideshow":{"slide_type":"slide"},"id":"ED8E01DEB3FC4488BC11C332D70C47F9","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"dA1 = [[ 0.36544439  0.         -0.00188233  0.         -0.17408748]\n [ 0.65515713  0.         -0.00337459  0.         -0.        ]]\ndA2 = [[ 0.58180856  0.         -0.00299679  0.         -0.27715731]\n [ 0.          0.53159854 -0.          0.53159854 -0.34089673]\n [ 0.          0.         -0.00292733  0.         -0.        ]]\n","name":"stdout"}],"source":"X_assess, Y_assess, cache = backward_propagation_with_dropout_test_case()\n\ngradients = backward_propagation_with_dropout(X_assess, Y_assess, cache, keep_prob = 0.8)\n\nprint (\"dA1 = \" + str(gradients[\"dA1\"]))\nprint (\"dA2 = \" + str(gradients[\"dA2\"]))"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"A7BB861049BB484C81FB97B74CAFDAED","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**预期输出**: \ndA1 = [[ 0.36544439  0.         -0.00188233  0.         -0.17408748]\n [ 0.65515713  0.         -0.00337459  0.         -0.        ]]\ndA2 = [[ 0.58180856  0.         -0.00299679  0.         -0.27715731]\n [ 0.          0.53159854 -0.          0.53159854 -0.34089673]\n [ 0.          0.         -0.00292733  0.         -0.        ]]"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"9634C93F93AD42ACA1AC270D4AFDF101","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"现在让我们使用dropout（`keep_prob = 0.86`）运行模型。 这意味着在每次迭代中，你都以24％的概率关闭第1层和第2层的每个神经元。 函数`model()`将调用：\n- `forward_propagation_with_dropout`而不是`forward_propagation`。\n- `backward_propagation_with_dropout`，而不是`backward_propagation`。"},{"cell_type":"code","execution_count":17,"metadata":{"slideshow":{"slide_type":"slide"},"id":"2E4DAC73D8C34D02BF0F1D26DF3E8026","collapsed":false,"scrolled":false,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"stream","text":"Cost after iteration 0: 0.6543912405149825\n","name":"stdout"},{"output_type":"stream","text":"/home/kesci/input/deeplearning34288/reg_utils.py:236: RuntimeWarning: divide by zero encountered in log\n  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n/home/kesci/input/deeplearning34288/reg_utils.py:236: RuntimeWarning: invalid value encountered in multiply\n  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n","name":"stderr"},{"output_type":"stream","text":"Cost after iteration 10000: 0.0610169865749056\nCost after iteration 20000: 0.060582435798513114\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/2E4DAC73D8C34D02BF0F1D26DF3E8026/q1aud1pns9.png\">"},"transient":{}},{"output_type":"stream","text":"On the train set:\nAccuracy: 0.9289099526066351\nOn the test set:\nAccuracy: 0.95\n","name":"stdout"}],"source":"parameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)\n\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"691F73BC5F914EE385951BC3373D3E88","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"Dropout效果很好！测试精度再次提高（达到95％）！模型并未过拟合训练集，并且在测试集上表现很好。法国足球队将永远感激你！\n\n运行以下代码以绘制决策边界。"},{"cell_type":"code","execution_count":18,"metadata":{"slideshow":{"slide_type":"slide"},"id":"3E558F9F14DF444F9D89E113A14DBD57","collapsed":false,"scrolled":true,"jupyter":{},"tags":[],"trusted":true},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/3E558F9F14DF444F9D89E113A14DBD57/q1aud71of8.png\">"},"transient":{}}],"source":"plt.title(\"Model with dropout\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"6E70DC1C267A41E48A331FBFB7D5C197","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"  \n**注意**：\n- 使用dropout时的**常见错误**是在训练和测试中都使用。你只能在训练中使用dropout（随机删除节点）。\n- 深度学习框架，例如[tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/dropout), [PaddlePaddle](http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html), [keras](https://keras.io/layers/core/#dropout)或者 [caffe](http://caffe.berkeleyvision.org/tutorial/layers/dropout.html) 附带dropout层的实现。不需强调-相信你很快就会学习到其中的一些框架。\n\n**关dropout你应该记住的事情：**\n- dropout是一种正则化技术。\n- 仅在训练期间使用dropout，在测试期间不要使用。\n- 在正向和反向传播期间均应用dropout。\n- 在训练期间，将每个dropout层除以keep_prob，以保持激活的期望值相同。例如，如果keep_prob为0.5，则平均而言，我们将关闭一半的节点，因此输出将按0.5缩放，因为只有剩余的一半对解决方案有所贡献。除以0.5等于乘以2，因此输出现在具有相同的期望值。你可以检查此方法是否有效，即使keep_prob的值不是0.5。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"546AD22814E6466A8F59EBF286016C7A","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"## 4 结论"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"70BBCA4A4CEB4553A351F64B8EF8E417","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**这是我们三个模型的结果**：\n\n| 模型| 训练精度| 测试精度\n| --------------------------------- | -------------- | ------------- |\n| 三层神经网络，无正则化 |95％| 91.50％|\n| 具有L2正则化的3层NN | 94％| 93％|\n| 具有dropout的3层NN | 93％| 95％|"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"99CCBD6AF57B476B834CDA4476F68B6F","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"请注意，正则化会损害训练集的性能！ 这是因为它限制了网络过拟合训练集的能力。 但是，由于它最终可以提供更好的测试准确性，因此可以为你的系统提供帮助。"},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"},"id":"91D1D71E8A714C708A17996D63327321","jupyter":{},"tags":[],"mdEditEnable":false,"trusted":true},"source":"恭喜你完成此作业！ 同时也帮助了法国足球。 :-)"},{"cell_type":"markdown","metadata":{"collapsed":true,"slideshow":{"slide_type":"slide"},"id":"79CA2EB8578C4D94B6B3663D4A51D59A","mdEditEnable":false,"jupyter":{},"tags":[],"trusted":true},"source":"**我们希望你从此笔记本中记住的内容**：\n- 正则化将帮助减少过拟合。\n- 正则化将使权重降低到较低的值。\n- L2正则化和Dropout是两种非常有效的正则化技术。"},{"metadata":{"id":"EBB1C71687FF4572B93A0E2034510ECA","slideshow":{"slide_type":"slide"},"jupyter":{},"tags":[],"trusted":true},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}